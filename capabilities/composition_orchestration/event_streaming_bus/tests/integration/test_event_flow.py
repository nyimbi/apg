"""
APG Event Streaming Bus - Integration Tests for Event Flow

Integration tests for end-to-end event flow scenarios including publishing,
routing, consumption, and cross-capability integration.

Â© 2025 Datacraft. All rights reserved.
Author: Nyimbi Odero <nyimbi@gmail.com>
"""

import pytest
import asyncio
from datetime import datetime, timezone
from typing import List, Dict, Any

from ...models import EventConfig, StreamConfig, SubscriptionConfig, EventStatus
from ...service import EventStreamingService, EventPublishingService, EventConsumptionService
from ...apg_integration import APGEventStreamingIntegration, APGCapabilityInfo, EventRoutingRule

# =============================================================================
# End-to-End Event Flow Tests
# =============================================================================

@pytest.mark.integration
class TestEventFlow:
	"""Test complete event flow scenarios."""
	
	@pytest.mark.asyncio
	async def test_complete_event_lifecycle(
		self,
		integration_event_streaming_service,
		mock_event_publishing_service,
		mock_event_consumption_service,
		sample_stream_config,
		test_tenant_id,
		test_user_id
	):
		"""Test complete event lifecycle from creation to consumption."""
		
		# 1. Create stream
		stream_id = await integration_event_streaming_service.create_stream(
			config=sample_stream_config,
			tenant_id=test_tenant_id,
			created_by=test_user_id
		)
		
		assert stream_id.startswith("str_")
		
		# 2. Create subscription
		subscription_config = SubscriptionConfig(
			subscription_name="test_subscription",
			stream_id=stream_id,
			consumer_group_id="test_consumers",
			consumer_name="test_consumer",
			event_type_patterns=["user.*"]
		)
		
		subscription_id = await mock_event_consumption_service.create_subscription(
			config=subscription_config,
			tenant_id=test_tenant_id,
			created_by=test_user_id
		)
		
		assert subscription_id.startswith("sub_")
		
		# 3. Publish events
		event_config = EventConfig(
			event_type="user.created",
			source_capability="user_management",
			aggregate_id="user_123",
			aggregate_type="User"
		)
		
		payload = {
			"user_name": "john.doe",
			"email": "john.doe@example.com",
			"department": "Engineering"
		}
		
		event_id = await mock_event_publishing_service.publish_event(
			event_config=event_config,
			payload=payload,
			stream_id=stream_id,
			tenant_id=test_tenant_id,
			user_id=test_user_id
		)
		
		assert event_id.startswith("evt_")
		
		# Verify the flow worked
		mock_event_publishing_service.publish_event.assert_called_once()
		mock_event_consumption_service.create_subscription.assert_called_once()
	
	@pytest.mark.asyncio
	async def test_event_ordering_guarantees(
		self,
		mock_event_publishing_service,
		test_tenant_id,
		test_user_id
	):
		"""Test that events maintain proper ordering."""
		
		# Publish sequence of events with same partition key
		events = []
		for i in range(10):
			event_config = EventConfig(
				event_type="user.updated",
				source_capability="user_management",
				aggregate_id="user_123",
				aggregate_type="User",
				sequence_number=i,
				partition_key="user_123"  # Same partition key for ordering
			)
			
			payload = {"update_sequence": i, "field": f"value_{i}"}
			
			event_id = await mock_event_publishing_service.publish_event(
				event_config=event_config,
				payload=payload,
				stream_id="user_events",
				tenant_id=test_tenant_id,
				user_id=test_user_id
			)
			
			events.append(event_id)
		
		# Verify all events were published
		assert len(events) == 10
		assert mock_event_publishing_service.publish_event.call_count == 10
		
		# In a real integration test, we would verify ordering in Kafka
		# For now, we verify the service was called correctly
	
	@pytest.mark.asyncio
	async def test_event_batch_processing(
		self,
		mock_event_publishing_service,
		test_tenant_id,
		test_user_id
	):
		"""Test batch event processing performance."""
		
		# Create batch of events
		batch_events = []
		for i in range(100):
			event_config = EventConfig(
				event_type="order.created",
				source_capability="order_management",
				aggregate_id=f"order_{i}",
				aggregate_type="Order"
			)
			
			payload = {
				"order_id": f"order_{i}",
				"customer_id": f"customer_{i % 10}",
				"amount": 100.0 + i
			}
			
			batch_events.append((event_config, payload))
		
		# Publish batch
		event_ids = await mock_event_publishing_service.publish_event_batch(
			events=batch_events,
			stream_id="order_events",
			tenant_id=test_tenant_id,
			user_id=test_user_id
		)
		
		assert len(event_ids) == 100
		mock_event_publishing_service.publish_event_batch.assert_called_once()
	
	@pytest.mark.asyncio
	async def test_consumer_group_load_balancing(
		self,
		mock_event_consumption_service,
		test_tenant_id,
		test_user_id
	):
		"""Test consumer group load balancing across multiple consumers."""
		
		# Create multiple subscriptions in the same consumer group
		consumer_group_id = "load_balanced_group"
		subscriptions = []
		
		for i in range(3):
			subscription_config = SubscriptionConfig(
				subscription_name=f"consumer_{i}",
				stream_id="high_volume_stream",
				consumer_group_id=consumer_group_id,
				consumer_name=f"consumer_instance_{i}",
				event_type_patterns=["*"]  # All events
			)
			
			subscription_id = await mock_event_consumption_service.create_subscription(
				config=subscription_config,
				tenant_id=test_tenant_id,
				created_by=test_user_id
			)
			
			subscriptions.append(subscription_id)
		
		assert len(subscriptions) == 3
		assert mock_event_consumption_service.create_subscription.call_count == 3
		
		# In a real test, we would verify load distribution across consumers

# =============================================================================
# Cross-Capability Integration Tests
# =============================================================================

@pytest.mark.integration
class TestCrossCapabilityIntegration:
	"""Test integration between different APG capabilities."""
	
	@pytest.mark.asyncio
	async def test_capability_registration_and_discovery(
		self,
		integration_apg_integration,
		sample_capability_info
	):
		"""Test capability registration and discovery."""
		
		# Register capability
		success = await integration_apg_integration.register_capability(sample_capability_info)
		assert success == True
		
		# Discover capabilities
		capabilities = await integration_apg_integration.discover_capabilities()
		assert len(capabilities) >= 1
		
		# Find our registered capability
		found_capability = None
		for cap in capabilities:
			if cap.capability_id == sample_capability_info.capability_id:
				found_capability = cap
				break
		
		assert found_capability is not None
		assert found_capability.capability_name == sample_capability_info.capability_name
	
	@pytest.mark.asyncio
	async def test_event_routing_between_capabilities(
		self,
		integration_apg_integration,
		sample_capability_info,
		sample_routing_rule,
		sample_event_data
	):
		"""Test event routing between capabilities."""
		
		# Register capabilities
		await integration_apg_integration.register_capability(sample_capability_info)
		
		# Create target capability
		target_capability = APGCapabilityInfo(
			capability_id="target_capability",
			capability_name="Target Capability",
			capability_type="service",
			version="1.0.0",
			endpoints={"api": "/api/v1"},
			event_patterns=["user.*"],
			dependencies=[]
		)
		await integration_apg_integration.register_capability(target_capability)
		
		# Add routing rule
		routing_rule = EventRoutingRule(
			rule_id="test_routing_rule",
			source_pattern="user_management",
			target_capabilities=["target_capability"],
			event_type_patterns=["user.*"]
		)
		
		success = await integration_apg_integration.add_routing_rule(routing_rule)
		assert success == True
		
		# Create and route event
		from ...models import ESEvent
		event = ESEvent(**sample_event_data)
		
		routed_to = await integration_apg_integration.route_event(event)
		assert "target_capability" in routed_to
	
	@pytest.mark.asyncio
	async def test_cross_capability_workflow_execution(
		self,
		integration_apg_integration,
		sample_workflow,
		sample_event_data
	):
		"""Test cross-capability workflow execution."""
		
		# Register workflow
		success = await integration_apg_integration.register_workflow(sample_workflow)
		assert success == True
		
		# Create trigger event
		from ...models import ESEvent
		trigger_event = ESEvent(**sample_event_data)
		trigger_event.event_type = "user.created"  # Matches workflow trigger
		
		# Trigger workflow
		instance_id = await integration_apg_integration.trigger_workflow(
			sample_workflow.workflow_id,
			trigger_event
		)
		
		assert instance_id is not None
		assert instance_id.startswith(sample_workflow.workflow_id)
		
		# Check workflow status
		# Give it a moment to start (in real test, might need longer wait)
		await asyncio.sleep(0.1)
		
		status = await integration_apg_integration.get_workflow_status(instance_id)
		assert status is not None
		assert status["workflow_id"] == sample_workflow.workflow_id
		assert status["status"] in ["running", "completed"]
	
	@pytest.mark.asyncio
	async def test_event_composition_pattern(
		self,
		integration_apg_integration,
		sample_composition_pattern,
		sample_event_data
	):
		"""Test event composition patterns."""
		
		# Register composition pattern
		success = await integration_apg_integration.register_composition_pattern(sample_composition_pattern)
		assert success == True
		
		# Create input events for pattern
		from ...models import ESEvent
		
		# First event: payment received
		payment_event_data = sample_event_data.copy()
		payment_event_data["event_type"] = "order.payment_received"
		payment_event_data["aggregate_id"] = "order_123"
		payment_event_data["payload"] = {"order_id": "order_123", "amount": 100.0}
		payment_event = ESEvent(**payment_event_data)
		
		# Second event: items shipped
		shipping_event_data = sample_event_data.copy()
		shipping_event_data["event_type"] = "order.items_shipped"
		shipping_event_data["aggregate_id"] = "order_123"
		shipping_event_data["payload"] = {"order_id": "order_123", "tracking_number": "TRK123"}
		shipping_event = ESEvent(**shipping_event_data)
		
		# Process events for pattern
		result1 = await integration_apg_integration.process_pattern_event(
			sample_composition_pattern.pattern_id,
			payment_event
		)
		assert result1 is None  # Pattern not complete yet
		
		result2 = await integration_apg_integration.process_pattern_event(
			sample_composition_pattern.pattern_id,
			shipping_event
		)
		# In a real test, this might complete the pattern and return an event ID
		# For now, we just verify the method was called successfully

# =============================================================================
# Multi-tenant Isolation Tests
# =============================================================================

@pytest.mark.integration
class TestMultiTenantIsolation:
	"""Test multi-tenant data isolation."""
	
	@pytest.mark.asyncio
	async def test_tenant_stream_isolation(
		self,
		integration_event_streaming_service,
		sample_stream_config
	):
		"""Test that tenants can only access their own streams."""
		
		tenant1_id = "tenant_1"
		tenant2_id = "tenant_2"
		user_id = "test_user"
		
		# Create stream for tenant 1
		stream1_config = sample_stream_config.model_copy()
		stream1_config.stream_name = "tenant1_stream"
		stream1_id = await integration_event_streaming_service.create_stream(
			config=stream1_config,
			tenant_id=tenant1_id,
			created_by=user_id
		)
		
		# Create stream for tenant 2
		stream2_config = sample_stream_config.model_copy()
		stream2_config.stream_name = "tenant2_stream"
		stream2_id = await integration_event_streaming_service.create_stream(
			config=stream2_config,
			tenant_id=tenant2_id,
			created_by=user_id
		)
		
		# Tenant 1 should only see their stream
		tenant1_streams = await integration_event_streaming_service.list_streams(tenant1_id)
		tenant1_stream_ids = [s.stream_id for s in tenant1_streams]
		assert stream1_id in tenant1_stream_ids
		assert stream2_id not in tenant1_stream_ids
		
		# Tenant 2 should only see their stream
		tenant2_streams = await integration_event_streaming_service.list_streams(tenant2_id)
		tenant2_stream_ids = [s.stream_id for s in tenant2_streams]
		assert stream2_id in tenant2_stream_ids
		assert stream1_id not in tenant2_stream_ids
	
	@pytest.mark.asyncio
	async def test_tenant_event_isolation(
		self,
		mock_event_publishing_service,
		integration_apg_integration
	):
		"""Test that tenant events are properly isolated."""
		
		tenant1_id = "tenant_1"
		tenant2_id = "tenant_2"
		user_id = "test_user"
		
		# Create event for tenant 1
		event1_config = EventConfig(
			event_type="user.created",
			source_capability="user_management",
			aggregate_id="user_1",
			aggregate_type="User"
		)
		
		event1_id = await mock_event_publishing_service.publish_event(
			event_config=event1_config,
			payload={"user_name": "tenant1_user"},
			stream_id="user_events",
			tenant_id=tenant1_id,
			user_id=user_id
		)
		
		# Create event for tenant 2
		event2_config = EventConfig(
			event_type="user.created",
			source_capability="user_management",
			aggregate_id="user_2",
			aggregate_type="User"
		)
		
		event2_id = await mock_event_publishing_service.publish_event(
			event_config=event2_config,
			payload={"user_name": "tenant2_user"},
			stream_id="user_events",
			tenant_id=tenant2_id,
			user_id=user_id
		)
		
		# Verify isolation through APG integration
		tenant1_streams = await integration_apg_integration.get_tenant_streams(tenant1_id)
		tenant2_streams = await integration_apg_integration.get_tenant_streams(tenant2_id)
		
		# In a real test, we would verify that each tenant can only access their events
		assert isinstance(tenant1_streams, list)
		assert isinstance(tenant2_streams, list)

# =============================================================================
# Error Handling and Recovery Tests
# =============================================================================

@pytest.mark.integration
class TestErrorHandlingAndRecovery:
	"""Test error handling and recovery scenarios."""
	
	@pytest.mark.asyncio
	async def test_event_retry_mechanism(
		self,
		mock_event_publishing_service,
		test_tenant_id,
		test_user_id
	):
		"""Test event retry mechanism for failed events."""
		
		# Configure service to fail initially then succeed
		call_count = 0
		original_publish = mock_event_publishing_service.publish_event
		
		async def failing_publish(*args, **kwargs):
			nonlocal call_count
			call_count += 1
			if call_count <= 2:  # Fail first 2 attempts
				raise Exception("Simulated failure")
			return await original_publish(*args, **kwargs)
		
		mock_event_publishing_service.publish_event = failing_publish
		
		event_config = EventConfig(
			event_type="test.retry",
			source_capability="test_service",
			aggregate_id="test_123",
			aggregate_type="Test"
		)
		
		# This should succeed after retries
		with pytest.raises(Exception):  # First attempt will fail
			await mock_event_publishing_service.publish_event(
				event_config=event_config,
				payload={"test": "data"},
				stream_id="test_stream",
				tenant_id=test_tenant_id,
				user_id=test_user_id
			)
	
	@pytest.mark.asyncio
	async def test_dead_letter_queue_handling(
		self,
		mock_event_consumption_service,
		test_tenant_id,
		test_user_id
	):
		"""Test dead letter queue handling for failed event processing."""
		
		# Create subscription with dead letter queue enabled
		subscription_config = SubscriptionConfig(
			subscription_name="dlq_test_subscription",
			stream_id="test_stream",
			consumer_group_id="test_consumers",
			consumer_name="test_consumer",
			dead_letter_enabled=True,
			dead_letter_topic="test_dlq"
		)
		
		subscription_id = await mock_event_consumption_service.create_subscription(
			config=subscription_config,
			tenant_id=test_tenant_id,
			created_by=test_user_id
		)
		
		assert subscription_id.startswith("sub_")
		
		# Simulate processing failed events
		failed_events = [
			{"event_id": "evt_failed_1", "error": "Processing error 1"},
			{"event_id": "evt_failed_2", "error": "Processing error 2"}
		]
		
		# Mock DLQ processing
		mock_event_consumption_service._send_to_dlq = AsyncMock(return_value=True)
		
		for event in failed_events:
			await mock_event_consumption_service._send_to_dlq(
				subscription_id,
				event,
				event["error"]
			)
		
		# Verify DLQ calls
		assert mock_event_consumption_service._send_to_dlq.call_count == 2
	
	@pytest.mark.asyncio
	async def test_stream_recovery_after_failure(
		self,
		integration_event_streaming_service,
		sample_stream_config,
		test_tenant_id,
		test_user_id
	):
		"""Test stream recovery after system failure."""
		
		# Create stream
		stream_id = await integration_event_streaming_service.create_stream(
			config=sample_stream_config,
			tenant_id=test_tenant_id,
			created_by=test_user_id
		)
		
		# Simulate stream going to error state
		stream = await integration_event_streaming_service.get_stream(stream_id, test_tenant_id)
		if stream:
			stream.status = "error"
		
		# Recover stream (in real implementation, this would involve more complex logic)
		recovery_success = await integration_event_streaming_service._recover_stream(stream_id)
		
		# In a real test, we would verify the stream is back to active state
		# For now, we just verify the method can be called
		assert recovery_success is not None