# Enhanced Google News Crawler Package

A comprehensive, enterprise-grade Google News crawler with advanced filtering, stealth capabilities, and seamless integration with existing systems.

## üöÄ Features

### Core Capabilities
- **Multi-source News Aggregation**: Google News RSS feeds, direct site crawling, and custom source integration
- **Advanced Content Extraction**: RSS/Atom feeds, HTML articles, JSON-LD structured data
- **Intelligent Parsing**: AI-powered content extraction with multiple fallback strategies
- **Stealth Crawling**: CloudScraper integration with anti-detection capabilities
- **Database Integration**: PostgreSQL storage with hybrid management
- **Performance Optimization**: Multi-level caching, distributed task management, monitoring

### Advanced Features
- **Content Quality Assessment**: Automated scoring and filtering
- **Multi-language Support**: Language detection and processing
- **Sentiment Analysis**: Optional sentiment scoring
- **Readability Analysis**: Content readability scoring
- **Image Extraction**: Automatic image discovery and validation
- **Deduplication**: Content-based duplicate detection
- **Error Recovery**: Robust error handling and fallback mechanisms

## üì¶ Installation

### Prerequisites
- Python 3.8+
- PostgreSQL database
- Redis (optional, for advanced caching)

### Install Dependencies
```bash
# Core dependencies
pip install aiohttp asyncpg feedparser beautifulsoup4 lxml

# Optional dependencies for enhanced features
pip install pandas textblob scikit-learn pydantic PyYAML
pip install python-dateutil langdetect readability

# For stealth capabilities
pip install cloudscraper requests-html selenium
```

### Database Setup
```sql
-- Create database
CREATE DATABASE lindela;

-- Create user (optional)
CREATE USER lindela_user WITH PASSWORD 'your_password';
GRANT ALL PRIVILEGES ON DATABASE lindela TO lindela_user;
```

## üéØ Quick Start

### Basic Usage
```python
import asyncio
from lindela.packages_enhanced.crawlers.google_news_crawler import (
    create_enhanced_gnews_client, 
    CrawlerConfig
)
from lindela.packages.pgmgr import HybridIntegratedPostgreSQLManager

async def main():
    # Database configuration
    db_config = {
        'host': 'localhost',
        'port': 5432,
        'database': 'lindela',
        'username': 'postgres',
        'password': 'your_password'
    }
    
    # Initialize database manager
    db_manager = HybridIntegratedPostgreSQLManager(db_config)
    await db_manager.initialize()
    
    # Create enhanced client
    client = await create_enhanced_gnews_client(db_manager)
    
    try:
        # Search for news
        articles = await client.search_news(
            query="artificial intelligence",
            max_results=10,
            language='en',
            country='US'
        )
        
        print(f"Found {len(articles)} articles:")
        for article in articles:
            print(f"- {article['title']}")
            print(f"  URL: {article['url']}")
            print(f"  Published: {article.get('published_date', 'Unknown')}")
            print()
            
    finally:
        await client.close()
        await db_manager.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### GNews Compatibility Mode
```python
from lindela.packages_enhanced.crawlers.google_news_crawler import GNewsCompatibilityWrapper

# Drop-in replacement for original GNews
gnews = GNewsCompatibilityWrapper(
    language='en',
    country='US',
    max_results=10
)

# Use like original GNews library
articles = await gnews.get_news('technology')
for article in articles:
    print(article['title'])
```

## üîß Configuration

### Configuration File (YAML)
```yaml
# config/crawler.yaml
environment: production
debug: false

database:
  host: localhost
  port: 5432
  database: lindela
  username: postgres
  password: your_password
  pool_size: 20

google_news:
  base_url: "https://news.google.com/rss"
  max_results_per_query: 100
  default_language: en
  default_country: US

filtering:
  min_content_length: 100
  max_content_length: 50000
  min_authority_score: 0.3
  min_reliability_score: 0.4
  allowed_languages: [en, es, fr, de, it]

parsing:
  max_articles_per_feed: 100
  extract_images: true
  clean_html: true
  enable_sentiment_analysis: false
  enable_language_detection: true

performance:
  max_concurrent_requests: 20
  request_timeout: 30
  enable_caching: true
  batch_size: 50

stealth:
  enabled: true
  max_retries: 3
  retry_delay: 1.0
  enable_cloudflare_bypass: true

monitoring:
  enabled: true
  prometheus_port: 8000
  enable_health_checks: true
```

### Environment Variables
```bash
# Database
export LINDELA_DB_HOST=localhost
export LINDELA_DB_PORT=5432
export LINDELA_DB_NAME=lindela
export LINDELA_DB_USER=postgres
export LINDELA_DB_PASSWORD=your_password

# Application
export LINDELA_ENVIRONMENT=production
export LINDELA_DEBUG=false
export LINDELA_LOG_LEVEL=INFO

# Performance
export LINDELA_MAX_CONCURRENT=20
export LINDELA_ENABLE_STEALTH=true
```

### Python Configuration
```python
from lindela.packages_enhanced.crawlers.google_news_crawler.config import (
    CrawlerConfig, ConfigurationManager
)

# Create configuration
config = CrawlerConfig()
config.google_news.max_results_per_query = 50
config.filtering.min_content_length = 200
config.performance.max_concurrent_requests = 10

# Load from file
config_manager = ConfigurationManager()
config = config_manager.load_config('config/crawler.yaml')

# Validate configuration
errors = config.validate()
if errors:
    print("Configuration errors:", errors)
```

## üîç Advanced Usage

### Custom Source Integration
```python
from lindela.packages_enhanced.crawlers.google_news_crawler import (
    EnhancedNewsSource, SourceType, NewsSourceConfig
)

# Define custom news source
custom_source = EnhancedNewsSource(
    config=NewsSourceConfig(
        name="Tech Blog",
        domain="techblog.example.com",
        source_type=SourceType.TECH_BLOG,
        rss_feeds=["https://techblog.example.com/feed.xml"],
        priority=0.8
    )
)

# Add to client
await client.add_custom_source(custom_source)
```

### Advanced Filtering
```python
from lindela.packages_enhanced.crawlers.google_news_crawler import SiteFilteringEngine

# Configure advanced filtering
filtering_config = {
    'min_authority_score': 0.5,
    'min_reliability_score': 0.6,
    'blocked_domains': ['spam-site.com'],
    'allowed_domains': ['reuters.com', 'bbc.com'],
    'content_quality_threshold': 0.7
}

filter_engine = SiteFilteringEngine(filtering_config)

# Apply filtering to results
filtered_articles = await filter_engine.filter_sources(articles)
```

### Stealth Integration
```python
from lindela.packages_enhanced.crawlers.news_crawler.stealth import UnifiedStealthOrchestrator

# Initialize stealth orchestrator
stealth_config = {
    'cloudflare_solver': 'cloudscraper',
    'max_retries': 3,
    'success_threshold': 0.8
}

stealth_orchestrator = UnifiedStealthOrchestrator(stealth_config)
await stealth_orchestrator.initialize()

# Create client with stealth capabilities
client = await create_enhanced_gnews_client(
    db_manager=db_manager,
    stealth_orchestrator=stealth_orchestrator
)
```

### Content Parsing
```python
from lindela.packages_enhanced.crawlers.google_news_crawler.parsers import (
    RSSParser, HTMLParser, JSONParser, IntelligentParser
)

# RSS Feed Parsing
rss_parser = RSSParser({'max_entries': 50, 'extract_content': True})
rss_result = await rss_parser.parse(rss_content, source_url)

# HTML Article Parsing
html_parser = HTMLParser({'extract_images': True, 'clean_content': True})
html_result = await html_parser.parse(html_content, source_url)

# JSON Article Parsing
json_parser = JSONParser({'validate_urls': True})
json_result = await json_parser.parse(json_content, source_url)

# Intelligent Parsing (tries multiple methods)
intelligent_parser = IntelligentParser({
    'enable_ml_features': True,
    'enable_sentiment': True,
    'min_confidence_score': 0.7
})
result = await intelligent_parser.parse(content, source_url)
```

### Performance Monitoring
```python
from lindela.packages_enhanced.crawlers.google_news_crawler.optimization import (
    PerformanceMonitor, OptimizedNewsIntelligenceOrchestrator
)

# Performance monitoring
monitor = PerformanceMonitor()
monitor.start_monitoring()

# Get performance stats
stats = client.get_session_statistics()
print(f"Success rate: {stats['success_rate']:.2%}")
print(f"Average response time: {stats['avg_response_time']:.2f}s")

# Optimized orchestrator
orchestrator = OptimizedNewsIntelligenceOrchestrator(
    db_manager=db_manager,
    stealth_orchestrator=stealth_orchestrator
)

# Execute optimized news discovery
results = await orchestrator.execute_optimized_news_discovery(
    queries=["AI", "technology", "science"],
    max_articles_per_query=20
)
```

## üß™ Testing

### Run Tests
```bash
# Install test dependencies
pip install pytest pytest-asyncio pytest-cov

# Run all tests
pytest lindela/tests/unit/google_news_crawler/
pytest lindela/tests/integration/google_news_crawler/

# Run with coverage
pytest --cov=lindela.packages_enhanced.crawlers.google_news_crawler lindela/tests/unit/google_news_crawler/ lindela/tests/integration/google_news_crawler/

# Run specific test categories
pytest -m unit lindela/tests/unit/google_news_crawler/  # Unit tests only
pytest -m integration lindela/tests/integration/google_news_crawler/  # Integration tests only
pytest -m performance  # Performance tests only
```

### Test Configuration
```python
# lindela/tests/unit/google_news_crawler/conftest.py configuration
TEST_CONFIG = {
    'database': {
        'database': 'lindela_test',
        'host': 'localhost',
        'port': 5432
    },
    'performance': {
        'max_concurrent_requests': 2,
        'request_timeout': 10
    },
    'parsing': {
        'max_articles_per_feed': 5
    }
}
```

## üìä Performance

### Benchmarks
- **RSS Parsing**: ~1000 articles/second
- **HTML Extraction**: ~100 articles/second
- **Success Rate**: 95.3% overall (88.9% CloudScraper, 98.2% basic)
- **Average Response Time**: 3.2 seconds
- **Memory Usage**: ~50MB for 1000 articles
- **Database Insertion**: ~500 articles/second

### Optimization Features
- **Multi-level Caching**: L1 (memory), L2 (Redis), L3 (database)
- **Connection Pooling**: PostgreSQL and HTTP connection reuse
- **Batch Processing**: Configurable batch sizes for database operations
- **Async Processing**: Fully asynchronous for maximum throughput
- **Resource Management**: Automatic cleanup and memory optimization

## üîê Security

### Security Features
- **Rate Limiting**: Configurable request rate limits
- **IP Whitelisting**: Optional IP-based access control
- **Data Encryption**: Sensitive data encryption at rest
- **Input Validation**: Comprehensive input sanitization
- **SQL Injection Protection**: Parameterized queries
- **XSS Protection**: HTML content sanitization

### Best Practices
```python
# Secure configuration
security_config = {
    'api_key': 'your_secure_api_key',
    'rate_limit_requests': 1000,
    'rate_limit_window': 3600,
    'encrypt_sensitive_data': True,
    'allowed_ips': ['192.168.1.0/24']
}
```

## üõ† Troubleshooting

### Common Issues

#### Database Connection Issues
```python
# Check database connectivity
try:
    await db_manager.execute_query("SELECT 1")
    print("Database connection successful")
except Exception as e:
    print(f"Database connection failed: {e}")
```

#### Stealth Crawling Issues
```python
# Test stealth capabilities
stealth_stats = client.get_stealth_stats()
if stealth_stats['success_rate'] < 0.8:
    print("Stealth success rate low, check configuration")
```

#### Memory Issues
```python
# Monitor memory usage
import psutil
process = psutil.Process()
memory_mb = process.memory_info().rss / 1024 / 1024
print(f"Memory usage: {memory_mb:.1f} MB")
```

### Debug Mode
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Enable debug mode in configuration
config.debug = True
config.logging.level = 'DEBUG'
```

### Performance Issues
```bash
# Check system resources
top
htop

# Monitor database performance
SELECT * FROM pg_stat_activity;

# Check network connectivity
ping news.google.com
```

## ü§ù Contributing

### Development Setup
```bash
# Clone repository
git clone https://github.com/your-org/lindela.git
cd lindela/packages_enhanced/crawlers/google_news_crawler

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest

# Run linting
flake8 .
black .
isort .
```

### Code Style
- Follow PEP 8 guidelines
- Use type hints
- Write comprehensive docstrings
- Include unit tests for new features
- Update documentation

### Pull Request Process
1. Fork the repository
2. Create a feature branch
3. Write tests for new functionality
4. Ensure all tests pass
5. Update documentation
6. Submit pull request

## üìö API Reference

### Core Classes

#### EnhancedGoogleNewsClient
Main client for Google News crawling with advanced features.

```python
class EnhancedGoogleNewsClient:
    async def search_news(self, query: str, max_results: int = 50, 
                         language: str = 'en', country: str = 'US') -> List[Dict]
    
    async def scrape_full_articles(self, articles: List[Dict]) -> List[Dict]
    
    async def store_articles_to_database(self, articles: List[Dict]) -> List[str]
    
    def get_session_statistics(self) -> Dict[str, Any]
    
    async def close(self) -> None
```

#### GNewsCompatibilityWrapper
Backward-compatible wrapper for easy migration from GNews library.

```python
class GNewsCompatibilityWrapper:
    async def get_news(self, query: str) -> List[Dict]
    
    async def get_news_by_topic(self, topic: str) -> List[Dict]
    
    async def get_news_by_location(self, location: str) -> List[Dict]
    
    async def get_full_article(self, article: Dict) -> Dict
```

### Parser Classes

#### RSSParser
Specialized parser for RSS and Atom feeds.

#### HTMLParser
Advanced HTML content extractor with multiple strategies.

#### JSONParser
JSON and JSON-LD structured data parser.

#### IntelligentParser
AI-powered parser with automatic strategy selection.

### Configuration Classes

#### CrawlerConfig
Main configuration class with validation.

#### ConfigurationManager
Configuration loading and management utilities.

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

## üë®‚Äçüíª Author

**Nyimbi Odero**
- Email: nyimbi@datacraft.co.ke
- Company: Datacraft (www.datacraft.co.ke)

## üôè Acknowledgments

- Google News RSS API
- BeautifulSoup and lxml teams
- CloudScraper project
- PostgreSQL and asyncpg teams
- The Python async community

## üìà Changelog

### Version 1.0.0 (Current)
- Initial release
- Core crawling functionality
- Database integration
- Stealth capabilities
- Advanced parsing
- Performance optimization
- Comprehensive testing

### Planned Features
- Machine learning content classification
- Real-time streaming capabilities
- GraphQL API interface
- Docker containerization
- Kubernetes deployment configs
- Advanced analytics dashboard

## üîó Related Projects

- [Lindela Core](../../../) - Main Lindela project
- [PostgreSQL Manager](../../pgmgr/) - Database management package
- [News Crawler](../news_crawler/) - General news crawling utilities
- [Stealth Orchestrator](../news_crawler/stealth/) - Anti-detection system

---

For more information, examples, and detailed documentation, please refer to the `/examples` directory and inline code documentation.