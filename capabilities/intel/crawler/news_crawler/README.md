# News Crawler Package

Advanced news crawling system with comprehensive stealth capabilities, intelligent content extraction, and unified configuration management.

## üåü Features

### üõ°Ô∏è **Advanced Stealth Capabilities**
- **üîì Cloudflare Bypass**: CloudScraper integration with intelligent challenge solving
- **ü§ñ Enhanced Browser Mimicking**: Realistic user agents, headers, and fingerprint protection
- **‚è±Ô∏è Intelligent Rate Limiting**: Adaptive delays and exponential backoff
- **üîÑ Session Management**: Automatic rotation and persistence
- **üéØ Multi-Strategy Bypass**: Fallback chain for maximum success rates

### üï∑Ô∏è **Comprehensive Crawling**
- **üì∞ Enhanced News Crawler**: Individual article extraction with stealth
- **üåê Deep Site Crawler**: Complete site discovery and bulk extraction
- **üì° Smart Discovery**: Sitemap, RSS, and intelligent link parsing
- **üîç Universal Compatibility**: Works across diverse news site architectures
- **üìä Quality Assessment**: Automatic content quality scoring

### ‚öôÔ∏è **Unified Configuration**
- **üîß Utils/Config Integration**: Leverages Lindela's unified configuration system
- **üåç Environment Support**: Configuration via environment variables
- **üìù Validation**: Comprehensive configuration validation
- **üîÑ Hot Reloading**: Dynamic configuration updates

### üß† **Intelligent Content Processing**
- **üìä Multi-Parser System**: newspaper3k, trafilatura, BeautifulSoup fallback chain
- **üè∑Ô∏è ML Analysis**: Content quality, sentiment, and entity extraction
- **üìà Performance Monitoring**: Real-time metrics and health checking
- **üéØ Smart Filtering**: Quality-based content validation

### üóÑÔ∏è **Enterprise Ready**
- **üêò Database Integration**: PostgreSQL support with transaction safety
- **üìä Monitoring**: Comprehensive performance and health metrics
- **üîí Security**: Ethical crawling with respect for robots.txt
- **üìà Scalability**: Async-first design for high throughput

## üöÄ Quick Start

### Installation

```bash
# Core dependencies
pip install aiohttp cloudscraper newspaper3k trafilatura beautifulsoup4

# Optional: Database support
pip install asyncpg

# Optional: Browser automation
pip install playwright
playwright install
```

### Basic Usage

#### Single Article Extraction

```python
import asyncio
from news_crawler import NewsCrawler

async def extract_article():
    crawler = NewsCrawler()
    
    try:
        article = await crawler.crawl_url("https://example-news.com/article")
        if article:
            print(f"Title: {article.title}")
            print(f"Content: {article.content[:200]}...")
            print(f"Quality Score: {article.quality_score}")
    finally:
        await crawler.cleanup()

asyncio.run(extract_article())
```

#### Entire Site Crawling

```python
from news_crawler import CloudScraperStealthCrawler

async def crawl_entire_site():
    crawler = CloudScraperStealthCrawler()
    
    try:
        articles = await crawler.crawl_entire_site(
            "https://news-site.com", 
            max_articles=100
        )
        
        print(f"Extracted {len(articles)} articles")
        for article in articles[:5]:
            print(f"- {article.title}")
            
    finally:
        await crawler.cleanup()

asyncio.run(crawl_entire_site())
```

#### Convenience Functions

```python
from news_crawler import crawl_url, crawl_multiple

# Single URL (automatic cleanup)
article = await crawl_url("https://news-site.com/article")

# Multiple URLs (automatic cleanup)
urls = ["https://site1.com/article1", "https://site2.com/article2"]
articles = await crawl_multiple(urls)
```

## üîß Configuration

### Using Default Configuration

```python
from news_crawler import get_default_config, NewsCrawler

# Get default configuration
config = get_default_config()
print(config)

# Use with crawler
crawler = NewsCrawler(config)
```

### Custom Configuration

```python
config = {
    # Performance settings
    'max_concurrent_requests': 5,
    'requests_per_second': 2.0,
    'request_timeout': 30,
    'max_retries': 3,
    
    # Stealth settings
    'enable_stealth': True,
    'enable_enhanced_stealth': True,
    'min_delay': 2.0,
    'max_delay': 5.0,
    
    # Content settings
    'enable_ml_analysis': True,
    'min_content_length': 200,
    
    # Database settings
    'enable_database': False,
    'database_connection_string': 'postgresql://...'
}

crawler = NewsCrawler(config)
```

### Unified Configuration System

```python
from news_crawler.config import create_news_crawler_config

# Create configuration using utils/config
config = await create_news_crawler_config()
crawler = NewsCrawler(config)
```

### Environment Variables

```bash
# Configuration via environment
export NEWS_CRAWLER_MAX_CONCURRENT=10
export NEWS_CRAWLER_TIMEOUT=45
export NEWS_CRAWLER_ENABLE_STEALTH=true
export NEWS_CRAWLER_RATE_LIMIT=3.0
```

## üèóÔ∏è Architecture

### Package Structure

```
news_crawler/
‚îú‚îÄ‚îÄ __init__.py              # Package interface
‚îú‚îÄ‚îÄ README.md                # This documentation
‚îú‚îÄ‚îÄ docs/                    # Comprehensive documentation
‚îÇ   ‚îú‚îÄ‚îÄ user_guide.md       # Detailed user guide
‚îÇ   ‚îî‚îÄ‚îÄ architecture.md     # Architecture documentation
‚îú‚îÄ‚îÄ core/                    # Core crawler implementations
‚îÇ   ‚îú‚îÄ‚îÄ enhanced_news_crawler.py    # Primary stealth crawler
‚îÇ   ‚îî‚îÄ‚îÄ deep_crawling_news_crawler.py  # Site discovery crawler
‚îú‚îÄ‚îÄ bypass/                  # Anti-detection systems
‚îÇ   ‚îú‚îÄ‚îÄ bypass_manager.py    # Unified bypass coordination
‚îÇ   ‚îú‚îÄ‚îÄ cloudflare_bypass.py # Cloudflare-specific handling
‚îÇ   ‚îî‚îÄ‚îÄ anti_403_handler.py  # HTTP 403 error recovery
‚îú‚îÄ‚îÄ parsers/                 # Content extraction
‚îÇ   ‚îú‚îÄ‚îÄ content_parser.py    # Unified parser interface
‚îÇ   ‚îú‚îÄ‚îÄ article_extractor.py # Article content extraction
‚îÇ   ‚îî‚îÄ‚îÄ metadata_extractor.py # Metadata extraction
‚îú‚îÄ‚îÄ config/                  # Configuration management
‚îÇ   ‚îî‚îÄ‚îÄ unified_config.py    # Utils/config integration
‚îî‚îÄ‚îÄ examples/               # Usage examples
    ‚îú‚îÄ‚îÄ comprehensive_example.py
    ‚îú‚îÄ‚îÄ deep_crawl_example.py
    ‚îî‚îÄ‚îÄ stealth_example.py
```

### Core Components

#### 1. Enhanced News Crawler
Primary crawler optimized for individual article extraction with advanced stealth.

**Features:**
- CloudScraper-based Cloudflare bypass
- Intelligent user agent rotation
- Adaptive rate limiting
- Multi-method content extraction
- Real-time quality assessment

#### 2. Deep Crawling News Crawler  
Specialized for comprehensive site discovery and bulk article extraction.

**Features:**
- Site discovery via sitemaps, RSS, and link crawling
- Hierarchical URL filtering and prioritization
- Batch processing with session management
- Progress tracking and resumption
- Enhanced crawler integration

#### 3. Bypass System
Unified bypass manager coordinating multiple anti-detection strategies.

**Strategy Chain:**
1. Direct request (baseline)
2. Cloudflare bypass (for CF-protected sites)
3. Anti-403 handling (for forbidden errors)
4. User agent rotation (for basic detection)

#### 4. Parser System
Unified content parsing with multiple extraction methods and quality assessment.

**Parser Chain:**
1. HTML Parser ‚Üí Structure extraction
2. Article Extractor ‚Üí Content extraction
3. Metadata Extractor ‚Üí Metadata extraction
4. ML Analyzer ‚Üí Quality assessment

## üõ°Ô∏è Stealth Features

### Built-in Stealth (Default)

```python
from news_crawler import NewsCrawler

# All stealth features enabled by default
crawler = NewsCrawler()

# Check stealth status
health = crawler.get_crawler_health()
print(f"Stealth enabled: {health['bypass_available']}")
```

### Advanced Stealth

```python
from news_crawler import create_stealth_crawler

# Maximum stealth for heavily protected sites
crawler = create_stealth_crawler(aggressive=True)
# Uses longer delays, conservative concurrency

# Balanced stealth for normal sites
crawler = create_stealth_crawler(aggressive=False)
# Balanced performance and stealth
```

### Bypass Configuration

```python
config = {
    'enable_bypass': True,
    'bypass_config': {
        'enable_cloudflare_bypass': True,
        'enable_403_handling': True,
        'enable_js_challenge_solving': True,
        'max_retries': 5,
        'exponential_backoff': True
    }
}
```

## üìä Performance

### Typical Performance Metrics

| Scenario | Concurrency | Articles/Hour | Success Rate |
|----------|-------------|---------------|--------------|
| Basic crawling | 5 | 1,000-2,000 | 95%+ |
| Stealth mode | 2 | 300-600 | 90%+ |
| Cloudflare sites | 1 | 200-400 | 85%+ |
| Deep site crawl | 3 | 500-1,000 | 90%+ |

### Performance Monitoring

```python
# Check crawler health
from news_crawler import get_crawler_health

health = get_crawler_health()
print(f"Status: {health['status']}")
print(f"Components: {health}")

# Monitor individual crawler
crawler = NewsCrawler({'enable_monitoring': True})
# ... after crawling
if crawler.performance_monitor:
    stats = crawler.performance_monitor.get_summary()
    print(f"Response time: {stats['avg_response_time']}ms")
```

## üéØ Use Cases

### News Monitoring

```python
async def monitor_news_sites():
    crawler = NewsCrawler({
        'enable_stealth': True,
        'enable_ml_analysis': True
    })
    
    sites = [
        "https://news-site1.com",
        "https://news-site2.com",
        "https://news-site3.com"
    ]
    
    all_articles = []
    for site in sites:
        try:
            articles = await crawler.discover_site_articles(site)
            
            # Process recent articles
            for url in articles[:20]:
                article = await crawler.crawl_url(url)
                if article and article.quality_score > 0.7:
                    all_articles.append(article)
                    
        except Exception as e:
            print(f"Error monitoring {site}: {e}")
    
    return all_articles
```

### Research Collection

```python
from news_crawler import CloudScraperStealthCrawler

async def collect_research_articles():
    crawler = CloudScraperStealthCrawler()
    
    research_sites = [
        "https://research-site1.com",
        "https://research-site2.com"
    ]
    
    all_articles = []
    for site in research_sites:
        articles = await crawler.crawl_entire_site(site, max_articles=50)
        
        # Filter by keywords
        keywords = ["artificial intelligence", "machine learning"]
        relevant = [a for a in articles 
                   if any(kw.lower() in a.content.lower() for kw in keywords)]
        
        all_articles.extend(relevant)
    
    return all_articles
```

## üö® Troubleshooting

### Common Issues

#### 1. Cloudflare Protection
```python
# Enhanced Cloudflare handling
config = {
    'enable_cloudflare_bypass': True,
    'cloudflare_wait_time': 20.0,
    'max_retries': 5
}
```

#### 2. Rate Limiting
```python
# Reduce crawling speed
config = {
    'requests_per_second': 0.5,
    'min_delay': 5.0,
    'max_delay': 10.0,
    'max_concurrent_requests': 1
}
```

#### 3. Content Extraction Issues
```python
# Enable all extraction methods
config = {
    'parser_config': {
        'enable_ml_analysis': True,
        'quality_threshold': 0.3,
        'min_content_length': 100
    }
}
```

### Debug Mode

```python
import logging

# Enable debug logging
logging.basicConfig(level=logging.DEBUG)

# Check component health
health = get_crawler_health()
if health['status'] != 'healthy':
    print(f"Issues: {health}")
```

## üìö Documentation

### Complete Documentation

- **[User Guide](docs/user_guide.md)** - Comprehensive usage guide with examples
- **[Architecture](docs/architecture.md)** - System design and component details
- **[Examples](examples/)** - Working examples for different use cases

### API Reference

#### Core Classes
- `NewsCrawler` - Primary stealth crawler
- `CloudScraperStealthCrawler` - Deep site crawler
- `NewsArticle` - Article data structure
- `BypassManager` - Bypass coordination
- `ContentParser` - Unified content parsing

#### Factory Functions
- `create_news_crawler()` - Create configured crawler
- `create_stealth_crawler()` - Create stealth-optimized crawler
- `get_default_config()` - Get default configuration
- `get_crawler_health()` - Check system health

#### Convenience Functions
- `crawl_url()` - Crawl single URL with automatic cleanup
- `crawl_multiple()` - Crawl multiple URLs with automatic cleanup

## üîß Advanced Configuration

### Site-Specific Configuration (When Needed)

```python
# Custom delays for specific sites
site_configs = {
    'slow-site.com': {
        'min_delay': 8.0,
        'max_delay': 15.0,
        'max_concurrent_requests': 1
    },
    'fast-site.com': {
        'min_delay': 1.0,
        'max_delay': 3.0,
        'max_concurrent_requests': 5
    }
}

# Apply configuration based on URL
def get_site_config(url):
    domain = urlparse(url).netloc
    return site_configs.get(domain, {})
```

### Database Integration

```python
config = {
    'enable_database': True,
    'database_connection_string': 'postgresql://user:pass@localhost/news_db',
    'database_config': {
        'min_connections': 5,
        'max_connections': 20,
        'command_timeout': 60
    }
}

crawler = NewsCrawler(config)
# Articles automatically stored in database
```

### ML Analysis Configuration

```python
config = {
    'enable_ml_analysis': True,
    'ml_config': {
        'confidence_threshold': 0.7,
        'enable_sentiment_analysis': True,
        'enable_entity_extraction': True,
        'enable_topic_modeling': True
    }
}
```

## ü§ù Contributing

We welcome contributions! Please see our contributing guidelines:

1. **Bug Reports**: Submit detailed issues with reproduction steps
2. **Feature Requests**: Propose features with use cases
3. **Code Contributions**: Follow coding standards and include tests
4. **Documentation**: Improve docs and examples

## üìÑ License

This project is licensed under the MIT License.

## üÜò Support

For support:
1. Check the [User Guide](docs/user_guide.md) and [examples](examples/)
2. Review troubleshooting section above
3. Check component health with `get_crawler_health()`
4. Submit GitHub issues for bugs or questions

---

## üîó Integration with Lindela Ecosystem

This news crawler is part of the Lindela packages_enhanced system:

- **Utils/Config**: Unified configuration management
- **Database**: PostgreSQL integration for persistence
- **ML Scoring**: Content analysis and quality assessment
- **Monitoring**: Performance tracking and health checks

### Ecosystem Benefits

- **Consistent Configuration**: Same config patterns across all packages
- **Shared Utilities**: Reuse of common functionality
- **Integrated Monitoring**: Unified performance tracking
- **Enterprise Features**: Professional-grade reliability

---

**Ready for production news crawling with advanced stealth! üì∞üõ°Ô∏è**

*Building intelligent content acquisition systems for informed decision making.*