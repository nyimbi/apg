"""
APG Workflow Neural Architecture Search (NAS)

Advanced neural architecture search for automated deep learning model design
and optimization in workflow orchestration systems.

Â© 2025 Datacraft. All rights reserved.
Author: Nyimbi Odero <nyimbi@gmail.com>
"""

import asyncio
import logging
import json
import pickle
from typing import Dict, List, Optional, Any, Union, Tuple, Callable
from datetime import datetime, timezone, timedelta
from dataclasses import dataclass, field
from pathlib import Path
import hashlib
import numpy as np
import random
from concurrent.futures import ThreadPoolExecutor
import threading

from pydantic import BaseModel, Field, ConfigDict
from sqlalchemy.orm import Session
from uuid_extensions import uuid7str

# ML Libraries
try:
	import torch
	import torch.nn as nn
	import torch.optim as optim
	from torch.utils.data import DataLoader, Dataset, TensorDataset
	import torch.nn.functional as F
	TORCH_AVAILABLE = True
except ImportError:
	TORCH_AVAILABLE = False

try:
	import tensorflow as tf
	from tensorflow import keras
	TF_AVAILABLE = True
except ImportError:
	TF_AVAILABLE = False

# NAS Libraries
try:
	import nni
	from nni.nas.pytorch import mutables
	from nni.algorithms.nas.pytorch import DartsTrainer, ProxylessTrainer
	NNI_AVAILABLE = True
except ImportError:
	NNI_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class ArchitectureGenotype:
	"""Neural architecture genotype representation."""
	genotype_id: str = field(default_factory=uuid7str)
	
	# Architecture structure
	normal_cells: List[Dict[str, Any]] = field(default_factory=list)
	reduction_cells: List[Dict[str, Any]] = field(default_factory=list)
	
	# Network configuration
	layers: int = 8
	channels: int = 16
	input_size: Tuple[int, ...] = (224, 224, 3)
	num_classes: int = 10
	
	# Performance metrics
	accuracy: float = 0.0
	latency_ms: float = 0.0
	flops: int = 0
	params: int = 0
	memory_mb: float = 0.0
	
	# Training info
	epochs_trained: int = 0
	best_epoch: int = 0
	training_time: float = 0.0
	
	# Metadata
	created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
	search_space: str = "DARTS"
	fitness_score: float = 0.0

@dataclass
class SearchSpace:
	"""Neural architecture search space definition."""
	space_id: str = field(default_factory=uuid7str)
	name: str = ""
	
	# Operations
	operations: List[str] = field(default_factory=list)
	connections: List[Tuple[int, int]] = field(default_factory=list)
	
	# Constraints
	max_layers: int = 20
	max_channels: int = 512
	max_params: int = 50_000_000
	max_flops: int = 600_000_000
	
	# Search configuration
	population_size: int = 50
	generations: int = 100
	mutation_rate: float = 0.1
	crossover_rate: float = 0.8

class NASMethod(str):
	"""Neural Architecture Search methods."""
	DIFFERENTIABLE = "differentiable"  # DARTS, PC-DARTS, etc.
	EVOLUTIONARY = "evolutionary"      # NSGA-II, ENAS, etc.
	REINFORCEMENT = "reinforcement"    # NASNet, ENAS, etc.
	RANDOM = "random"                  # Random search
	BAYESIAN = "bayesian"              # BOHB, etc.
	PROGRESSIVE = "progressive"        # Progressive search

class NASConfig(BaseModel):
	"""Neural Architecture Search configuration."""
	
	model_config = ConfigDict(extra='forbid', validate_by_name=True, validate_by_alias=True)
	
	method: NASMethod = Field(default=NASMethod.DIFFERENTIABLE, description="NAS method")
	search_space: str = Field(default="DARTS", description="Search space name")
	
	# Search parameters
	max_epochs: int = Field(default=50, ge=1, le=500, description="Maximum training epochs")
	population_size: int = Field(default=50, ge=10, le=500, description="Population size for evolutionary methods")
	generations: int = Field(default=100, ge=10, le=1000, description="Number of generations")
	
	# Architecture constraints
	max_layers: int = Field(default=20, ge=1, le=100, description="Maximum number of layers")
	max_params: int = Field(default=10_000_000, ge=1000, le=100_000_000, description="Maximum parameters")
	max_flops: int = Field(default=600_000_000, ge=1000, le=10_000_000_000, description="Maximum FLOPs")
	
	# Performance targets
	target_accuracy: float = Field(default=0.95, ge=0.0, le=1.0, description="Target accuracy")
	max_latency_ms: float = Field(default=100.0, ge=1.0, le=10000.0, description="Maximum latency in ms")
	max_memory_mb: float = Field(default=500.0, ge=1.0, le=10000.0, description="Maximum memory in MB")
	
	# Multi-objective weights
	accuracy_weight: float = Field(default=0.5, ge=0.0, le=1.0, description="Accuracy weight")
	efficiency_weight: float = Field(default=0.3, ge=0.0, le=1.0, description="Efficiency weight")
	latency_weight: float = Field(default=0.2, ge=0.0, le=1.0, description="Latency weight")
	
	# Training configuration
	batch_size: int = Field(default=32, ge=1, le=512, description="Training batch size")
	learning_rate: float = Field(default=0.001, ge=0.0001, le=0.1, description="Learning rate")
	weight_decay: float = Field(default=0.0005, ge=0.0, le=0.01, description="Weight decay")

class NeuralArchitectureSearch:
	"""
	Neural Architecture Search system for automated deep learning model design.
	
	Features:
	- Multiple NAS algorithms (DARTS, Evolutionary, RL-based)
	- Custom search spaces
	- Multi-objective optimization
	- Hardware-aware search
	- Progressive search strategies
	- Architecture performance prediction
	"""
	
	def __init__(self, config: NASConfig, db_session: Session):
		self.config = config
		self.db_session = db_session
		
		# Search state
		self.current_search_id: Optional[str] = None
		self.search_history: Dict[str, List[ArchitectureGenotype]] = {}
		self.best_architectures: Dict[str, ArchitectureGenotype] = {}
		
		# Search spaces
		self.search_spaces: Dict[str, SearchSpace] = {}
		self._initialize_search_spaces()
		
		# Performance optimization
		self.executor = ThreadPoolExecutor(max_workers=4)
		self.architecture_cache: Dict[str, ArchitectureGenotype] = {}
		self.cache_lock = threading.Lock()
		
		# Device configuration
		self.device = torch.device('cuda' if torch.cuda.is_available() and TORCH_AVAILABLE else 'cpu')
		
		logger.info(f"NAS system initialized with device: {self.device}")
	
	def _initialize_search_spaces(self) -> None:
		"""Initialize predefined search spaces."""
		# DARTS search space
		darts_space = SearchSpace(
			name="DARTS",
			operations=[
				"none", "max_pool_3x3", "avg_pool_3x3", "skip_connect",
				"sep_conv_3x3", "sep_conv_5x5", "dil_conv_3x3", "dil_conv_5x5"
			],
			max_layers=8,
			max_channels=64
		)
		self.search_spaces["DARTS"] = darts_space
		
		# MobileNet search space
		mobilenet_space = SearchSpace(
			name="MobileNet",
			operations=[
				"mb_conv_3x3", "mb_conv_5x5", "mb_conv_7x7", "skip_connect",
				"dw_conv_3x3", "dw_conv_5x5", "se_block"
			],
			max_layers=16,
			max_channels=320
		)
		self.search_spaces["MobileNet"] = mobilenet_space
		
		# EfficientNet search space
		efficientnet_space = SearchSpace(
			name="EfficientNet",
			operations=[
				"mb_conv_3x3", "mb_conv_5x5", "mb_conv_7x7", "se_block",
				"swish", "identity", "drop_connect"
			],
			max_layers=32,
			max_channels=512
		)
		self.search_spaces["EfficientNet"] = efficientnet_space
	
	async def run_architecture_search(self, 
									  dataset_config: Dict[str, Any], 
									  search_name: str = "") -> str:
		"""Run neural architecture search."""
		search_id = uuid7str()
		self.current_search_id = search_id
		
		search_name = search_name or f"NAS_{self.config.method}_{search_id[:8]}"
		
		try:
			logger.info(f"Starting NAS with method: {self.config.method}")
			
			# Load dataset
			train_loader, val_loader = await self._prepare_dataset(dataset_config)
			
			# Initialize search history
			self.search_history[search_id] = []
			
			# Run search based on method
			if self.config.method == NASMethod.DIFFERENTIABLE:
				best_arch = await self._run_differentiable_search(train_loader, val_loader, search_id)
			elif self.config.method == NASMethod.EVOLUTIONARY:
				best_arch = await self._run_evolutionary_search(train_loader, val_loader, search_id)
			elif self.config.method == NASMethod.REINFORCEMENT:
				best_arch = await self._run_reinforcement_search(train_loader, val_loader, search_id)
			elif self.config.method == NASMethod.RANDOM:
				best_arch = await self._run_random_search(train_loader, val_loader, search_id)
			elif self.config.method == NASMethod.BAYESIAN:
				best_arch = await self._run_bayesian_search(train_loader, val_loader, search_id)
			elif self.config.method == NASMethod.PROGRESSIVE:
				best_arch = await self._run_progressive_search(train_loader, val_loader, search_id)
			else:
				raise ValueError(f"Unsupported NAS method: {self.config.method}")
			
			# Store best architecture
			self.best_architectures[search_id] = best_arch
			
			# Save search results
			await self._save_search_results(search_id, search_name)
			
			logger.info(f"NAS completed: {search_id}, best accuracy: {best_arch.accuracy:.4f}")
			return search_id
			
		except Exception as e:
			logger.error(f"NAS failed: {e}")
			raise
	
	async def _run_differentiable_search(self, 
										 train_loader: DataLoader, 
										 val_loader: DataLoader, 
										 search_id: str) -> ArchitectureGenotype:
		"""Run differentiable architecture search (DARTS)."""
		if not TORCH_AVAILABLE:
			raise ValueError("PyTorch is required for differentiable NAS")
		
		# Create supernet
		supernet = await self._create_darts_supernet()
		supernet.to(self.device)
		
		# Optimizers
		model_optimizer = optim.SGD(
			supernet.model_parameters(),
			lr=self.config.learning_rate,
			momentum=0.9,
			weight_decay=self.config.weight_decay
		)
		
		arch_optimizer = optim.Adam(
			supernet.arch_parameters(),
			lr=3e-4,
			betas=(0.5, 0.999),
			weight_decay=1e-3
		)
		
		best_arch = None
		best_accuracy = 0.0
		
		# Training loop
		for epoch in range(self.config.max_epochs):
			# Train model weights
			supernet.train()
			for batch_idx, (data, target) in enumerate(train_loader):
				data, target = data.to(self.device), target.to(self.device)
				
				# Model step
				model_optimizer.zero_grad()
				output = supernet(data)
				model_loss = F.cross_entropy(output, target)
				model_loss.backward()
				model_optimizer.step()
				
				# Architecture step (alternate with model step)
				if batch_idx % 2 == 1:
					arch_optimizer.zero_grad()
					output = supernet(data)
					arch_loss = F.cross_entropy(output, target)
					arch_loss.backward()
					arch_optimizer.step()
			
			# Validation
			accuracy = await self._evaluate_architecture(supernet, val_loader)
			
			# Create architecture genotype
			genotype = await self._extract_genotype(supernet, accuracy, epoch)
			self.search_history[search_id].append(genotype)
			
			if accuracy > best_accuracy:
				best_accuracy = accuracy
				best_arch = genotype
			
			logger.info(f"Epoch {epoch}: accuracy={accuracy:.4f}, best={best_accuracy:.4f}")
		
		return best_arch or self.search_history[search_id][-1]
	
	async def _run_evolutionary_search(self, 
									   train_loader: DataLoader, 
									   val_loader: DataLoader, 
									   search_id: str) -> ArchitectureGenotype:
		"""Run evolutionary architecture search."""
		# Initialize population
		population = []
		for _ in range(self.config.population_size):
			arch = await self._generate_random_architecture()
			arch.fitness_score = await self._evaluate_architecture_fitness(arch, train_loader, val_loader)
			population.append(arch)
			self.search_history[search_id].append(arch)
		
		best_arch = max(population, key=lambda x: x.fitness_score)
		
		# Evolutionary loop
		for generation in range(self.config.generations):
			# Selection
			selected = await self._tournament_selection(population, tournament_size=3)
			
			# Crossover and mutation
			offspring = []
			for i in range(0, len(selected), 2):
				if i + 1 < len(selected):
					parent1, parent2 = selected[i], selected[i + 1]
					
					# Crossover
					if random.random() < 0.8:  # crossover rate
						child1, child2 = await self._crossover(parent1, parent2)
					else:
						child1, child2 = parent1, parent2
					
					# Mutation
					if random.random() < 0.1:  # mutation rate
						child1 = await self._mutate(child1)
					if random.random() < 0.1:
						child2 = await self._mutate(child2)
					
					offspring.extend([child1, child2])
			
			# Evaluate offspring
			for arch in offspring:
				arch.fitness_score = await self._evaluate_architecture_fitness(arch, train_loader, val_loader)
				self.search_history[search_id].append(arch)
			
			# Environmental selection (keep best individuals)
			population = sorted(population + offspring, key=lambda x: x.fitness_score, reverse=True)
			population = population[:self.config.population_size]
			
			current_best = population[0]
			if current_best.fitness_score > best_arch.fitness_score:
				best_arch = current_best
			
			logger.info(f"Generation {generation}: best_fitness={best_arch.fitness_score:.4f}")
		
		return best_arch
	
	async def _run_reinforcement_search(self, 
										train_loader: DataLoader, 
										val_loader: DataLoader, 
										search_id: str) -> ArchitectureGenotype:
		"""Run reinforcement learning-based architecture search."""
		if not TORCH_AVAILABLE:
			raise ValueError("PyTorch is required for RL-based NAS")
		
		# Simple RL controller (simplified ENAS approach)
		controller = await self._create_rl_controller()
		controller.to(self.device)
		
		controller_optimizer = optim.Adam(controller.parameters(), lr=3.5e-4)
		
		best_arch = None
		best_accuracy = 0.0
		
		# Training loop
		for episode in range(self.config.max_epochs):
			# Sample architecture from controller
			arch_sequence = await self._sample_architecture(controller)
			
			# Build and evaluate architecture
			arch = await self._build_architecture_from_sequence(arch_sequence)
			accuracy = await self._quick_evaluate_architecture(arch, train_loader, val_loader)
			
			arch.accuracy = accuracy
			arch.fitness_score = accuracy
			self.search_history[search_id].append(arch)
			
			# Update best architecture
			if accuracy > best_accuracy:
				best_accuracy = accuracy
				best_arch = arch
			
			# Update controller using REINFORCE
			reward = accuracy  # Use accuracy as reward
			baseline = 0.8  # Moving average baseline (simplified)
			
			# Calculate policy gradient loss
			log_probs = await self._get_log_probabilities(controller, arch_sequence)
			loss = -torch.sum(log_probs * (reward - baseline))
			
			controller_optimizer.zero_grad()
			loss.backward()
			controller_optimizer.step()
			
			logger.info(f"Episode {episode}: accuracy={accuracy:.4f}, reward={reward:.4f}")
		
		return best_arch or self.search_history[search_id][-1]
	
	async def _run_random_search(self, 
								 train_loader: DataLoader, 
								 val_loader: DataLoader, 
								 search_id: str) -> ArchitectureGenotype:
		"""Run random architecture search."""
		best_arch = None
		best_accuracy = 0.0
		
		for iteration in range(self.config.population_size):
			# Generate random architecture
			arch = await self._generate_random_architecture()
			
			# Evaluate architecture
			accuracy = await self._quick_evaluate_architecture(arch, train_loader, val_loader)
			arch.accuracy = accuracy
			arch.fitness_score = accuracy
			
			self.search_history[search_id].append(arch)
			
			if accuracy > best_accuracy:
				best_accuracy = accuracy
				best_arch = arch
			
			logger.info(f"Random search {iteration}: accuracy={accuracy:.4f}, best={best_accuracy:.4f}")
		
		return best_arch or self.search_history[search_id][-1]
	
	async def _run_bayesian_search(self, 
								   train_loader: DataLoader, 
								   val_loader: DataLoader, 
								   search_id: str) -> ArchitectureGenotype:
		"""Run Bayesian optimization-based architecture search."""
		try:
			from sklearn.gaussian_process import GaussianProcessRegressor
			from sklearn.gaussian_process.kernels import Matern
			from sklearn.preprocessing import StandardScaler
		except ImportError:
			logger.warning("scikit-learn required for Bayesian search, falling back to random search")
			return await self._run_random_search(train_loader, val_loader, search_id)
		
		# Initialize with random architectures
		architectures = []
		performances = []
		
		# Initial random sampling
		n_initial = 10
		for _ in range(n_initial):
			arch = await self._generate_random_architecture()
			accuracy = await self._quick_evaluate_architecture(arch, train_loader, val_loader)
			
			arch_vector = await self._architecture_to_vector(arch)
			architectures.append(arch_vector)
			performances.append(accuracy)
			
			arch.accuracy = accuracy
			arch.fitness_score = accuracy
			self.search_history[search_id].append(arch)
		
		# Gaussian Process model
		scaler = StandardScaler()
		X = scaler.fit_transform(architectures)
		y = np.array(performances)
		
		kernel = Matern(length_scale=1.0, nu=2.5)
		gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)
		
		best_arch = max(self.search_history[search_id], key=lambda x: x.accuracy)
		
		# Bayesian optimization loop
		for iteration in range(self.config.population_size - n_initial):
			gp.fit(X, y)
			
			# Acquisition function (Upper Confidence Bound)
			candidates = []
			for _ in range(100):  # Sample candidates
				candidate_arch = await self._generate_random_architecture()
				candidate_vector = await self._architecture_to_vector(candidate_arch)
				candidates.append((candidate_arch, candidate_vector))
			
			# Evaluate acquisition function
			best_candidate = None
			best_acquisition = -np.inf
			
			for arch, vector in candidates:
				vector_scaled = scaler.transform([vector])
				mean, std = gp.predict(vector_scaled, return_std=True)
				acquisition = mean + 2.0 * std  # UCB with beta=2
				
				if acquisition > best_acquisition:
					best_acquisition = acquisition
					best_candidate = arch
			
			# Evaluate selected architecture
			accuracy = await self._quick_evaluate_architecture(best_candidate, train_loader, val_loader)
			best_candidate.accuracy = accuracy
			best_candidate.fitness_score = accuracy
			
			# Update data
			candidate_vector = await self._architecture_to_vector(best_candidate)
			architectures.append(candidate_vector)
			performances.append(accuracy)
			
			X = scaler.fit_transform(architectures)
			y = np.array(performances)
			
			self.search_history[search_id].append(best_candidate)
			
			if accuracy > best_arch.accuracy:
				best_arch = best_candidate
			
			logger.info(f"Bayesian iteration {iteration}: accuracy={accuracy:.4f}, best={best_arch.accuracy:.4f}")
		
		return best_arch
	
	async def _run_progressive_search(self, 
									  train_loader: DataLoader, 
									  val_loader: DataLoader, 
									  search_id: str) -> ArchitectureGenotype:
		"""Run progressive architecture search."""
		# Start with simple architectures and progressively increase complexity
		best_arch = None
		best_accuracy = 0.0
		
		# Progressive stages
		complexity_stages = [
			{'max_layers': 4, 'max_channels': 32},
			{'max_layers': 8, 'max_channels': 64},
			{'max_layers': 12, 'max_channels': 128},
			{'max_layers': 16, 'max_channels': 256}
		]
		
		for stage_idx, stage_config in enumerate(complexity_stages):
			logger.info(f"Progressive stage {stage_idx + 1}: {stage_config}")
			
			# Generate architectures for this complexity level
			stage_architectures = []
			for _ in range(self.config.population_size // len(complexity_stages)):
				arch = await self._generate_random_architecture(complexity_constraints=stage_config)
				accuracy = await self._quick_evaluate_architecture(arch, train_loader, val_loader)
				
				arch.accuracy = accuracy
				arch.fitness_score = accuracy
				stage_architectures.append(arch)
				self.search_history[search_id].append(arch)
			
			# Find best in this stage
			stage_best = max(stage_architectures, key=lambda x: x.accuracy)
			if stage_best.accuracy > best_accuracy:
				best_accuracy = stage_best.accuracy
				best_arch = stage_best
			
			logger.info(f"Stage {stage_idx + 1} best: {stage_best.accuracy:.4f}")
		
		return best_arch or self.search_history[search_id][-1]
	
	async def _prepare_dataset(self, dataset_config: Dict[str, Any]) -> Tuple[DataLoader, DataLoader]:
		"""Prepare dataset for NAS."""
		# This is a simplified implementation
		# In practice, you would load actual dataset based on config
		
		# Generate synthetic data for demonstration
		batch_size = self.config.batch_size
		
		# Create synthetic CIFAR-10 like dataset
		train_data = torch.randn(1000, 3, 32, 32)
		train_targets = torch.randint(0, 10, (1000,))
		
		val_data = torch.randn(200, 3, 32, 32)
		val_targets = torch.randint(0, 10, (200,))
		
		train_dataset = TensorDataset(train_data, train_targets)
		val_dataset = TensorDataset(val_data, val_targets)
		
		train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
		val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
		
		return train_loader, val_loader
	
	async def _create_darts_supernet(self):
		"""Create DARTS supernet."""
		if not TORCH_AVAILABLE:
			raise ValueError("PyTorch required for DARTS supernet")
		
		class DARTSSupernet(nn.Module):
			def __init__(self, num_classes=10):
				super().__init__()
				self.num_classes = num_classes
				
				# Simplified supernet architecture
				self.stem = nn.Sequential(
					nn.Conv2d(3, 16, 3, padding=1, bias=False),
					nn.BatchNorm2d(16),
					nn.ReLU(inplace=True)
				)
				
				# Architecture parameters (alpha)
				self.alphas_normal = nn.Parameter(torch.randn(4, 8))  # 4 edges, 8 operations
				self.alphas_reduce = nn.Parameter(torch.randn(4, 8))
				
				# Cells
				self.cells = nn.ModuleList()
				for i in range(8):
					self.cells.append(self._make_cell(16, 16, i == 2 or i == 5))
				
				self.global_pooling = nn.AdaptiveAvgPool2d(1)
				self.classifier = nn.Linear(16, num_classes)
			
			def _make_cell(self, in_channels, out_channels, reduction=False):
				# Simplified cell implementation
				if reduction:
					return nn.Sequential(
						nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1, bias=False),
						nn.BatchNorm2d(out_channels),
						nn.ReLU(inplace=True)
					)
				else:
					return nn.Sequential(
						nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
						nn.BatchNorm2d(out_channels),
						nn.ReLU(inplace=True)
					)
			
			def forward(self, x):
				x = self.stem(x)
				
				for cell in self.cells:
					x = cell(x)
				
				x = self.global_pooling(x)
				x = x.view(x.size(0), -1)
				x = self.classifier(x)
				
				return x
			
			def arch_parameters(self):
				return [self.alphas_normal, self.alphas_reduce]
			
			def model_parameters(self):
				return [p for p in self.parameters() if p not in self.arch_parameters()]
		
		return DARTSSupernet()
	
	async def _create_rl_controller(self):
		"""Create RL controller for architecture sampling."""
		if not TORCH_AVAILABLE:
			raise ValueError("PyTorch required for RL controller")
		
		class RLController(nn.Module):
			def __init__(self, num_layers=6, num_ops=8):
				super().__init__()
				self.num_layers = num_layers
				self.num_ops = num_ops
				
				self.lstm = nn.LSTM(32, 64, batch_first=True)
				self.embedding = nn.Embedding(num_ops, 32)
				self.decoder = nn.Linear(64, num_ops)
			
			def forward(self, sequence_length=6):
				batch_size = 1
				hidden = self.init_hidden(batch_size)
				
				# Start token
				inputs = torch.zeros(batch_size, 1, dtype=torch.long)
				
				outputs = []
				for _ in range(sequence_length):
					embedded = self.embedding(inputs)
					lstm_out, hidden = self.lstm(embedded, hidden)
					logits = self.decoder(lstm_out)
					outputs.append(logits)
					
					# Sample next input
					probs = F.softmax(logits, dim=-1)
					inputs = torch.multinomial(probs.squeeze(1), 1)
				
				return torch.cat(outputs, dim=1)
			
			def init_hidden(self, batch_size):
				h0 = torch.zeros(1, batch_size, 64)
				c0 = torch.zeros(1, batch_size, 64)
				return (h0, c0)
		
		return RLController()
	
	async def _evaluate_architecture(self, model, val_loader) -> float:
		"""Evaluate architecture performance."""
		if not TORCH_AVAILABLE:
			return 0.5  # Fallback
		
		model.eval()
		correct = 0
		total = 0
		
		with torch.no_grad():
			for data, target in val_loader:
				data, target = data.to(self.device), target.to(self.device)
				output = model(data)
				pred = output.argmax(dim=1)
				correct += pred.eq(target).sum().item()
				total += target.size(0)
		
		return correct / total if total > 0 else 0.0
	
	async def _quick_evaluate_architecture(self, arch: ArchitectureGenotype, train_loader, val_loader) -> float:
		"""Quick evaluation of architecture (simplified training)."""
		# Build model from architecture
		model = await self._build_model_from_genotype(arch)
		if model is None:
			return 0.0
		
		model.to(self.device)
		
		# Quick training (few epochs)
		optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
		criterion = nn.CrossEntropyLoss()
		
		model.train()
		for epoch in range(3):  # Quick training
			for batch_idx, (data, target) in enumerate(train_loader):
				if batch_idx > 10:  # Limit batches for speed
					break
				
				data, target = data.to(self.device), target.to(self.device)
				optimizer.zero_grad()
				output = model(data)
				loss = criterion(output, target)
				loss.backward()
				optimizer.step()
		
		# Evaluate
		return await self._evaluate_architecture(model, val_loader)
	
	async def _generate_random_architecture(self, complexity_constraints: Optional[Dict[str, Any]] = None) -> ArchitectureGenotype:
		"""Generate random architecture genotype."""
		constraints = complexity_constraints or {}
		
		max_layers = constraints.get('max_layers', self.config.max_layers)
		max_channels = constraints.get('max_channels', 64)
		
		# Random architecture parameters
		layers = random.randint(4, min(max_layers, 16))
		channels = random.choice([16, 32, 64, min(max_channels, 128)])
		
		# Random cell structures (simplified)
		normal_cells = []
		reduction_cells = []
		
		operations = ["conv_3x3", "conv_5x5", "max_pool", "avg_pool", "skip", "sep_conv_3x3"]
		
		for _ in range(layers):
			normal_cells.append({
				'operation': random.choice(operations),
				'channels': channels
			})
			
			if len(reduction_cells) < 2:  # Limit reduction cells
				reduction_cells.append({
					'operation': random.choice(operations),
					'channels': channels * 2
				})
		
		return ArchitectureGenotype(
			normal_cells=normal_cells,
			reduction_cells=reduction_cells,
			layers=layers,
			channels=channels
		)
	
	async def _build_model_from_genotype(self, genotype: ArchitectureGenotype):
		"""Build PyTorch model from genotype."""
		if not TORCH_AVAILABLE:
			return None
		
		class SimpleNet(nn.Module):
			def __init__(self, genotype):
				super().__init__()
				self.layers = nn.ModuleList()
				
				# Build layers from genotype
				in_channels = 3
				for cell in genotype.normal_cells:
					out_channels = cell['channels']
					operation = cell['operation']
					
					if operation == "conv_3x3":
						layer = nn.Conv2d(in_channels, out_channels, 3, padding=1)
					elif operation == "conv_5x5":
						layer = nn.Conv2d(in_channels, out_channels, 5, padding=2)
					elif operation == "sep_conv_3x3":
						layer = nn.Sequential(
							nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels),
							nn.Conv2d(in_channels, out_channels, 1)
						)
					elif operation == "max_pool":
						layer = nn.MaxPool2d(3, stride=1, padding=1)
						out_channels = in_channels  # No channel change
					elif operation == "avg_pool":
						layer = nn.AvgPool2d(3, stride=1, padding=1)
						out_channels = in_channels  # No channel change
					else:  # skip
						layer = nn.Identity()
						out_channels = in_channels
					
					self.layers.append(layer)
					if operation not in ["max_pool", "avg_pool", "skip"]:
						self.layers.append(nn.BatchNorm2d(out_channels))
						self.layers.append(nn.ReLU(inplace=True))
					
					in_channels = out_channels
				
				self.global_pool = nn.AdaptiveAvgPool2d(1)
				self.classifier = nn.Linear(in_channels, 10)  # 10 classes
			
			def forward(self, x):
				for layer in self.layers:
					x = layer(x)
				x = self.global_pool(x)
				x = x.view(x.size(0), -1)
				x = self.classifier(x)
				return x
		
		return SimpleNet(genotype)
	
	async def _extract_genotype(self, supernet, accuracy: float, epoch: int) -> ArchitectureGenotype:
		"""Extract genotype from trained supernet."""
		# Simplified genotype extraction
		genotype = ArchitectureGenotype(
			accuracy=accuracy,
			epochs_trained=epoch + 1,
			best_epoch=epoch,
			search_space="DARTS"
		)
		
		# Extract architecture from alphas (simplified)
		if hasattr(supernet, 'alphas_normal'):
			# Get dominant operations
			normal_ops = torch.argmax(supernet.alphas_normal, dim=-1).cpu().numpy()
			operations = ["none", "max_pool", "avg_pool", "skip", "sep_conv_3x3", "sep_conv_5x5", "dil_conv_3x3", "dil_conv_5x5"]
			
			genotype.normal_cells = [
				{'operation': operations[op], 'channels': 16}
				for op in normal_ops
			]
		
		return genotype
	
	async def _evaluate_architecture_fitness(self, arch: ArchitectureGenotype, train_loader, val_loader) -> float:
		"""Evaluate architecture fitness (multi-objective)."""
		# Quick evaluation
		accuracy = await self._quick_evaluate_architecture(arch, train_loader, val_loader)
		
		# Estimate efficiency metrics (simplified)
		efficiency_score = 1.0 / (1.0 + arch.layers * 0.1)  # Simpler is better
		latency_score = 1.0 / (1.0 + arch.channels * 0.001)  # Fewer channels = faster
		
		# Multi-objective fitness
		fitness = (
			self.config.accuracy_weight * accuracy +
			self.config.efficiency_weight * efficiency_score +
			self.config.latency_weight * latency_score
		)
		
		arch.accuracy = accuracy
		return fitness
	
	async def _tournament_selection(self, population: List[ArchitectureGenotype], tournament_size: int = 3) -> List[ArchitectureGenotype]:
		"""Tournament selection for evolutionary search."""
		selected = []
		
		for _ in range(len(population)):
			tournament = random.sample(population, min(tournament_size, len(population)))
			winner = max(tournament, key=lambda x: x.fitness_score)
			selected.append(winner)
		
		return selected
	
	async def _crossover(self, parent1: ArchitectureGenotype, parent2: ArchitectureGenotype) -> Tuple[ArchitectureGenotype, ArchitectureGenotype]:
		"""Crossover operation for evolutionary search."""
		# Simple crossover: exchange parts of architectures
		child1 = ArchitectureGenotype(
			normal_cells=parent1.normal_cells[:len(parent1.normal_cells)//2] + parent2.normal_cells[len(parent2.normal_cells)//2:],
			reduction_cells=parent1.reduction_cells,
			layers=parent1.layers,
			channels=parent2.channels
		)
		
		child2 = ArchitectureGenotype(
			normal_cells=parent2.normal_cells[:len(parent2.normal_cells)//2] + parent1.normal_cells[len(parent1.normal_cells)//2:],
			reduction_cells=parent2.reduction_cells,
			layers=parent2.layers,
			channels=parent1.channels
		)
		
		return child1, child2
	
	async def _mutate(self, architecture: ArchitectureGenotype) -> ArchitectureGenotype:
		"""Mutation operation for evolutionary search."""
		mutated = ArchitectureGenotype(
			normal_cells=architecture.normal_cells.copy(),
			reduction_cells=architecture.reduction_cells.copy(),
			layers=architecture.layers,
			channels=architecture.channels
		)
		
		# Random mutations
		if random.random() < 0.3 and mutated.normal_cells:
			# Mutate a random cell
			idx = random.randint(0, len(mutated.normal_cells) - 1)
			operations = ["conv_3x3", "conv_5x5", "max_pool", "avg_pool", "skip", "sep_conv_3x3"]
			mutated.normal_cells[idx]['operation'] = random.choice(operations)
		
		if random.random() < 0.2:
			# Mutate channels
			mutated.channels = random.choice([16, 32, 64, 128])
		
		return mutated
	
	async def _sample_architecture(self, controller) -> List[int]:
		"""Sample architecture sequence from RL controller."""
		if not TORCH_AVAILABLE:
			return [0, 1, 2, 3, 4, 5]  # Fallback
		
		controller.eval()
		with torch.no_grad():
			outputs = controller()
			probs = F.softmax(outputs, dim=-1)
			sequence = torch.multinomial(probs.squeeze(0), 1).squeeze(-1).tolist()
		
		return sequence
	
	async def _build_architecture_from_sequence(self, sequence: List[int]) -> ArchitectureGenotype:
		"""Build architecture genotype from sequence."""
		operations = ["conv_3x3", "conv_5x5", "max_pool", "avg_pool", "skip", "sep_conv_3x3", "dil_conv_3x3", "none"]
		
		normal_cells = []
		for i, op_idx in enumerate(sequence):
			if op_idx < len(operations):
				normal_cells.append({
					'operation': operations[op_idx],
					'channels': 32
				})
		
		return ArchitectureGenotype(
			normal_cells=normal_cells,
			reduction_cells=[],
			layers=len(sequence),
			channels=32
		)
	
	async def _get_log_probabilities(self, controller, sequence: List[int]) -> torch.Tensor:
		"""Get log probabilities for sequence from controller."""
		if not TORCH_AVAILABLE:
			return torch.tensor(0.0)
		
		controller.eval()
		outputs = controller()
		log_probs = F.log_softmax(outputs, dim=-1)
		
		sequence_log_probs = []
		for i, action in enumerate(sequence):
			if i < log_probs.size(1) and action < log_probs.size(-1):
				sequence_log_probs.append(log_probs[0, i, action])
		
		return torch.stack(sequence_log_probs) if sequence_log_probs else torch.tensor(0.0)
	
	async def _architecture_to_vector(self, arch: ArchitectureGenotype) -> List[float]:
		"""Convert architecture to feature vector for Bayesian optimization."""
		vector = []
		
		# Architecture features
		vector.append(float(arch.layers))
		vector.append(float(arch.channels))
		
		# Operation counts
		operations = ["conv_3x3", "conv_5x5", "max_pool", "avg_pool", "skip", "sep_conv_3x3"]
		op_counts = {op: 0 for op in operations}
		
		for cell in arch.normal_cells:
			op = cell.get('operation', 'none')
			if op in op_counts:
				op_counts[op] += 1
		
		vector.extend([float(count) for count in op_counts.values()])
		
		return vector
	
	async def _save_search_results(self, search_id: str, search_name: str) -> None:
		"""Save search results to disk."""
		try:
			results_dir = Path('./nas_results')
			results_dir.mkdir(exist_ok=True)
			
			results = {
				'search_id': search_id,
				'search_name': search_name,
				'config': self.config.model_dump(),
				'history': [arch.__dict__ for arch in self.search_history.get(search_id, [])],
				'best_architecture': self.best_architectures[search_id].__dict__ if search_id in self.best_architectures else None
			}
			
			# Convert datetime objects to strings
			def convert_datetime(obj):
				if isinstance(obj, datetime):
					return obj.isoformat()
				elif isinstance(obj, dict):
					return {k: convert_datetime(v) for k, v in obj.items()}
				elif isinstance(obj, list):
					return [convert_datetime(item) for item in obj]
				return obj
			
			results = convert_datetime(results)
			
			results_file = results_dir / f"nas_results_{search_id}.json"
			with open(results_file, 'w') as f:
				json.dump(results, f, indent=2)
			
			logger.info(f"NAS results saved: {results_file}")
			
		except Exception as e:
			logger.error(f"Failed to save NAS results: {e}")
	
	def get_search_status(self, search_id: str) -> Dict[str, Any]:
		"""Get search status and progress."""
		if search_id not in self.search_history:
			return {'error': 'Search not found'}
		
		history = self.search_history[search_id]
		best_arch = self.best_architectures.get(search_id)
		
		return {
			'search_id': search_id,
			'method': self.config.method,
			'architectures_evaluated': len(history),
			'best_accuracy': best_arch.accuracy if best_arch else 0.0,
			'best_architecture': {
				'layers': best_arch.layers,
				'channels': best_arch.channels,
				'params': best_arch.params,
				'flops': best_arch.flops
			} if best_arch else None,
			'search_progress': min(100.0, (len(history) / self.config.population_size) * 100),
			'status': 'completed' if search_id in self.best_architectures else 'running'
		}
	
	def list_search_history(self) -> List[Dict[str, Any]]:
		"""List all search history."""
		searches = []
		for search_id, history in self.search_history.items():
			best_arch = self.best_architectures.get(search_id)
			searches.append({
				'search_id': search_id,
				'method': self.config.method,
				'architectures_count': len(history),
				'best_accuracy': best_arch.accuracy if best_arch else 0.0,
				'created_at': history[0].created_at.isoformat() if history else None
			})
		
		return sorted(searches, key=lambda x: x['created_at'] or '', reverse=True)
	
	async def cleanup_search_data(self, older_than_days: int = 30) -> Dict[str, int]:
		"""Cleanup old search data."""
		cutoff_date = datetime.now(timezone.utc) - timedelta(days=older_than_days)
		
		cleaned_searches = 0
		cleaned_files = 0
		
		# Cleanup in-memory data
		searches_to_remove = []
		for search_id, history in self.search_history.items():
			if history and history[0].created_at < cutoff_date:
				searches_to_remove.append(search_id)
		
		for search_id in searches_to_remove:
			del self.search_history[search_id]
			if search_id in self.best_architectures:
				del self.best_architectures[search_id]
			cleaned_searches += 1
		
		# Cleanup result files
		results_dir = Path('./nas_results')
		if results_dir.exists():
			for result_file in results_dir.glob('nas_results_*.json'):
				try:
					if datetime.fromtimestamp(result_file.stat().st_mtime) < cutoff_date:
						result_file.unlink()
						cleaned_files += 1
				except Exception as e:
					logger.warning(f"Failed to cleanup result file {result_file}: {e}")
		
		return {'cleaned_searches': cleaned_searches, 'cleaned_files': cleaned_files}

# Global NAS instance
_nas_instance: Optional[NeuralArchitectureSearch] = None

def get_neural_architecture_search(config: Optional[NASConfig] = None, 
								   db_session: Optional[Session] = None) -> NeuralArchitectureSearch:
	"""Get the global NAS instance."""
	global _nas_instance
	if _nas_instance is None:
		if config is None:
			config = NASConfig()
		_nas_instance = NeuralArchitectureSearch(config, db_session)
	return _nas_instance