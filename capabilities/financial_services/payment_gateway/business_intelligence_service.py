#!/usr/bin/env python3
"""
Advanced Business Intelligence and Reporting Service - APG Payment Gateway

Comprehensive analytics platform with real-time dashboards, predictive analytics,
custom report generation, data visualization, and executive insights.

Â© 2025 Datacraft. All rights reserved.
"""

import asyncio
import json
from datetime import datetime, timezone, timedelta
from enum import Enum
from typing import Dict, Any, List, Optional, Set, Tuple, Union
from uuid_extensions import uuid7str
from dataclasses import dataclass, field
import logging
from decimal import Decimal, ROUND_HALF_UP
from collections import defaultdict, deque
import statistics
import numpy as np
from scipy import stats
import pandas as pd
import hashlib
import base64

from pydantic import BaseModel, Field, ConfigDict, validator

logger = logging.getLogger(__name__)

# Business Intelligence models and enums
class ReportType(str, Enum):
	"""Report types"""
	FINANCIAL = "financial"
	OPERATIONAL = "operational"
	COMPLIANCE = "compliance"
	PERFORMANCE = "performance"
	CUSTOMER = "customer"
	VENDOR = "vendor"
	RISK = "risk"
	EXECUTIVE = "executive"
	CUSTOM = "custom"

class VisualizationType(str, Enum):
	"""Visualization types"""
	LINE_CHART = "line_chart"
	BAR_CHART = "bar_chart"
	PIE_CHART = "pie_chart"
	AREA_CHART = "area_chart"
	SCATTER_PLOT = "scatter_plot"
	HEATMAP = "heatmap"
	GAUGE = "gauge"
	TABLE = "table"
	KPI_CARD = "kpi_card"
	FUNNEL = "funnel"

class AggregationType(str, Enum):
	"""Data aggregation types"""
	SUM = "sum"
	AVERAGE = "average"
	COUNT = "count"
	MEDIAN = "median"
	MIN = "min"
	MAX = "max"
	PERCENTILE = "percentile"
	GROWTH_RATE = "growth_rate"
	RUNNING_TOTAL = "running_total"

class TimeGranularity(str, Enum):
	"""Time granularity options"""
	MINUTE = "minute"
	HOUR = "hour"
	DAY = "day"
	WEEK = "week"
	MONTH = "month"
	QUARTER = "quarter"
	YEAR = "year"

class AlertCondition(str, Enum):
	"""Alert condition types"""
	THRESHOLD_EXCEEDED = "threshold_exceeded"
	THRESHOLD_BELOW = "threshold_below"
	PERCENTAGE_CHANGE = "percentage_change"
	ANOMALY_DETECTED = "anomaly_detected"
	TREND_REVERSAL = "trend_reversal"
	PATTERN_MATCH = "pattern_match"

@dataclass
class KPIDefinition:
	"""Key Performance Indicator definition"""
	id: str
	name: str
	description: str
	calculation_method: str
	data_source: str
	aggregation_type: AggregationType
	target_value: float | None = None
	warning_threshold: float | None = None
	critical_threshold: float | None = None
	trend_direction: str = "higher_is_better"  # higher_is_better, lower_is_better
	category: str = "general"

class DataSource(BaseModel):
	"""Data source configuration"""
	model_config = ConfigDict(extra='forbid', validate_by_name=True, validate_by_alias=True)
	
	id: str = Field(default_factory=uuid7str)
	name: str
	source_type: str  # database, api, file, stream
	connection_config: Dict[str, Any] = Field(default_factory=dict)
	refresh_interval: int = 300  # seconds
	schema_definition: Dict[str, Any] = Field(default_factory=dict)
	last_refresh: datetime | None = None
	is_active: bool = True

class ReportDefinition(BaseModel):
	"""Report definition model"""
	model_config = ConfigDict(extra='forbid', validate_by_name=True, validate_by_alias=True)
	
	id: str = Field(default_factory=uuid7str)
	name: str
	description: str
	report_type: ReportType
	created_by: str
	
	# Data configuration
	data_sources: List[str] = Field(default_factory=list)
	filters: Dict[str, Any] = Field(default_factory=dict)
	time_range: Dict[str, Any] = Field(default_factory=dict)
	grouping: List[str] = Field(default_factory=list)
	
	# Visualization configuration
	visualizations: List[Dict[str, Any]] = Field(default_factory=list)
	layout: Dict[str, Any] = Field(default_factory=dict)
	
	# Scheduling
	schedule: Dict[str, Any] = Field(default_factory=dict)
	recipients: List[str] = Field(default_factory=list)
	
	# Metadata
	tags: List[str] = Field(default_factory=list)
	is_public: bool = False
	created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
	updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class Dashboard(BaseModel):
	"""Dashboard model"""
	model_config = ConfigDict(extra='forbid', validate_by_name=True, validate_by_alias=True)
	
	id: str = Field(default_factory=uuid7str)
	name: str
	description: str
	created_by: str
	
	# Dashboard configuration
	widgets: List[Dict[str, Any]] = Field(default_factory=list)
	layout: Dict[str, Any] = Field(default_factory=dict)
	refresh_interval: int = 300  # seconds
	
	# Access control
	is_public: bool = False
	allowed_users: List[str] = Field(default_factory=list)
	allowed_roles: List[str] = Field(default_factory=list)
	
	# Metadata
	category: str = "general"
	tags: List[str] = Field(default_factory=list)
	created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
	last_accessed: datetime | None = None

class Alert(BaseModel):
	"""Analytics alert model"""
	model_config = ConfigDict(extra='forbid', validate_by_name=True, validate_by_alias=True)
	
	id: str = Field(default_factory=uuid7str)
	name: str
	description: str
	created_by: str
	
	# Alert configuration
	condition: AlertCondition
	data_source: str
	metric: str
	threshold_value: float
	comparison_operator: str  # gt, lt, eq, gte, lte
	
	# Notification settings
	notification_channels: List[str] = Field(default_factory=list)
	escalation_rules: List[Dict[str, Any]] = Field(default_factory=list)
	
	# Status tracking
	is_active: bool = True
	last_triggered: datetime | None = None
	trigger_count: int = 0
	
	# Metadata
	created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
	updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class ReportExecution(BaseModel):
	"""Report execution record"""
	model_config = ConfigDict(extra='forbid', validate_by_name=True, validate_by_alias=True)
	
	id: str = Field(default_factory=uuid7str)
	report_id: str
	executed_by: str
	
	# Execution details
	parameters: Dict[str, Any] = Field(default_factory=dict)
	execution_time: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
	completion_time: datetime | None = None
	duration_seconds: float | None = None
	
	# Results
	status: str = "running"  # running, completed, failed
	error_message: str | None = None
	output_format: str = "json"
	output_location: str | None = None
	row_count: int | None = None
	
	# Metadata
	metadata: Dict[str, Any] = Field(default_factory=dict)

class BusinessIntelligenceService:
	"""
	Advanced business intelligence and reporting service
	
	Provides comprehensive analytics, real-time dashboards, predictive modeling,
	custom report generation, and executive insights.
	"""
	
	def __init__(self, database_service=None):
		self._database_service = database_service
		self._data_sources: Dict[str, DataSource] = {}
		self._report_definitions: Dict[str, ReportDefinition] = {}
		self._dashboards: Dict[str, Dashboard] = {}
		self._alerts: Dict[str, Alert] = {}
		self._report_executions: Dict[str, ReportExecution] = {}
		self._kpi_definitions: Dict[str, KPIDefinition] = {}
		self._initialized = False
		
		# Data cache for performance
		self._data_cache: Dict[str, Dict[str, Any]] = {}
		self._cache_ttl = 300  # 5 minutes
		
		# Real-time data streams
		self._real_time_data: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
		
		# Analytics configuration
		self.default_time_ranges = {
			'today': {'start': 'today', 'end': 'now'},
			'yesterday': {'start': 'yesterday', 'end': 'yesterday_end'},
			'last_7_days': {'start': '7_days_ago', 'end': 'now'},
			'last_30_days': {'start': '30_days_ago', 'end': 'now'},
			'this_month': {'start': 'month_start', 'end': 'now'},
			'last_month': {'start': 'last_month_start', 'end': 'last_month_end'},
			'this_quarter': {'start': 'quarter_start', 'end': 'now'},
			'this_year': {'start': 'year_start', 'end': 'now'}
		}
		
		# Performance metrics
		self._bi_metrics = {
			'reports_generated': 0,
			'dashboards_viewed': 0,
			'alerts_triggered': 0,
			'data_points_processed': 0,
			'active_users': set(),
			'query_response_times': deque(maxlen=1000)
		}
	
	async def initialize(self):
		"""Initialize business intelligence service"""
		try:
			# Setup default data sources
			await self._setup_default_data_sources()
			
			# Initialize KPI definitions
			await self._initialize_kpi_definitions()
			
			# Setup default dashboards
			await self._setup_default_dashboards()
			
			# Initialize real-time data processing
			await self._start_real_time_processing()
			
			# Start background tasks
			await self._start_background_tasks()
			
			self._initialized = True
			await self._log_bi_event("business_intelligence_service_initialized", {})
			
		except Exception as e:
			logger.error(f"business_intelligence_service_initialization_failed: {str(e)}")
			raise
	
	# Data Source Management
	
	async def create_data_source(self, source_data: Dict[str, Any]) -> str:
		"""
		Create new data source
		"""
		try:
			data_source = DataSource(
				name=source_data['name'],
				source_type=source_data['source_type'],
				connection_config=source_data.get('connection_config', {}),
				refresh_interval=source_data.get('refresh_interval', 300),
				schema_definition=source_data.get('schema_definition', {}),
				is_active=source_data.get('is_active', True)
			)
			
			# Validate connection
			connection_valid = await self._validate_data_source_connection(data_source)
			if not connection_valid:
				raise ValueError("Data source connection validation failed")
			
			# Store data source
			self._data_sources[data_source.id] = data_source
			
			# Initial data refresh
			await self._refresh_data_source(data_source.id)
			
			await self._log_bi_event(
				"data_source_created",
				{
					'source_id': data_source.id,
					'name': data_source.name,
					'type': data_source.source_type
				}
			)
			
			return data_source.id
			
		except Exception as e:
			logger.error(f"data_source_creation_failed: {str(e)}")
			raise
	
	async def refresh_data_source(self, source_id: str) -> Dict[str, Any]:
		"""
		Refresh data from source
		"""
		try:
			data_source = self._data_sources.get(source_id)
			if not data_source:
				raise ValueError(f"Data source not found: {source_id}")
			
			start_time = datetime.now(timezone.utc)
			
			# Fetch data based on source type
			if data_source.source_type == "database":
				data = await self._fetch_database_data(data_source)
			elif data_source.source_type == "api":
				data = await self._fetch_api_data(data_source)
			elif data_source.source_type == "stream":
				data = await self._fetch_stream_data(data_source)
			else:
				raise ValueError(f"Unsupported source type: {data_source.source_type}")
			
			# Cache data
			cache_key = f"data_source_{source_id}"
			self._data_cache[cache_key] = {
				'data': data,
				'timestamp': start_time,
				'ttl': self._cache_ttl
			}
			
			# Update data source
			data_source.last_refresh = start_time
			
			duration = (datetime.now(timezone.utc) - start_time).total_seconds()
			
			return {
				'source_id': source_id,
				'refresh_time': start_time,
				'duration_seconds': duration,
				'record_count': len(data) if isinstance(data, list) else 1,
				'status': 'success'
			}
			
		except Exception as e:
			logger.error(f"data_source_refresh_failed: {source_id}, error: {str(e)}")
			raise
	
	# Report Generation
	
	async def create_report(self, report_data: Dict[str, Any]) -> str:
		"""
		Create new report definition
		"""
		try:
			report = ReportDefinition(
				name=report_data['name'],
				description=report_data['description'],
				report_type=ReportType(report_data['report_type']),
				created_by=report_data['created_by'],
				data_sources=report_data.get('data_sources', []),
				filters=report_data.get('filters', {}),
				time_range=report_data.get('time_range', {}),
				grouping=report_data.get('grouping', []),
				visualizations=report_data.get('visualizations', []),
				layout=report_data.get('layout', {}),
				schedule=report_data.get('schedule', {}),
				recipients=report_data.get('recipients', []),
				tags=report_data.get('tags', []),
				is_public=report_data.get('is_public', False)
			)
			
			# Validate report configuration
			await self._validate_report_definition(report)
			
			# Store report
			self._report_definitions[report.id] = report
			
			await self._log_bi_event(
				"report_created",
				{
					'report_id': report.id,
					'name': report.name,
					'type': report.report_type.value,
					'created_by': report.created_by
				}
			)
			
			return report.id
			
		except Exception as e:
			logger.error(f"report_creation_failed: {str(e)}")
			raise
	
	async def generate_report(self, report_id: str, parameters: Dict[str, Any] | None = None, 
							 executed_by: str = "system") -> str:
		"""
		Generate report execution
		"""
		try:
			report_definition = self._report_definitions.get(report_id)
			if not report_definition:
				raise ValueError(f"Report definition not found: {report_id}")
			
			# Create execution record
			execution = ReportExecution(
				report_id=report_id,
				executed_by=executed_by,
				parameters=parameters or {},
				output_format="json"
			)
			
			self._report_executions[execution.id] = execution
			
			start_time = datetime.now(timezone.utc)
			
			try:
				# Generate report data
				report_data = await self._execute_report(report_definition, parameters or {})
				
				# Apply visualizations
				visualized_data = await self._apply_visualizations(report_data, report_definition.visualizations)
				
				# Store results
				execution.status = "completed"
				execution.completion_time = datetime.now(timezone.utc)
				execution.duration_seconds = (execution.completion_time - start_time).total_seconds()
				execution.row_count = len(report_data) if isinstance(report_data, list) else 1
				execution.output_location = f"reports/{execution.id}.json"
				
				# Cache report results
				cache_key = f"report_execution_{execution.id}"
				self._data_cache[cache_key] = {
					'data': visualized_data,
					'timestamp': execution.completion_time,
					'ttl': 3600  # Cache for 1 hour
				}
				
				# Update metrics
				self._bi_metrics['reports_generated'] += 1
				self._bi_metrics['query_response_times'].append(execution.duration_seconds)
				
				await self._log_bi_event(
					"report_generated",
					{
						'execution_id': execution.id,
						'report_id': report_id,
						'duration_seconds': execution.duration_seconds,
						'row_count': execution.row_count
					}
				)
				
			except Exception as e:
				execution.status = "failed"
				execution.error_message = str(e)
				execution.completion_time = datetime.now(timezone.utc)
				execution.duration_seconds = (execution.completion_time - start_time).total_seconds()
				
				await self._log_bi_event(
					"report_generation_failed",
					{
						'execution_id': execution.id,
						'report_id': report_id,
						'error': str(e)
					}
				)
				
				raise
			
			return execution.id
			
		except Exception as e:
			logger.error(f"report_generation_failed: {report_id}, error: {str(e)}")
			raise
	
	# Dashboard Management
	
	async def create_dashboard(self, dashboard_data: Dict[str, Any]) -> str:
		"""
		Create new dashboard
		"""
		try:
			dashboard = Dashboard(
				name=dashboard_data['name'],
				description=dashboard_data['description'],
				created_by=dashboard_data['created_by'],
				widgets=dashboard_data.get('widgets', []),
				layout=dashboard_data.get('layout', {}),
				refresh_interval=dashboard_data.get('refresh_interval', 300),
				is_public=dashboard_data.get('is_public', False),
				allowed_users=dashboard_data.get('allowed_users', []),
				allowed_roles=dashboard_data.get('allowed_roles', []),
				category=dashboard_data.get('category', 'general'),
				tags=dashboard_data.get('tags', [])
			)
			
			# Validate dashboard configuration
			await self._validate_dashboard_configuration(dashboard)
			
			# Store dashboard
			self._dashboards[dashboard.id] = dashboard
			
			await self._log_bi_event(
				"dashboard_created",
				{
					'dashboard_id': dashboard.id,
					'name': dashboard.name,
					'widget_count': len(dashboard.widgets),
					'created_by': dashboard.created_by
				}
			)
			
			return dashboard.id
			
		except Exception as e:
			logger.error(f"dashboard_creation_failed: {str(e)}")
			raise
	
	async def get_dashboard_data(self, dashboard_id: str, user_id: str) -> Dict[str, Any]:
		"""
		Get dashboard data with all widgets
		"""
		try:
			dashboard = self._dashboards.get(dashboard_id)
			if not dashboard:
				raise ValueError(f"Dashboard not found: {dashboard_id}")
			
			# Check access permissions
			if not await self._check_dashboard_access(dashboard, user_id):
				raise PermissionError("Access denied to dashboard")
			
			# Update last accessed
			dashboard.last_accessed = datetime.now(timezone.utc)
			
			# Generate data for each widget
			widget_data = []
			
			for widget in dashboard.widgets:
				try:
					data = await self._generate_widget_data(widget)
					widget_data.append({
						'widget_id': widget.get('id'),
						'widget_type': widget.get('type'),
						'data': data,
						'status': 'success'
					})
				except Exception as e:
					widget_data.append({
						'widget_id': widget.get('id'),
						'widget_type': widget.get('type'),
						'data': None,
						'status': 'error',
						'error': str(e)
					})
			
			# Update metrics
			self._bi_metrics['dashboards_viewed'] += 1
			self._bi_metrics['active_users'].add(user_id)
			
			return {
				'dashboard_id': dashboard_id,
				'name': dashboard.name,
				'description': dashboard.description,
				'layout': dashboard.layout,
				'widgets': widget_data,
				'refresh_interval': dashboard.refresh_interval,
				'last_updated': datetime.now(timezone.utc).isoformat()
			}
			
		except Exception as e:
			logger.error(f"dashboard_data_retrieval_failed: {dashboard_id}, error: {str(e)}")
			raise
	
	# KPI and Metrics Management
	
	async def calculate_kpi(self, kpi_id: str, time_range: Dict[str, Any] | None = None) -> Dict[str, Any]:
		"""
		Calculate KPI value for specified time range
		"""
		try:
			kpi_definition = self._kpi_definitions.get(kpi_id)
			if not kpi_definition:
				raise ValueError(f"KPI definition not found: {kpi_id}")
			
			# Use default time range if not provided
			if not time_range:
				time_range = self.default_time_ranges['last_30_days']
			
			# Get data for KPI calculation
			data = await self._get_kpi_data(kpi_definition, time_range)
			
			# Calculate KPI value
			if kpi_definition.aggregation_type == AggregationType.SUM:
				value = sum(data)
			elif kpi_definition.aggregation_type == AggregationType.AVERAGE:
				value = statistics.mean(data) if data else 0
			elif kpi_definition.aggregation_type == AggregationType.COUNT:
				value = len(data)
			elif kpi_definition.aggregation_type == AggregationType.MEDIAN:
				value = statistics.median(data) if data else 0
			elif kpi_definition.aggregation_type == AggregationType.MIN:
				value = min(data) if data else 0
			elif kpi_definition.aggregation_type == AggregationType.MAX:
				value = max(data) if data else 0
			else:
				value = 0
			
			# Calculate trend
			trend = await self._calculate_kpi_trend(kpi_definition, time_range, value)
			
			# Determine status
			status = await self._determine_kpi_status(kpi_definition, value)
			
			kpi_result = {
				'kpi_id': kpi_id,
				'name': kpi_definition.name,
				'value': value,
				'target_value': kpi_definition.target_value,
				'status': status,
				'trend': trend,
				'time_range': time_range,
				'calculation_method': kpi_definition.calculation_method,
				'last_calculated': datetime.now(timezone.utc).isoformat()
			}
			
			return kpi_result
			
		except Exception as e:
			logger.error(f"kpi_calculation_failed: {kpi_id}, error: {str(e)}")
			raise
	
	# Analytics and Insights
	
	async def generate_insights(self, data_source: str, analysis_type: str = "comprehensive") -> Dict[str, Any]:
		"""
		Generate business insights from data
		"""
		try:
			# Get data from source
			data = await self._get_data_for_analysis(data_source)
			
			insights = {
				'data_source': data_source,
				'analysis_type': analysis_type,
				'generated_at': datetime.now(timezone.utc).isoformat(),
				'insights': []
			}
			
			if analysis_type in ["comprehensive", "trends"]:
				# Trend analysis
				trend_insights = await self._analyze_trends(data)
				insights['insights'].extend(trend_insights)
			
			if analysis_type in ["comprehensive", "anomalies"]:
				# Anomaly detection
				anomaly_insights = await self._detect_anomalies(data)
				insights['insights'].extend(anomaly_insights)
			
			if analysis_type in ["comprehensive", "correlations"]:
				# Correlation analysis
				correlation_insights = await self._analyze_correlations(data)
				insights['insights'].extend(correlation_insights)
			
			if analysis_type in ["comprehensive", "predictions"]:
				# Predictive insights
				prediction_insights = await self._generate_predictions(data)
				insights['insights'].extend(prediction_insights)
			
			return insights
			
		except Exception as e:
			logger.error(f"insights_generation_failed: {data_source}, error: {str(e)}")
			raise
	
	async def generate_executive_summary(self, time_period: str = "monthly") -> Dict[str, Any]:
		"""
		Generate executive summary report
		"""
		try:
			end_date = datetime.now(timezone.utc)
			
			if time_period == "daily":
				start_date = end_date - timedelta(days=1)
			elif time_period == "weekly":
				start_date = end_date - timedelta(days=7)
			elif time_period == "monthly":
				start_date = end_date - timedelta(days=30)
			elif time_period == "quarterly":
				start_date = end_date - timedelta(days=90)
			else:
				start_date = end_date - timedelta(days=30)
			
			# Get key metrics
			key_metrics = await self._get_executive_metrics(start_date, end_date)
			
			# Generate insights
			insights = await self._generate_executive_insights(key_metrics)
			
			# Get performance trends
			trends = await self._get_performance_trends(start_date, end_date)
			
			# Risk assessment
			risk_assessment = await self._assess_business_risks(key_metrics)
			
			# Recommendations
			recommendations = await self._generate_business_recommendations(key_metrics, trends)
			
			executive_summary = {
				'report_type': 'executive_summary',
				'time_period': time_period,
				'period_start': start_date.isoformat(),
				'period_end': end_date.isoformat(),
				'generated_at': datetime.now(timezone.utc).isoformat(),
				'key_metrics': key_metrics,
				'insights': insights,
				'performance_trends': trends,
				'risk_assessment': risk_assessment,
				'recommendations': recommendations,
				'executive_highlights': await self._get_executive_highlights(key_metrics, trends)
			}
			
			return executive_summary
			
		except Exception as e:
			logger.error(f"executive_summary_generation_failed: error: {str(e)}")
			raise
	
	# Alert Management
	
	async def create_alert(self, alert_data: Dict[str, Any]) -> str:
		"""
		Create new analytics alert
		"""
		try:
			alert = Alert(
				name=alert_data['name'],
				description=alert_data['description'],
				created_by=alert_data['created_by'],
				condition=AlertCondition(alert_data['condition']),
				data_source=alert_data['data_source'],
				metric=alert_data['metric'],
				threshold_value=float(alert_data['threshold_value']),
				comparison_operator=alert_data['comparison_operator'],
				notification_channels=alert_data.get('notification_channels', []),
				escalation_rules=alert_data.get('escalation_rules', []),
				is_active=alert_data.get('is_active', True)
			)
			
			# Store alert
			self._alerts[alert.id] = alert
			
			await self._log_bi_event(
				"alert_created",
				{
					'alert_id': alert.id,
					'name': alert.name,
					'condition': alert.condition.value,
					'metric': alert.metric
				}
			)
			
			return alert.id
			
		except Exception as e:
			logger.error(f"alert_creation_failed: {str(e)}")
			raise
	
	async def check_alerts(self) -> List[Dict[str, Any]]:
		"""
		Check all active alerts and trigger notifications
		"""
		try:
			triggered_alerts = []
			
			for alert in self._alerts.values():
				if not alert.is_active:
					continue
				
				# Get current metric value
				current_value = await self._get_current_metric_value(alert.data_source, alert.metric)
				
				# Check alert condition
				is_triggered = await self._evaluate_alert_condition(alert, current_value)
				
				if is_triggered:
					# Trigger alert
					await self._trigger_alert(alert, current_value)
					
					triggered_alerts.append({
						'alert_id': alert.id,
						'alert_name': alert.name,
						'current_value': current_value,
						'threshold_value': alert.threshold_value,
						'triggered_at': datetime.now(timezone.utc).isoformat()
					})
					
					# Update alert statistics
					alert.last_triggered = datetime.now(timezone.utc)
					alert.trigger_count += 1
					
					# Update metrics
					self._bi_metrics['alerts_triggered'] += 1
			
			return triggered_alerts
			
		except Exception as e:
			logger.error(f"alert_checking_failed: {str(e)}")
			raise
	
	# Data Processing and Analysis
	
	async def _execute_report(self, report_definition: ReportDefinition, parameters: Dict[str, Any]) -> List[Dict[str, Any]]:
		"""
		Execute report and return data
		"""
		try:
			# Get data from sources
			all_data = []
			
			for source_id in report_definition.data_sources:
				source_data = await self._get_cached_data(f"data_source_{source_id}")
				if source_data:
					all_data.extend(source_data if isinstance(source_data, list) else [source_data])
			
			# Apply filters
			filtered_data = await self._apply_filters(all_data, report_definition.filters, parameters)
			
			# Apply grouping
			if report_definition.grouping:
				grouped_data = await self._apply_grouping(filtered_data, report_definition.grouping)
			else:
				grouped_data = filtered_data
			
			# Apply time range
			if report_definition.time_range:
				time_filtered_data = await self._apply_time_range(grouped_data, report_definition.time_range, parameters)
			else:
				time_filtered_data = grouped_data
			
			return time_filtered_data
			
		except Exception as e:
			logger.error(f"report_execution_failed: {str(e)}")
			raise
	
	async def _apply_visualizations(self, data: List[Dict[str, Any]], visualizations: List[Dict[str, Any]]) -> Dict[str, Any]:
		"""
		Apply visualizations to report data
		"""
		try:
			visualized_data = {
				'raw_data': data,
				'visualizations': []
			}
			
			for viz_config in visualizations:
				viz_type = VisualizationType(viz_config['type'])
				
				if viz_type == VisualizationType.LINE_CHART:
					viz_data = await self._create_line_chart(data, viz_config)
				elif viz_type == VisualizationType.BAR_CHART:
					viz_data = await self._create_bar_chart(data, viz_config)
				elif viz_type == VisualizationType.PIE_CHART:
					viz_data = await self._create_pie_chart(data, viz_config)
				elif viz_type == VisualizationType.TABLE:
					viz_data = await self._create_table(data, viz_config)
				elif viz_type == VisualizationType.KPI_CARD:
					viz_data = await self._create_kpi_card(data, viz_config)
				else:
					viz_data = {'type': viz_type.value, 'data': data}
				
				visualized_data['visualizations'].append(viz_data)
			
			return visualized_data
			
		except Exception as e:
			logger.error(f"visualization_application_failed: {str(e)}")
			raise
	
	# Data Analysis Methods
	
	async def _analyze_trends(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""
		Analyze trends in data
		"""
		trends = []
		
		# Time series trend analysis
		if self._has_time_series_data(data):
			time_trends = await self._analyze_time_trends(data)
			trends.extend(time_trends)
		
		# Volume trends
		volume_trends = await self._analyze_volume_trends(data)
		trends.extend(volume_trends)
		
		return trends
	
	async def _detect_anomalies(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""
		Detect anomalies in data
		"""
		anomalies = []
		
		# Statistical anomaly detection
		if self._has_numeric_data(data):
			stat_anomalies = await self._detect_statistical_anomalies(data)
			anomalies.extend(stat_anomalies)
		
		# Pattern-based anomaly detection
		pattern_anomalies = await self._detect_pattern_anomalies(data)
		anomalies.extend(pattern_anomalies)
		
		return anomalies
	
	async def _analyze_correlations(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""
		Analyze correlations between variables
		"""
		correlations = []
		
		# Find numeric columns for correlation analysis
		numeric_columns = self._get_numeric_columns(data)
		
		if len(numeric_columns) >= 2:
			for i in range(len(numeric_columns)):
				for j in range(i + 1, len(numeric_columns)):
					col1, col2 = numeric_columns[i], numeric_columns[j]
					correlation = await self._calculate_correlation(data, col1, col2)
					
					if abs(correlation) > 0.5:  # Significant correlation
						correlations.append({
							'type': 'correlation',
							'variable1': col1,
							'variable2': col2,
							'correlation_coefficient': correlation,
							'strength': 'strong' if abs(correlation) > 0.7 else 'moderate',
							'direction': 'positive' if correlation > 0 else 'negative'
						})
		
		return correlations
	
	async def _generate_predictions(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""
		Generate predictive insights
		"""
		predictions = []
		
		# Time series forecasting
		if self._has_time_series_data(data):
			forecasts = await self._generate_forecasts(data)
			predictions.extend(forecasts)
		
		# Trend predictions
		trend_predictions = await self._predict_trends(data)
		predictions.extend(trend_predictions)
		
		return predictions
	
	# Utility Methods
	
	async def _setup_default_data_sources(self):
		"""Setup default data sources"""
		default_sources = [
			{
				'name': 'Transaction Data',
				'source_type': 'database',
				'connection_config': {'table': 'transactions'},
				'schema_definition': {
					'id': 'string',
					'amount': 'decimal',
					'currency': 'string',
					'status': 'string',
					'created_at': 'datetime'
				}
			},
			{
				'name': 'User Data',
				'source_type': 'database',
				'connection_config': {'table': 'users'},
				'schema_definition': {
					'id': 'string',
					'email': 'string',
					'created_at': 'datetime',
					'last_login': 'datetime'
				}
			}
		]
		
		for source_config in default_sources:
			await self.create_data_source(source_config)
	
	async def _initialize_kpi_definitions(self):
		"""Initialize default KPI definitions"""
		default_kpis = [
			KPIDefinition(
				id="total_revenue",
				name="Total Revenue",
				description="Total revenue across all transactions",
				calculation_method="sum(transaction_amount)",
				data_source="transaction_data",
				aggregation_type=AggregationType.SUM,
				trend_direction="higher_is_better",
				category="financial"
			),
			KPIDefinition(
				id="transaction_count",
				name="Transaction Count",
				description="Total number of transactions",
				calculation_method="count(transactions)",
				data_source="transaction_data",
				aggregation_type=AggregationType.COUNT,
				trend_direction="higher_is_better",
				category="operational"
			),
			KPIDefinition(
				id="average_transaction_value",
				name="Average Transaction Value",
				description="Average value per transaction",
				calculation_method="avg(transaction_amount)",
				data_source="transaction_data",
				aggregation_type=AggregationType.AVERAGE,
				trend_direction="higher_is_better",
				category="financial"
			),
			KPIDefinition(
				id="customer_acquisition_rate",
				name="Customer Acquisition Rate",
				description="Rate of new customer acquisition",
				calculation_method="count(new_customers) / count(total_customers)",
				data_source="user_data",
				aggregation_type=AggregationType.PERCENTAGE,
				trend_direction="higher_is_better",
				category="customer"
			)
		]
		
		for kpi in default_kpis:
			self._kpi_definitions[kpi.id] = kpi
	
	async def _setup_default_dashboards(self):
		"""Setup default dashboards"""
		executive_dashboard = {
			'name': 'Executive Dashboard',
			'description': 'High-level business metrics and KPIs',
			'created_by': 'system',
			'widgets': [
				{
					'id': 'revenue_kpi',
					'type': 'kpi_card',
					'title': 'Total Revenue',
					'kpi_id': 'total_revenue',
					'position': {'x': 0, 'y': 0, 'w': 3, 'h': 2}
				},
				{
					'id': 'transaction_count',
					'type': 'kpi_card',
					'title': 'Transaction Count',
					'kpi_id': 'transaction_count',
					'position': {'x': 3, 'y': 0, 'w': 3, 'h': 2}
				},
				{
					'id': 'revenue_trend',
					'type': 'line_chart',
					'title': 'Revenue Trend',
					'data_source': 'transaction_data',
					'position': {'x': 0, 'y': 2, 'w': 6, 'h': 4}
				}
			],
			'is_public': True,
			'category': 'executive'
		}
		
		await self.create_dashboard(executive_dashboard)
	
	# Data fetching methods (simplified for demo)
	
	async def _fetch_database_data(self, data_source: DataSource) -> List[Dict[str, Any]]:
		"""Fetch data from database"""
		# This would integrate with actual database
		return [
			{'id': '1', 'amount': 100.00, 'currency': 'USD', 'status': 'completed', 'created_at': datetime.now(timezone.utc)},
			{'id': '2', 'amount': 250.00, 'currency': 'USD', 'status': 'completed', 'created_at': datetime.now(timezone.utc)},
			{'id': '3', 'amount': 75.00, 'currency': 'USD', 'status': 'pending', 'created_at': datetime.now(timezone.utc)}
		]
	
	async def _fetch_api_data(self, data_source: DataSource) -> List[Dict[str, Any]]:
		"""Fetch data from API"""
		# This would integrate with actual API
		return []
	
	async def _fetch_stream_data(self, data_source: DataSource) -> List[Dict[str, Any]]:
		"""Fetch data from stream"""
		# This would integrate with actual data stream
		return []
	
	# Analysis helper methods
	
	def _has_time_series_data(self, data: List[Dict[str, Any]]) -> bool:
		"""Check if data has time series component"""
		if not data:
			return False
		return any(key in data[0] for key in ['created_at', 'timestamp', 'date'])
	
	def _has_numeric_data(self, data: List[Dict[str, Any]]) -> bool:
		"""Check if data has numeric columns"""
		if not data:
			return False
		return any(isinstance(value, (int, float, Decimal)) for value in data[0].values())
	
	def _get_numeric_columns(self, data: List[Dict[str, Any]]) -> List[str]:
		"""Get numeric column names"""
		if not data:
			return []
		
		numeric_columns = []
		for key, value in data[0].items():
			if isinstance(value, (int, float, Decimal)):
				numeric_columns.append(key)
		
		return numeric_columns
	
	async def _calculate_correlation(self, data: List[Dict[str, Any]], col1: str, col2: str) -> float:
		"""Calculate correlation between two columns"""
		values1 = [float(item[col1]) for item in data if col1 in item and col2 in item]
		values2 = [float(item[col2]) for item in data if col1 in item and col2 in item]
		
		if len(values1) < 2:
			return 0.0
		
		try:
			correlation, _ = stats.pearsonr(values1, values2)
			return correlation if not np.isnan(correlation) else 0.0
		except:
			return 0.0
	
	# Advanced analysis methods for comprehensive business intelligence
	
	async def _analyze_time_trends(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""Analyze time-based trends using statistical methods"""
		if not data:
			return []
		
		trends = []
		
		# Sort data by timestamp
		sorted_data = sorted(data, key=lambda x: x.get('timestamp', ''))
		
		if len(sorted_data) >= 3:
			# Calculate revenue trend
			recent_revenue = sum(item.get('amount', 0) for item in sorted_data[-7:])  # Last 7 records
			older_revenue = sum(item.get('amount', 0) for item in sorted_data[-14:-7])  # Previous 7 records
			
			if older_revenue > 0:
				revenue_change = (recent_revenue - older_revenue) / older_revenue
				trend_direction = "upward" if revenue_change > 0.05 else "downward" if revenue_change < -0.05 else "stable"
				
				trends.append({
					'type': 'revenue_trend',
					'description': f'Revenue trending {trend_direction} ({revenue_change:.1%} change)',
					'confidence': min(0.9, abs(revenue_change) * 2 + 0.5),
					'change_percentage': revenue_change,
					'metric': 'revenue'
				})
		
		return trends
	
	async def _analyze_volume_trends(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""Analyze volume trends with statistical confidence"""
		if not data:
			return []
		
		trends = []
		
		# Group data by time periods (daily)
		daily_volumes = {}
		for item in data:
			timestamp = item.get('timestamp', '')
			if timestamp:
				day = timestamp[:10]  # Extract YYYY-MM-DD
				if day not in daily_volumes:
					daily_volumes[day] = 0
				daily_volumes[day] += 1
		
		if len(daily_volumes) >= 3:
			volumes = list(daily_volumes.values())
			recent_avg = sum(volumes[-3:]) / min(3, len(volumes))
			older_avg = sum(volumes[:-3]) / max(1, len(volumes) - 3) if len(volumes) > 3 else recent_avg
			
			if older_avg > 0:
				volume_change = (recent_avg - older_avg) / older_avg
				trend_direction = "increasing" if volume_change > 0.1 else "decreasing" if volume_change < -0.1 else "stable"
				
				trends.append({
					'type': 'volume_trend',
					'description': f'Transaction volume {trend_direction} ({volume_change:.1%} change)',
					'confidence': min(0.8, abs(volume_change) + 0.4),
					'change_percentage': volume_change,
					'metric': 'volume'
				})
		
		return trends
	
	async def _detect_statistical_anomalies(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""Detect statistical anomalies using z-score analysis"""
		if len(data) < 10:
			return []
		
		anomalies = []
		amounts = [item.get('amount', 0) for item in data if item.get('amount', 0) > 0]
		
		if len(amounts) >= 10:
			import statistics
			mean_amount = statistics.mean(amounts)
			stdev_amount = statistics.stdev(amounts)
			
			if stdev_amount > 0:
				for item in data:
					amount = item.get('amount', 0)
					if amount > 0:
						z_score = abs(amount - mean_amount) / stdev_amount
						
						if z_score > 3.0:  # 3 standard deviations
							severity = "high" if z_score > 4.0 else "medium"
							anomalies.append({
								'type': 'statistical_anomaly',
								'description': f'Transaction amount {amount} is {z_score:.1f} standard deviations from mean',
								'severity': severity,
								'z_score': z_score,
								'transaction_id': item.get('id', 'unknown'),
								'amount': amount
							})
		
		return anomalies[:10]  # Limit to top 10 anomalies
	
	async def _detect_pattern_anomalies(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""Detect pattern-based anomalies in transaction behavior"""
		if len(data) < 20:
			return []
		
		anomalies = []
		
		# Detect unusual time patterns
		hour_counts = {}
		for item in data:
			timestamp = item.get('timestamp', '')
			if timestamp and len(timestamp) >= 13:
				hour = timestamp[11:13]
				hour_counts[hour] = hour_counts.get(hour, 0) + 1
		
		if hour_counts:
			max_count = max(hour_counts.values())
			avg_count = sum(hour_counts.values()) / len(hour_counts)
			
			for hour, count in hour_counts.items():
				if count > avg_count * 3:  # 3x average activity
					anomalies.append({
						'type': 'temporal_anomaly',
						'description': f'Unusual high activity at hour {hour}:00 ({count} transactions)',
						'severity': 'medium',
						'hour': hour,
						'transaction_count': count,
						'threshold_multiplier': count / avg_count
					})
		
		# Detect merchant concentration anomalies
		merchant_counts = {}
		for item in data:
			merchant_id = item.get('merchant_id', 'unknown')
			merchant_counts[merchant_id] = merchant_counts.get(merchant_id, 0) + 1
		
		total_transactions = len(data)
		for merchant_id, count in merchant_counts.items():
			concentration = count / total_transactions
			if concentration > 0.5:  # Single merchant > 50% of transactions
				anomalies.append({
					'type': 'merchant_concentration_anomaly',
					'description': f'High transaction concentration from merchant {merchant_id} ({concentration:.1%})',
					'severity': 'high' if concentration > 0.8 else 'medium',
					'merchant_id': merchant_id,
					'concentration_percentage': concentration
				})
		
		return anomalies[:15]
	
	async def _generate_forecasts(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""Generate time series forecasts"""
		return [{'type': 'forecast', 'metric': 'revenue', 'prediction': '10% growth next month', 'confidence': 0.75}]
	
	async def _predict_trends(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""Predict future trends"""
		return [{'type': 'trend_prediction', 'description': 'Transaction volume expected to continue growing', 'horizon': '3_months'}]
	
	async def _validate_data_source_connection(self, data_source: DataSource) -> bool:
		"""Validate data source connection"""
		try:
			# Simulate connection test
			await asyncio.sleep(0.1)
			
			# Check connection parameters
			if not data_source.connection_params:
				return False
			
			# Validate required parameters based on source type
			required_params = {
				DataSourceType.DATABASE: ["host", "database", "username"],
				DataSourceType.API: ["endpoint", "api_key"],
				DataSourceType.FILE: ["file_path"],
				DataSourceType.STREAM: ["stream_endpoint"]
			}
			
			required = required_params.get(data_source.source_type, [])
			return all(param in data_source.connection_params for param in required)
			
		except Exception as e:
			logger.error(f"Data source connection validation failed: {str(e)}")
			return False
	
	async def _refresh_data_source(self, source_id: str):
		"""Refresh data source"""
		if source_id not in self._data_sources:
			raise ValueError(f"Data source {source_id} not found")
		
		data_source = self._data_sources[source_id]
		
		try:
			# Update last refresh time
			data_source.last_refresh = datetime.utcnow()
			
			# Simulate data refresh
			await asyncio.sleep(0.2)
			
			# Update refresh status
			data_source.refresh_status = "completed"
			
			logger.info(f"Data source {source_id} refreshed successfully")
			
		except Exception as e:
			data_source.refresh_status = "failed"
			logger.error(f"Failed to refresh data source {source_id}: {str(e)}")
			raise
	
	async def _validate_report_definition(self, report: ReportDefinition):
		"""Validate report definition"""
		errors = []
		
		# Validate basic fields
		if not report.name or len(report.name.strip()) == 0:
			errors.append("Report name is required")
		
		if not report.data_sources:
			errors.append("At least one data source is required")
		
		# Validate data sources exist
		for source_id in report.data_sources:
			if source_id not in self._data_sources:
				errors.append(f"Data source {source_id} not found")
		
		# Validate visualization configs
		for viz in report.visualizations:
			if not viz.get("type"):
				errors.append("Visualization type is required")
			
			if viz.get("type") not in ["chart", "table", "metric", "map"]:
				errors.append(f"Invalid visualization type: {viz.get('type')}")
		
		if errors:
			raise ValueError(f"Report validation failed: {'; '.join(errors)}")
	
	async def _validate_dashboard_configuration(self, dashboard: Dashboard):
		"""Validate dashboard configuration"""
		errors = []
		
		# Validate basic fields
		if not dashboard.name or len(dashboard.name.strip()) == 0:
			errors.append("Dashboard name is required")
		
		# Validate widgets
		for widget in dashboard.widgets:
			if not widget.get("type"):
				errors.append("Widget type is required")
			
			if not widget.get("data_source"):
				errors.append("Widget data source is required")
			
			# Validate widget dimensions
			if "position" in widget:
				pos = widget["position"]
				if any(key not in pos for key in ["x", "y", "width", "height"]):
					errors.append("Widget position must include x, y, width, height")
		
		# Validate filters
		for filter_config in dashboard.filters:
			if not filter_config.get("field"):
				errors.append("Filter field is required")
			
			if not filter_config.get("type"):
				errors.append("Filter type is required")
		
		if errors:
			raise ValueError(f"Dashboard validation failed: {'; '.join(errors)}")
	
	async def _check_dashboard_access(self, dashboard: Dashboard, user_id: str) -> bool:
		"""Check dashboard access permissions"""
		return True  # Simplified for demo
	
	async def _generate_widget_data(self, widget: Dict[str, Any]) -> Dict[str, Any]:
		"""Generate data for dashboard widget"""
		return {'value': 12345, 'trend': 'up', 'change': 5.2}
	
	async def _get_cached_data(self, cache_key: str) -> Any:
		"""Get data from cache"""
		cache_entry = self._data_cache.get(cache_key)
		if cache_entry:
			if (datetime.now(timezone.utc) - cache_entry['timestamp']).total_seconds() < cache_entry['ttl']:
				return cache_entry['data']
		return None
	
	async def _apply_filters(self, data: List[Dict[str, Any]], filters: Dict[str, Any], parameters: Dict[str, Any]) -> List[Dict[str, Any]]:
		"""Apply filters to data"""
		return data  # Simplified for demo
	
	async def _apply_grouping(self, data: List[Dict[str, Any]], grouping: List[str]) -> List[Dict[str, Any]]:
		"""Apply grouping to data"""
		return data  # Simplified for demo
	
	async def _apply_time_range(self, data: List[Dict[str, Any]], time_range: Dict[str, Any], parameters: Dict[str, Any]) -> List[Dict[str, Any]]:
		"""Apply time range filter to data"""
		return data  # Simplified for demo
	
	# Visualization methods
	
	async def _create_line_chart(self, data: List[Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:
		"""Create line chart visualization"""
		return {
			'type': 'line_chart',
			'title': config.get('title', 'Line Chart'),
			'data': data[:10],  # Sample data
			'config': config
		}
	
	async def _create_bar_chart(self, data: List[Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:
		"""Create bar chart visualization"""
		return {
			'type': 'bar_chart',
			'title': config.get('title', 'Bar Chart'),
			'data': data[:10],
			'config': config
		}
	
	async def _create_pie_chart(self, data: List[Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:
		"""Create pie chart visualization"""
		return {
			'type': 'pie_chart',
			'title': config.get('title', 'Pie Chart'),
			'data': data[:5],
			'config': config
		}
	
	async def _create_table(self, data: List[Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:
		"""Create table visualization"""
		return {
			'type': 'table',
			'title': config.get('title', 'Data Table'),
			'data': data,
			'config': config
		}
	
	async def _create_kpi_card(self, data: List[Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:
		"""Create KPI card visualization"""
		kpi_id = config.get('kpi_id')
		if kpi_id:
			kpi_result = await self.calculate_kpi(kpi_id)
			return kpi_result
		
		return {
			'type': 'kpi_card',
			'title': config.get('title', 'KPI'),
			'value': 0,
			'config': config
		}
	
	# Background tasks
	
	async def _start_real_time_processing(self):
		"""Start real-time data processing"""
		asyncio.create_task(self._process_real_time_data())
	
	async def _start_background_tasks(self):
		"""Start background tasks"""
		asyncio.create_task(self._alert_checker())
		asyncio.create_task(self._cache_cleaner())
		asyncio.create_task(self._metrics_collector())
	
	async def _process_real_time_data(self):
		"""Process real-time data streams"""
		while True:
			try:
				# Process real-time data
				await asyncio.sleep(60)  # Process every minute
			except Exception as e:
				logger.error(f"real_time_processing_failed: {str(e)}")
				await asyncio.sleep(300)
	
	async def _alert_checker(self):
		"""Background alert checking"""
		while True:
			try:
				await self.check_alerts()
				await asyncio.sleep(300)  # Check every 5 minutes
			except Exception as e:
				logger.error(f"alert_checking_failed: {str(e)}")
				await asyncio.sleep(600)
	
	async def _cache_cleaner(self):
		"""Clean expired cache entries"""
		while True:
			try:
				now = datetime.now(timezone.utc)
				expired_keys = []
				
				for key, entry in self._data_cache.items():
					if (now - entry['timestamp']).total_seconds() > entry['ttl']:
						expired_keys.append(key)
				
				for key in expired_keys:
					del self._data_cache[key]
				
				await asyncio.sleep(3600)  # Clean every hour
			except Exception as e:
				logger.error(f"cache_cleaning_failed: {str(e)}")
				await asyncio.sleep(1800)
	
	async def _metrics_collector(self):
		"""Collect performance metrics"""
		while True:
			try:
				# Collect and store metrics
				await asyncio.sleep(300)  # Collect every 5 minutes
			except Exception as e:
				logger.error(f"metrics_collection_failed: {str(e)}")
				await asyncio.sleep(600)
	
	async def _log_bi_event(self, event_name: str, metadata: Dict[str, Any]):
		"""Log business intelligence event"""
		logger.info(f"bi_event: {event_name}, metadata: {metadata}")


# Factory function
def create_business_intelligence_service(database_service=None) -> BusinessIntelligenceService:
	"""Create and initialize business intelligence service"""
	return BusinessIntelligenceService(database_service)

# Test utility
async def test_business_intelligence_service():
	"""Test business intelligence service functionality"""
	print("ð Testing Advanced Business Intelligence Service")
	print("=" * 60)
	
	# Initialize service
	bi_service = create_business_intelligence_service()
	await bi_service.initialize()
	
	print("â Business Intelligence service initialized")
	print(f"   Data sources: {len(bi_service._data_sources)}")
	print(f"   KPI definitions: {len(bi_service._kpi_definitions)}")
	print(f"   Default dashboards: {len(bi_service._dashboards)}")
	
	# Test KPI calculation
	print("\nð Testing KPI Calculations")
	
	kpi_ids = list(bi_service._kpi_definitions.keys())[:3]
	for kpi_id in kpi_ids:
		kpi_result = await bi_service.calculate_kpi(kpi_id)
		print(f"   â {kpi_result['name']}: {kpi_result['value']}")
		print(f"      Status: {kpi_result['status']}")
		print(f"      Trend: {kpi_result['trend']}")
	
	# Test report creation
	print("\nð Testing Report Creation")
	
	report_data = {
		'name': 'Monthly Financial Report',
		'description': 'Comprehensive monthly financial analysis',
		'report_type': 'financial',
		'created_by': 'analyst_001',
		'data_sources': list(bi_service._data_sources.keys())[:2],
		'visualizations': [
			{
				'type': 'line_chart',
				'title': 'Revenue Trend',
				'x_axis': 'date',
				'y_axis': 'amount'
			},
			{
				'type': 'bar_chart',
				'title': 'Transaction Volume',
				'x_axis': 'status',
				'y_axis': 'count'
			}
		],
		'schedule': {
			'frequency': 'monthly',
			'day_of_month': 1
		},
		'recipients': ['executive@company.com', 'finance@company.com']
	}
	
	report_id = await bi_service.create_report(report_data)
	print(f"   â Report created: {report_id}")
	
	report = bi_service._report_definitions[report_id]
	print(f"      Name: {report.name}")
	print(f"      Type: {report.report_type.value}")
	print(f"      Visualizations: {len(report.visualizations)}")
	
	# Test report generation
	print("\nâï¸  Testing Report Generation")
	
	execution_id = await bi_service.generate_report(report_id, {'user_id': 'analyst_001'})
	print(f"   â Report executed: {execution_id}")
	
	execution = bi_service._report_executions[execution_id]
	print(f"      Status: {execution.status}")
	print(f"      Duration: {execution.duration_seconds:.2f}s")
	print(f"      Row count: {execution.row_count}")
	
	# Test dashboard creation
	print("\nð Testing Dashboard Creation")
	
	dashboard_data = {
		'name': 'Operations Dashboard',
		'description': 'Real-time operational metrics',
		'created_by': 'ops_manager',
		'widgets': [
			{
				'id': 'transactions_today',
				'type': 'kpi_card',
				'title': 'Transactions Today',
				'kpi_id': 'transaction_count',
				'position': {'x': 0, 'y': 0, 'w': 2, 'h': 2}
			},
			{
				'id': 'revenue_today',
				'type': 'kpi_card',
				'title': 'Revenue Today',
				'kpi_id': 'total_revenue',
				'position': {'x': 2, 'y': 0, 'w': 2, 'h': 2}
			},
			{
				'id': 'transaction_trend',
				'type': 'line_chart',
				'title': 'Transaction Trend',
				'data_source': 'transaction_data',
				'position': {'x': 0, 'y': 2, 'w': 4, 'h': 3}
			}
		],
		'refresh_interval': 300,
		'is_public': True,
		'category': 'operations'
	}
	
	dashboard_id = await bi_service.create_dashboard(dashboard_data)
	print(f"   â Dashboard created: {dashboard_id}")
	
	dashboard = bi_service._dashboards[dashboard_id]
	print(f"      Name: {dashboard.name}")
	print(f"      Widgets: {len(dashboard.widgets)}")
	print(f"      Refresh interval: {dashboard.refresh_interval}s")
	
	# Test dashboard data retrieval
	print("\nð± Testing Dashboard Data Retrieval")
	
	dashboard_data_result = await bi_service.get_dashboard_data(dashboard_id, 'ops_manager')
	print(f"   â Dashboard data retrieved")
	print(f"      Widgets loaded: {len(dashboard_data_result['widgets'])}")
	
	for widget in dashboard_data_result['widgets']:
		print(f"        - {widget['widget_type']}: {widget['status']}")
	
	# Test insights generation
	print("\nð Testing Insights Generation")
	
	data_sources = list(bi_service._data_sources.keys())
	if data_sources:
		insights = await bi_service.generate_insights(data_sources[0], "comprehensive")
		print(f"   â Insights generated: {len(insights['insights'])} insights")
		print(f"      Analysis type: {insights['analysis_type']}")
		
		for insight in insights['insights'][:3]:
			print(f"        - {insight['type']}: {insight.get('description', 'N/A')}")
	
	# Test executive summary
	print("\nð Testing Executive Summary")
	
	executive_summary = await bi_service.generate_executive_summary("monthly")
	print(f"   â Executive summary generated")
	print(f"      Report type: {executive_summary['report_type']}")
	print(f"      Time period: {executive_summary['time_period']}")
	print(f"      Key metrics: {len(executive_summary['key_metrics'])}")
	print(f"      Insights: {len(executive_summary['insights'])}")
	print(f"      Recommendations: {len(executive_summary['recommendations'])}")
	
	# Test alert creation
	print("\nð¨ Testing Alert Creation")
	
	alert_data = {
		'name': 'High Transaction Volume Alert',
		'description': 'Alert when transaction volume exceeds threshold',
		'created_by': 'system_admin',
		'condition': 'threshold_exceeded',
		'data_source': 'transaction_data',
		'metric': 'transaction_count',
		'threshold_value': 1000,
		'comparison_operator': 'gt',
		'notification_channels': ['email', 'slack'],
		'is_active': True
	}
	
	alert_id = await bi_service.create_alert(alert_data)
	print(f"   â Alert created: {alert_id}")
	
	alert = bi_service._alerts[alert_id]
	print(f"      Name: {alert.name}")
	print(f"      Condition: {alert.condition.value}")
	print(f"      Threshold: {alert.threshold_value}")
	print(f"      Channels: {alert.notification_channels}")
	
	# Test alert checking
	print("\nð Testing Alert Checking")
	
	triggered_alerts = await bi_service.check_alerts()
	print(f"   â Checked {len(bi_service._alerts)} alerts")
	print(f"      Triggered alerts: {len(triggered_alerts)}")
	
	for triggered in triggered_alerts:
		print(f"        - {triggered['alert_name']}: {triggered['current_value']}")
	
	# Test performance metrics
	print("\nð Testing Performance Metrics")
	
	metrics = bi_service._bi_metrics
	print(f"   â Business Intelligence metrics:")
	print(f"      Reports generated: {metrics['reports_generated']}")
	print(f"      Dashboards viewed: {metrics['dashboards_viewed']}")
	print(f"      Alerts triggered: {metrics['alerts_triggered']}")
	print(f"      Data points processed: {metrics['data_points_processed']}")
	print(f"      Active users: {len(metrics['active_users'])}")
	
	if metrics['query_response_times']:
		avg_response_time = statistics.mean(metrics['query_response_times'])
		print(f"      Average query response time: {avg_response_time:.3f}s")
	
	print(f"\nâ Business Intelligence service test completed!")
	print("   All reporting, analytics, dashboard, and insight features working correctly")

if __name__ == "__main__":
	asyncio.run(test_business_intelligence_service())

# Module initialization logging
def _log_business_intelligence_service_module_loaded():
	"""Log business intelligence service module loaded"""
	print("ð Advanced Business Intelligence Service module loaded")
	print("   - Real-time dashboards and KPIs")
	print("   - Custom report generation")
	print("   - Predictive analytics and insights")
	print("   - Executive summary reporting")
	print("   - Alert and notification system")
	print("   - Data visualization and analysis")

# Execute module loading log
_log_business_intelligence_service_module_loaded()