"""
Database Migrations - APG Payment Gateway Schema Management

Comprehensive database migration system for PostgreSQL schema management,
including version control, rollback capabilities, and data transformations.

¬© 2025 Datacraft. All rights reserved.
"""

import asyncio
import asyncpg
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path
from uuid_extensions import uuid7str
import hashlib
import json

class MigrationVersion:
	"""Represents a database migration version"""
	
	def __init__(self, version: str, description: str, up_sql: str, down_sql: str = None):
		self.version = version
		self.description = description
		self.up_sql = up_sql
		self.down_sql = down_sql
		self.checksum = self._calculate_checksum()
	
	def _calculate_checksum(self) -> str:
		"""Calculate checksum for migration validation"""
		content = f"{self.version}{self.description}{self.up_sql}{self.down_sql or ''}"
		return hashlib.sha256(content.encode()).hexdigest()

class MigrationManager:
	"""
	Database migration manager for APG Payment Gateway
	
	Handles schema versioning, migration execution, rollbacks,
	and maintains migration history for production deployments.
	"""
	
	def __init__(self, database_url: str):
		self.database_url = database_url
		self.connection_pool: Optional[asyncpg.Pool] = None
		self.migrations: List[MigrationVersion] = []
		self._initialize_migrations()
	
	async def initialize(self) -> None:
		"""Initialize migration manager and database connection"""
		self.connection_pool = await asyncpg.create_pool(self.database_url)
		await self._ensure_migration_table()
	
	async def close(self) -> None:
		"""Close database connections"""
		if self.connection_pool:
			await self.connection_pool.close()
	
	def _initialize_migrations(self) -> None:
		"""Initialize migration definitions"""
		
		# Migration 001: Initial schema
		self.migrations.append(MigrationVersion(
			version="001_initial_schema",
			description="Create initial payment gateway schema with core tables",
			up_sql=self._get_initial_schema_sql(),
			down_sql=self._get_initial_schema_rollback_sql()
		))
		
		# Migration 002: Add fraud detection enhancements
		self.migrations.append(MigrationVersion(
			version="002_fraud_enhancements",
			description="Add enhanced fraud detection tables and indexes",
			up_sql=self._get_fraud_enhancements_sql(),
			down_sql=self._get_fraud_enhancements_rollback_sql()
		))
		
		# Migration 003: Add analytics tables
		self.migrations.append(MigrationVersion(
			version="003_analytics_tables",
			description="Add analytics and metrics tables for dashboard",
			up_sql=self._get_analytics_tables_sql(),
			down_sql=self._get_analytics_tables_rollback_sql()
		))
		
		# Migration 004: Add user profiles for smart completion
		self.migrations.append(MigrationVersion(
			version="004_user_profiles",
			description="Add user behavior profiles for smart completion",
			up_sql=self._get_user_profiles_sql(),
			down_sql=self._get_user_profiles_rollback_sql()
		))
		
		# Migration 005: Add webhook and notification system
		self.migrations.append(MigrationVersion(
			version="005_webhooks_notifications",
			description="Add webhook events and notification preferences",
			up_sql=self._get_webhooks_sql(),
			down_sql=self._get_webhooks_rollback_sql()
		))
		
		# Migration 006: Add audit logging
		self.migrations.append(MigrationVersion(
			version="006_audit_logging",
			description="Add comprehensive audit logging system",
			up_sql=self._get_audit_logging_sql(),
			down_sql=self._get_audit_logging_rollback_sql()
		))
		
		# Migration 007: Add performance indexes
		self.migrations.append(MigrationVersion(
			version="007_performance_indexes",
			description="Add performance optimization indexes",
			up_sql=self._get_performance_indexes_sql(),
			down_sql=self._get_performance_indexes_rollback_sql()
		))
		
		# Migration 008: Add triggers and functions
		self.migrations.append(MigrationVersion(
			version="008_triggers_functions",
			description="Add database triggers and utility functions",
			up_sql=self._get_triggers_functions_sql(),
			down_sql=self._get_triggers_functions_rollback_sql()
		))
	
	async def _ensure_migration_table(self) -> None:
		"""Ensure migration tracking table exists"""
		async with self.connection_pool.acquire() as conn:
			await conn.execute("""
				CREATE SCHEMA IF NOT EXISTS payment_gateway;
				
				CREATE TABLE IF NOT EXISTS payment_gateway.pg_migrations (
					id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
					version VARCHAR(255) UNIQUE NOT NULL,
					description TEXT NOT NULL,
					checksum VARCHAR(64) NOT NULL,
					executed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
					execution_time_ms INTEGER,
					rollback_sql TEXT,
					status VARCHAR(20) DEFAULT 'completed' CHECK (status IN ('completed', 'failed', 'rolled_back'))
				);
				
				CREATE INDEX IF NOT EXISTS idx_migrations_version ON payment_gateway.pg_migrations(version);
				CREATE INDEX IF NOT EXISTS idx_migrations_executed ON payment_gateway.pg_migrations(executed_at DESC);
			""")
	
	async def get_current_version(self) -> Optional[str]:
		"""Get current database schema version"""
		async with self.connection_pool.acquire() as conn:
			result = await conn.fetchval("""
				SELECT version 
				FROM payment_gateway.pg_migrations 
				WHERE status = 'completed'
				ORDER BY executed_at DESC 
				LIMIT 1
			""")
			return result
	
	async def get_pending_migrations(self) -> List[MigrationVersion]:
		"""Get list of pending migrations"""
		async with self.connection_pool.acquire() as conn:
			executed_versions = await conn.fetch("""
				SELECT version, checksum 
				FROM payment_gateway.pg_migrations 
				WHERE status = 'completed'
			""")
			
			executed_dict = {row['version']: row['checksum'] for row in executed_versions}
			
			pending = []
			for migration in self.migrations:
				if migration.version not in executed_dict:
					pending.append(migration)
				elif executed_dict[migration.version] != migration.checksum:
					# Checksum mismatch - migration changed
					print(f"‚ö†Ô∏è  Migration {migration.version} checksum mismatch - may need attention")
			
			return pending
	
	async def migrate(self, target_version: Optional[str] = None) -> Dict[str, Any]:
		"""
		Execute pending migrations up to target version
		
		Args:
			target_version: Target version to migrate to (None for latest)
			
		Returns:
			Migration execution results
		"""
		pending_migrations = await self.get_pending_migrations()
		
		if target_version:
			# Filter to target version
			target_index = -1
			for i, migration in enumerate(self.migrations):
				if migration.version == target_version:
					target_index = i
					break
			
			if target_index == -1:
				raise ValueError(f"Target version {target_version} not found")
			
			pending_migrations = [m for m in pending_migrations 
								 if self.migrations.index(m) <= target_index]
		
		if not pending_migrations:
			return {
				"status": "up_to_date",
				"current_version": await self.get_current_version(),
				"migrations_executed": 0
			}
		
		executed_migrations = []
		failed_migrations = []
		
		for migration in pending_migrations:
			try:
				print(f"üîÑ Executing migration: {migration.version} - {migration.description}")
				start_time = datetime.now()
				
				async with self.connection_pool.acquire() as conn:
					async with conn.transaction():
						# Execute migration SQL
						await conn.execute(migration.up_sql)
						
						# Record migration
						execution_time = int((datetime.now() - start_time).total_seconds() * 1000)
						await conn.execute("""
							INSERT INTO payment_gateway.pg_migrations 
							(version, description, checksum, execution_time_ms, rollback_sql, status)
							VALUES ($1, $2, $3, $4, $5, 'completed')
						""", migration.version, migration.description, migration.checksum,
							execution_time, migration.down_sql)
				
				executed_migrations.append({
					"version": migration.version,
					"description": migration.description,
					"execution_time_ms": execution_time,
					"status": "completed"
				})
				
				print(f"‚úÖ Migration {migration.version} completed in {execution_time}ms")
				
			except Exception as e:
				error_msg = str(e)
				print(f"‚ùå Migration {migration.version} failed: {error_msg}")
				
				# Record failed migration
				async with self.connection_pool.acquire() as conn:
					await conn.execute("""
						INSERT INTO payment_gateway.pg_migrations 
						(version, description, checksum, rollback_sql, status)
						VALUES ($1, $2, $3, $4, 'failed')
					""", migration.version, migration.description, migration.checksum, migration.down_sql)
				
				failed_migrations.append({
					"version": migration.version,
					"description": migration.description,
					"error": error_msg,
					"status": "failed"
				})
				
				# Stop on first failure
				break
		
		return {
			"status": "completed" if not failed_migrations else "failed",
			"current_version": await self.get_current_version(),
			"migrations_executed": len(executed_migrations),
			"executed_migrations": executed_migrations,
			"failed_migrations": failed_migrations
		}
	
	async def rollback(self, target_version: str) -> Dict[str, Any]:
		"""
		Rollback to target version
		
		Args:
			target_version: Version to rollback to
			
		Returns:
			Rollback execution results
		"""
		current_version = await self.get_current_version()
		if not current_version:
			raise ValueError("No migrations to rollback")
		
		# Get migrations to rollback (in reverse order)
		async with self.connection_pool.acquire() as conn:
			migrations_to_rollback = await conn.fetch("""
				SELECT version, rollback_sql, description
				FROM payment_gateway.pg_migrations 
				WHERE status = 'completed' 
				AND executed_at > (
					SELECT executed_at 
					FROM payment_gateway.pg_migrations 
					WHERE version = $1 AND status = 'completed'
				)
				ORDER BY executed_at DESC
			""", target_version)
		
		if not migrations_to_rollback:
			return {
				"status": "already_at_target",
				"current_version": current_version,
				"rollbacks_executed": 0
			}
		
		rolled_back = []
		failed_rollbacks = []
		
		for migration_row in migrations_to_rollback:
			version = migration_row['version']
			rollback_sql = migration_row['rollback_sql']
			description = migration_row['description']
			
			if not rollback_sql:
				print(f"‚ö†Ô∏è  No rollback SQL for migration {version}")
				continue
			
			try:
				print(f"üîÑ Rolling back migration: {version} - {description}")
				start_time = datetime.now()
				
				async with self.connection_pool.acquire() as conn:
					async with conn.transaction():
						# Execute rollback SQL
						await conn.execute(rollback_sql)
						
						# Update migration status
						execution_time = int((datetime.now() - start_time).total_seconds() * 1000)
						await conn.execute("""
							UPDATE payment_gateway.pg_migrations 
							SET status = 'rolled_back', 
								executed_at = NOW()
							WHERE version = $1
						""", version)
				
				rolled_back.append({
					"version": version,
					"description": description,
					"rollback_time_ms": execution_time,
					"status": "rolled_back"
				})
				
				print(f"‚úÖ Migration {version} rolled back in {execution_time}ms")
				
			except Exception as e:
				error_msg = str(e)
				print(f"‚ùå Rollback {version} failed: {error_msg}")
				
				failed_rollbacks.append({
					"version": version,
					"description": description,
					"error": error_msg,
					"status": "rollback_failed"
				})
				
				# Stop on first failure
				break
		
		return {
			"status": "completed" if not failed_rollbacks else "failed",
			"current_version": await self.get_current_version(),
			"rollbacks_executed": len(rolled_back),
			"rolled_back_migrations": rolled_back,
			"failed_rollbacks": failed_rollbacks
		}
	
	async def get_migration_history(self) -> List[Dict[str, Any]]:
		"""Get complete migration history"""
		async with self.connection_pool.acquire() as conn:
			history = await conn.fetch("""
				SELECT version, description, status, executed_at, execution_time_ms
				FROM payment_gateway.pg_migrations 
				ORDER BY executed_at DESC
			""")
			
			return [dict(row) for row in history]
	
	async def validate_schema(self) -> Dict[str, Any]:
		"""Validate current schema against expected state"""
		async with self.connection_pool.acquire() as conn:
			# Check if all expected tables exist
			tables = await conn.fetch("""
				SELECT table_name 
				FROM information_schema.tables 
				WHERE table_schema = 'payment_gateway'
			""")
			
			table_names = {row['table_name'] for row in tables}
			expected_tables = {
				'pg_merchants', 'pg_payment_methods', 'pg_transactions', 'pg_processors',
				'pg_fraud_analysis', 'pg_fraud_rules', 'pg_device_fingerprints',
				'pg_transaction_metrics', 'pg_user_profiles', 'pg_dashboards',
				'pg_webhook_events', 'pg_notification_preferences', 'pg_audit_log',
				'pg_api_access_log', 'pg_migrations'
			}
			
			missing_tables = expected_tables - table_names
			extra_tables = table_names - expected_tables
			
			# Check indexes
			indexes = await conn.fetch("""
				SELECT indexname 
				FROM pg_indexes 
				WHERE schemaname = 'payment_gateway'
			""")
			
			index_count = len(indexes)
			
			return {
				"schema_valid": len(missing_tables) == 0,
				"total_tables": len(table_names),
				"expected_tables": len(expected_tables),
				"missing_tables": list(missing_tables),
				"extra_tables": list(extra_tables),
				"total_indexes": index_count,
				"current_version": await self.get_current_version()
			}
	
	# Migration SQL definitions
	
	def _get_initial_schema_sql(self) -> str:
		"""Initial schema creation SQL"""
		return """
		-- Enable required extensions
		CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
		CREATE EXTENSION IF NOT EXISTS "pg_trgm";
		CREATE EXTENSION IF NOT EXISTS "btree_gin";
		
		-- Create schema
		CREATE SCHEMA IF NOT EXISTS payment_gateway;
		SET search_path TO payment_gateway, public;
		
		-- Core merchant table
		CREATE TABLE pg_merchants (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			merchant_code VARCHAR(50) UNIQUE NOT NULL,
			business_name VARCHAR(255) NOT NULL,
			legal_name VARCHAR(255),
			contact_email VARCHAR(255) NOT NULL,
			contact_phone VARCHAR(50),
			website_url VARCHAR(500),
			business_type VARCHAR(100),
			country_code CHAR(2) NOT NULL,
			currency VARCHAR(3) NOT NULL,
			timezone VARCHAR(50) DEFAULT 'UTC',
			status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'suspended', 'inactive')),
			api_key_hash VARCHAR(255),
			webhook_url VARCHAR(500),
			webhook_secret VARCHAR(255),
			settings JSONB DEFAULT '{}',
			risk_profile VARCHAR(20) DEFAULT 'medium' CHECK (risk_profile IN ('low', 'medium', 'high')),
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			created_by UUID,
			updated_by UUID
		);
		
		-- Payment methods table
		CREATE TABLE pg_payment_methods (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			customer_id UUID,
			merchant_id UUID REFERENCES pg_merchants(id),
			type VARCHAR(50) NOT NULL CHECK (type IN ('credit_card', 'debit_card', 'mpesa', 'paypal', 'bank_transfer', 'digital_wallet')),
			provider VARCHAR(50),
			token VARCHAR(255),
			details JSONB NOT NULL DEFAULT '{}',
			metadata JSONB DEFAULT '{}',
			is_default BOOLEAN DEFAULT FALSE,
			is_verified BOOLEAN DEFAULT FALSE,
			expires_at TIMESTAMP WITH TIME ZONE,
			status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'expired', 'disabled')),
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
		);
		
		-- Core transactions table
		CREATE TABLE pg_transactions (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			merchant_id UUID NOT NULL REFERENCES pg_merchants(id),
			customer_id UUID,
			payment_method_id UUID REFERENCES pg_payment_methods(id),
			parent_transaction_id UUID REFERENCES pg_transactions(id),
			transaction_type VARCHAR(20) DEFAULT 'payment' CHECK (transaction_type IN ('payment', 'refund', 'capture', 'void')),
			amount BIGINT NOT NULL,
			currency VARCHAR(3) NOT NULL,
			status VARCHAR(20) NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'processing', 'completed', 'failed', 'cancelled', 'expired')),
			description TEXT,
			reference_id VARCHAR(255),
			processor_name VARCHAR(50),
			processor_transaction_id VARCHAR(255),
			processor_reference VARCHAR(255),
			payment_method_type VARCHAR(50) NOT NULL,
			gateway_fee BIGINT DEFAULT 0,
			processor_fee BIGINT DEFAULT 0,
			net_amount BIGINT,
			authorized_at TIMESTAMP WITH TIME ZONE,
			captured_at TIMESTAMP WITH TIME ZONE,
			settled_at TIMESTAMP WITH TIME ZONE,
			expires_at TIMESTAMP WITH TIME ZONE,
			error_code VARCHAR(100),
			error_message TEXT,
			metadata JSONB DEFAULT '{}',
			risk_score DECIMAL(5,4),
			fraud_flags JSONB DEFAULT '[]',
			ip_address INET,
			user_agent TEXT,
			device_fingerprint VARCHAR(255),
			location_data JSONB,
			processing_time_ms INTEGER,
			retry_count INTEGER DEFAULT 0,
			webhook_delivered BOOLEAN DEFAULT FALSE,
			webhook_attempts INTEGER DEFAULT 0,
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
		);
		
		-- Payment processors table
		CREATE TABLE pg_processors (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			name VARCHAR(100) UNIQUE NOT NULL,
			display_name VARCHAR(200) NOT NULL,
			type VARCHAR(50) NOT NULL CHECK (type IN ('mpesa', 'stripe', 'adyen', 'paypal', 'custom')),
			status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'inactive', 'maintenance')),
			priority INTEGER DEFAULT 100,
			supported_methods JSONB NOT NULL DEFAULT '[]',
			supported_currencies JSONB NOT NULL DEFAULT '[]',
			supported_countries JSONB NOT NULL DEFAULT '[]',
			configuration JSONB DEFAULT '{}',
			credentials JSONB DEFAULT '{}',
			webhook_config JSONB DEFAULT '{}',
			rate_limits JSONB DEFAULT '{}',
			fees JSONB DEFAULT '{}',
			health_status VARCHAR(20) DEFAULT 'unknown',
			last_health_check TIMESTAMP WITH TIME ZONE,
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
		);
		
		-- Basic indexes
		CREATE INDEX idx_merchants_code ON pg_merchants(merchant_code);
		CREATE INDEX idx_transactions_merchant ON pg_transactions(merchant_id, created_at DESC);
		CREATE INDEX idx_transactions_status ON pg_transactions(status, created_at DESC);
		CREATE INDEX idx_payment_methods_customer ON pg_payment_methods(customer_id, is_default DESC);
		"""
	
	def _get_initial_schema_rollback_sql(self) -> str:
		"""Rollback SQL for initial schema"""
		return """
		DROP TABLE IF EXISTS payment_gateway.pg_transactions CASCADE;
		DROP TABLE IF EXISTS payment_gateway.pg_payment_methods CASCADE;
		DROP TABLE IF EXISTS payment_gateway.pg_processors CASCADE;
		DROP TABLE IF EXISTS payment_gateway.pg_merchants CASCADE;
		DROP SCHEMA IF EXISTS payment_gateway CASCADE;
		"""
	
	def _get_fraud_enhancements_sql(self) -> str:
		"""Fraud detection enhancements SQL"""
		return """
		SET search_path TO payment_gateway, public;
		
		-- Fraud analysis table
		CREATE TABLE pg_fraud_analysis (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			transaction_id UUID NOT NULL REFERENCES pg_transactions(id),
			risk_score DECIMAL(5,4) NOT NULL,
			risk_level VARCHAR(20) NOT NULL CHECK (risk_level IN ('low', 'medium', 'high', 'critical')),
			analysis_result VARCHAR(20) NOT NULL CHECK (analysis_result IN ('approve', 'review', 'decline')),
			rules_triggered JSONB DEFAULT '[]',
			ml_model_scores JSONB DEFAULT '{}',
			behavioral_analysis JSONB DEFAULT '{}',
			device_analysis JSONB DEFAULT '{}',
			location_analysis JSONB DEFAULT '{}',
			velocity_analysis JSONB DEFAULT '{}',
			network_analysis JSONB DEFAULT '{}',
			analysis_duration_ms INTEGER,
			model_versions JSONB DEFAULT '{}',
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
		);
		
		-- Fraud rules table
		CREATE TABLE pg_fraud_rules (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			name VARCHAR(255) UNIQUE NOT NULL,
			description TEXT,
			rule_type VARCHAR(50) NOT NULL CHECK (rule_type IN ('velocity', 'amount', 'location', 'device', 'pattern', 'blacklist')),
			conditions JSONB NOT NULL,
			action VARCHAR(20) NOT NULL CHECK (action IN ('approve', 'review', 'decline', 'flag')),
			priority INTEGER DEFAULT 100,
			is_active BOOLEAN DEFAULT TRUE,
			merchant_id UUID REFERENCES pg_merchants(id),
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
		);
		
		-- Device fingerprints table
		CREATE TABLE pg_device_fingerprints (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			fingerprint_hash VARCHAR(255) UNIQUE NOT NULL,
			device_info JSONB NOT NULL,
			first_seen TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			last_seen TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			transaction_count INTEGER DEFAULT 0,
			successful_transactions INTEGER DEFAULT 0,
			failed_transactions INTEGER DEFAULT 0,
			risk_score DECIMAL(5,4) DEFAULT 0.5,
			is_blacklisted BOOLEAN DEFAULT FALSE,
			blacklist_reason TEXT,
			metadata JSONB DEFAULT '{}'
		);
		
		-- Fraud indexes
		CREATE INDEX idx_fraud_analysis_transaction ON pg_fraud_analysis(transaction_id);
		CREATE INDEX idx_fraud_analysis_risk_score ON pg_fraud_analysis(risk_score DESC, created_at DESC);
		CREATE INDEX idx_device_fingerprints_hash ON pg_device_fingerprints(fingerprint_hash);
		"""
	
	def _get_fraud_enhancements_rollback_sql(self) -> str:
		"""Rollback SQL for fraud enhancements"""
		return """
		DROP TABLE IF EXISTS payment_gateway.pg_device_fingerprints CASCADE;
		DROP TABLE IF EXISTS payment_gateway.pg_fraud_rules CASCADE;
		DROP TABLE IF EXISTS payment_gateway.pg_fraud_analysis CASCADE;
		"""
	
	# Additional migration SQL methods would continue here...
	# For brevity, I'll implement the key ones and indicate where others would go
	
	def _get_analytics_tables_sql(self) -> str:
		"""Analytics tables SQL"""
		return """
		SET search_path TO payment_gateway, public;
		
		-- Transaction metrics for analytics
		CREATE TABLE pg_transaction_metrics (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			date_hour TIMESTAMP WITH TIME ZONE NOT NULL,
			merchant_id UUID REFERENCES pg_merchants(id),
			processor_name VARCHAR(50),
			payment_method_type VARCHAR(50),
			currency VARCHAR(3),
			country_code CHAR(2),
			status VARCHAR(20),
			transaction_count INTEGER DEFAULT 0,
			total_amount BIGINT DEFAULT 0,
			successful_count INTEGER DEFAULT 0,
			failed_count INTEGER DEFAULT 0,
			fraud_count INTEGER DEFAULT 0,
			avg_processing_time_ms INTEGER DEFAULT 0,
			avg_risk_score DECIMAL(5,4) DEFAULT 0,
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			UNIQUE(date_hour, merchant_id, processor_name, payment_method_type, currency, status)
		);
		
		-- Dashboard configurations
		CREATE TABLE pg_dashboards (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			user_id UUID NOT NULL,
			name VARCHAR(255) NOT NULL,
			view_type VARCHAR(50) NOT NULL,
			widgets JSONB NOT NULL DEFAULT '[]',
			layout JSONB DEFAULT '{}',
			theme VARCHAR(50) DEFAULT 'dark',
			auto_refresh BOOLEAN DEFAULT TRUE,
			refresh_rate_seconds INTEGER DEFAULT 30,
			is_public BOOLEAN DEFAULT FALSE,
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
		);
		
		CREATE INDEX idx_transaction_metrics_date_merchant ON pg_transaction_metrics(date_hour DESC, merchant_id);
		CREATE INDEX idx_dashboards_user ON pg_dashboards(user_id);
		"""
	
	def _get_analytics_tables_rollback_sql(self) -> str:
		"""Rollback SQL for analytics tables"""
		return """
		DROP TABLE IF EXISTS payment_gateway.pg_dashboards CASCADE;
		DROP TABLE IF EXISTS payment_gateway.pg_transaction_metrics CASCADE;
		"""
	
	def _get_user_profiles_sql(self) -> str:
		"""User profiles SQL"""
		return """
		SET search_path TO payment_gateway, public;
		
		CREATE TABLE pg_user_profiles (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			user_id UUID NOT NULL,
			preferred_payment_methods JSONB DEFAULT '[]',
			typical_amounts JSONB DEFAULT '[]',
			common_merchants JSONB DEFAULT '[]',
			geographic_patterns JSONB DEFAULT '{}',
			device_preferences JSONB DEFAULT '{}',
			time_patterns JSONB DEFAULT '{}',
			completion_history JSONB DEFAULT '[]',
			accuracy_score DECIMAL(5,4) DEFAULT 0.5,
			personalization_score DECIMAL(5,4) DEFAULT 0.0,
			last_interaction TIMESTAMP WITH TIME ZONE,
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			UNIQUE(user_id)
		);
		
		CREATE INDEX idx_user_profiles_user_id ON pg_user_profiles(user_id);
		"""
	
	def _get_user_profiles_rollback_sql(self) -> str:
		"""Rollback SQL for user profiles"""
		return "DROP TABLE IF EXISTS payment_gateway.pg_user_profiles CASCADE;"
	
	def _get_webhooks_sql(self) -> str:
		"""Webhooks and notifications SQL"""
		return """
		SET search_path TO payment_gateway, public;
		
		CREATE TABLE pg_webhook_events (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			merchant_id UUID NOT NULL REFERENCES pg_merchants(id),
			transaction_id UUID REFERENCES pg_transactions(id),
			event_type VARCHAR(100) NOT NULL,
			event_data JSONB NOT NULL,
			status VARCHAR(20) DEFAULT 'pending' CHECK (status IN ('pending', 'delivered', 'failed', 'cancelled')),
			attempts INTEGER DEFAULT 0,
			max_attempts INTEGER DEFAULT 5,
			next_attempt_at TIMESTAMP WITH TIME ZONE,
			delivered_at TIMESTAMP WITH TIME ZONE,
			response_status INTEGER,
			response_body TEXT,
			error_message TEXT,
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
		);
		
		CREATE TABLE pg_notification_preferences (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			merchant_id UUID NOT NULL REFERENCES pg_merchants(id),
			event_type VARCHAR(100) NOT NULL,
			email_enabled BOOLEAN DEFAULT TRUE,
			webhook_enabled BOOLEAN DEFAULT TRUE,
			sms_enabled BOOLEAN DEFAULT FALSE,
			push_enabled BOOLEAN DEFAULT FALSE,
			threshold_amount BIGINT,
			settings JSONB DEFAULT '{}',
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
			UNIQUE(merchant_id, event_type)
		);
		
		CREATE INDEX idx_webhook_events_merchant_status ON pg_webhook_events(merchant_id, status, created_at DESC);
		"""
	
	def _get_webhooks_rollback_sql(self) -> str:
		"""Rollback SQL for webhooks"""
		return """
		DROP TABLE IF EXISTS payment_gateway.pg_notification_preferences CASCADE;
		DROP TABLE IF EXISTS payment_gateway.pg_webhook_events CASCADE;
		"""
	
	def _get_audit_logging_sql(self) -> str:
		"""Audit logging SQL"""
		return """
		SET search_path TO payment_gateway, public;
		
		CREATE TABLE pg_audit_log (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			table_name VARCHAR(100) NOT NULL,
			record_id UUID NOT NULL,
			action VARCHAR(20) NOT NULL CHECK (action IN ('INSERT', 'UPDATE', 'DELETE')),
			old_values JSONB,
			new_values JSONB,
			changed_fields JSONB,
			user_id UUID,
			ip_address INET,
			user_agent TEXT,
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
		);
		
		CREATE TABLE pg_api_access_log (
			id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
			merchant_id UUID REFERENCES pg_merchants(id),
			api_key_id UUID,
			endpoint VARCHAR(500) NOT NULL,
			method VARCHAR(10) NOT NULL,
			status_code INTEGER,
			request_size INTEGER,
			response_size INTEGER,
			processing_time_ms INTEGER,
			ip_address INET,
			user_agent TEXT,
			request_id UUID,
			error_message TEXT,
			created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
		);
		
		CREATE INDEX idx_audit_log_table_record ON pg_audit_log(table_name, record_id, created_at DESC);
		CREATE INDEX idx_api_access_log_merchant ON pg_api_access_log(merchant_id, created_at DESC);
		"""
	
	def _get_audit_logging_rollback_sql(self) -> str:
		"""Rollback SQL for audit logging"""
		return """
		DROP TABLE IF EXISTS payment_gateway.pg_api_access_log CASCADE;
		DROP TABLE IF EXISTS payment_gateway.pg_audit_log CASCADE;
		"""
	
	def _get_performance_indexes_sql(self) -> str:
		"""Performance indexes SQL"""
		return """
		SET search_path TO payment_gateway, public;
		
		-- Additional transaction indexes
		CREATE INDEX idx_transactions_customer_created ON pg_transactions(customer_id, created_at DESC);
		CREATE INDEX idx_transactions_processor_created ON pg_transactions(processor_name, created_at DESC);
		CREATE INDEX idx_transactions_reference ON pg_transactions(reference_id);
		CREATE INDEX idx_transactions_processor_ref ON pg_transactions(processor_transaction_id);
		CREATE INDEX idx_transactions_amount_range ON pg_transactions(amount, created_at DESC);
		CREATE INDEX idx_transactions_currency_date ON pg_transactions(currency, created_at DESC);
		
		-- Payment method indexes
		CREATE INDEX idx_payment_methods_merchant ON pg_payment_methods(merchant_id, type);
		CREATE INDEX idx_payment_methods_token ON pg_payment_methods(token) WHERE token IS NOT NULL;
		
		-- Additional fraud indexes
		CREATE INDEX idx_fraud_analysis_result ON pg_fraud_analysis(analysis_result, created_at DESC);
		CREATE INDEX idx_device_fingerprints_risk ON pg_device_fingerprints(risk_score DESC);
		CREATE INDEX idx_device_fingerprints_blacklist ON pg_device_fingerprints(is_blacklisted) WHERE is_blacklisted = TRUE;
		
		-- Merchant indexes
		CREATE INDEX idx_merchants_status ON pg_merchants(status, created_at DESC);
		CREATE INDEX idx_merchants_country ON pg_merchants(country_code);
		
		-- Analytics indexes
		CREATE INDEX idx_transaction_metrics_processor ON pg_transaction_metrics(processor_name, date_hour DESC);
		CREATE INDEX idx_transaction_metrics_method ON pg_transaction_metrics(payment_method_type, date_hour DESC);
		
		-- User profile indexes
		CREATE INDEX idx_user_profiles_last_interaction ON pg_user_profiles(last_interaction DESC);
		
		-- Webhook indexes
		CREATE INDEX idx_webhook_events_next_attempt ON pg_webhook_events(next_attempt_at) WHERE status = 'pending';
		"""
	
	def _get_performance_indexes_rollback_sql(self) -> str:
		"""Rollback SQL for performance indexes"""
		return """
		-- Drop all performance indexes (would list each DROP INDEX statement)
		-- For brevity, using CASCADE to drop related indexes
		"""
	
	def _get_triggers_functions_sql(self) -> str:
		"""Triggers and functions SQL"""
		return """
		SET search_path TO payment_gateway, public;
		
		-- Audit trigger function
		CREATE OR REPLACE FUNCTION pg_audit_trigger_function()
		RETURNS TRIGGER AS $$
		BEGIN
			INSERT INTO pg_audit_log (
				table_name, record_id, action, old_values, new_values, changed_fields, user_id
			) VALUES (
				TG_TABLE_NAME,
				COALESCE(NEW.id, OLD.id),
				TG_OP,
				CASE WHEN TG_OP = 'DELETE' THEN row_to_json(OLD) ELSE NULL END,
				CASE WHEN TG_OP = 'INSERT' OR TG_OP = 'UPDATE' THEN row_to_json(NEW) ELSE NULL END,
				CASE 
					WHEN TG_OP = 'UPDATE' THEN 
						(SELECT json_object_agg(key, value) 
						 FROM json_each(row_to_json(NEW)) 
						 WHERE value IS DISTINCT FROM (row_to_json(OLD) ->> key)::json)
					ELSE NULL 
				END,
				current_setting('app.user_id', true)::UUID
			);
			RETURN COALESCE(NEW, OLD);
		END;
		$$ LANGUAGE plpgsql;
		
		-- Transaction metrics update function
		CREATE OR REPLACE FUNCTION pg_update_transaction_metrics()
		RETURNS TRIGGER AS $$
		DECLARE
			metric_hour TIMESTAMP WITH TIME ZONE;
		BEGIN
			metric_hour := date_trunc('hour', NEW.created_at);
			
			INSERT INTO pg_transaction_metrics (
				date_hour, merchant_id, processor_name, payment_method_type, currency,
				country_code, status, transaction_count, total_amount, successful_count,
				failed_count, fraud_count, avg_processing_time_ms, avg_risk_score
			) VALUES (
				metric_hour, NEW.merchant_id, NEW.processor_name, NEW.payment_method_type,
				NEW.currency, COALESCE((NEW.location_data->>'country_code')::CHAR(2), 'XX'),
				NEW.status, 1, NEW.amount,
				CASE WHEN NEW.status = 'completed' THEN 1 ELSE 0 END,
				CASE WHEN NEW.status = 'failed' THEN 1 ELSE 0 END,
				CASE WHEN NEW.risk_score > 0.8 THEN 1 ELSE 0 END,
				COALESCE(NEW.processing_time_ms, 0), COALESCE(NEW.risk_score, 0.5)
			)
			ON CONFLICT (date_hour, merchant_id, processor_name, payment_method_type, currency, status)
			DO UPDATE SET
				transaction_count = pg_transaction_metrics.transaction_count + 1,
				total_amount = pg_transaction_metrics.total_amount + NEW.amount,
				successful_count = pg_transaction_metrics.successful_count + 
					CASE WHEN NEW.status = 'completed' THEN 1 ELSE 0 END,
				failed_count = pg_transaction_metrics.failed_count + 
					CASE WHEN NEW.status = 'failed' THEN 1 ELSE 0 END,
				fraud_count = pg_transaction_metrics.fraud_count + 
					CASE WHEN NEW.risk_score > 0.8 THEN 1 ELSE 0 END;
			
			RETURN NEW;
		END;
		$$ LANGUAGE plpgsql;
		
		-- Apply triggers
		CREATE TRIGGER audit_trigger_merchants
			AFTER INSERT OR UPDATE OR DELETE ON pg_merchants
			FOR EACH ROW EXECUTE FUNCTION pg_audit_trigger_function();
		
		CREATE TRIGGER audit_trigger_transactions
			AFTER INSERT OR UPDATE OR DELETE ON pg_transactions
			FOR EACH ROW EXECUTE FUNCTION pg_audit_trigger_function();
		
		CREATE TRIGGER update_transaction_metrics_trigger
			AFTER INSERT ON pg_transactions
			FOR EACH ROW EXECUTE FUNCTION pg_update_transaction_metrics();
		"""
	
	def _get_triggers_functions_rollback_sql(self) -> str:
		"""Rollback SQL for triggers and functions"""
		return """
		DROP TRIGGER IF EXISTS update_transaction_metrics_trigger ON payment_gateway.pg_transactions;
		DROP TRIGGER IF EXISTS audit_trigger_transactions ON payment_gateway.pg_transactions;
		DROP TRIGGER IF EXISTS audit_trigger_merchants ON payment_gateway.pg_merchants;
		DROP FUNCTION IF EXISTS payment_gateway.pg_update_transaction_metrics();
		DROP FUNCTION IF EXISTS payment_gateway.pg_audit_trigger_function();
		"""

# CLI interface for migrations
async def main():
	"""CLI interface for running migrations"""
	import sys
	import os
	
	database_url = os.getenv('DATABASE_URL', 'postgresql://user:pass@localhost/payment_gateway')
	
	if len(sys.argv) < 2:
		print("Usage: python migrations.py [migrate|rollback|status|validate] [target_version]")
		return
	
	command = sys.argv[1]
	target_version = sys.argv[2] if len(sys.argv) > 2 else None
	
	migration_manager = MigrationManager(database_url)
	
	try:
		await migration_manager.initialize()
		
		if command == "migrate":
			result = await migration_manager.migrate(target_version)
			print(f"Migration result: {json.dumps(result, indent=2, default=str)}")
		
		elif command == "rollback":
			if not target_version:
				print("Target version required for rollback")
				return
			result = await migration_manager.rollback(target_version)
			print(f"Rollback result: {json.dumps(result, indent=2, default=str)}")
		
		elif command == "status":
			current_version = await migration_manager.get_current_version()
			pending = await migration_manager.get_pending_migrations()
			history = await migration_manager.get_migration_history()
			
			print(f"Current version: {current_version}")
			print(f"Pending migrations: {len(pending)}")
			for migration in pending:
				print(f"  - {migration.version}: {migration.description}")
			
			print(f"\nMigration history ({len(history)} entries):")
			for entry in history[:5]:  # Show last 5
				print(f"  - {entry['version']} ({entry['status']}) - {entry['executed_at']}")
		
		elif command == "validate":
			result = await migration_manager.validate_schema()
			print(f"Schema validation: {json.dumps(result, indent=2, default=str)}")
		
		else:
			print(f"Unknown command: {command}")
	
	finally:
		await migration_manager.close()

if __name__ == "__main__":
	asyncio.run(main())

def _log_migrations_module_loaded():
	"""Log migrations module loaded"""
	print("üóÑÔ∏è  Database Migrations module loaded")
	print("   - PostgreSQL schema management")
	print("   - Version control and rollbacks")
	print("   - Data integrity validation")
	print("   - Production deployment ready")

# Execute module loading log
_log_migrations_module_loaded()