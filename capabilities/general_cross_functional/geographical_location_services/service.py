"""
Geographical Location Services - Comprehensive Service Layer

Enterprise geospatial intelligence service providing:
- Advanced geocoding and address validation
- Real-time geofencing and location tracking
- Territory management and spatial analytics  
- Route optimization and logistics planning
- Geographic compliance and regulatory management

Copyright Â© 2025 Datacraft
Author: Nyimbi Odero <nyimbi@gmail.com>
Website: www.datacraft.co.ke
"""

import asyncio
import json
import math
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple, Union, Set
from decimal import Decimal
import logging
from pathlib import Path
from collections import defaultdict
import statistics
import numpy as np
from scipy import spatial, stats
from sklearn.cluster import DBSCAN, KMeans
from sklearn.preprocessing import StandardScaler

from .models import *

# =============================================================================
# Configuration and Constants
# =============================================================================

logger = logging.getLogger(__name__)

# Earth radius in kilometers for distance calculations
EARTH_RADIUS_KM = 6371.0

# Geocoding service providers
GEOCODING_PROVIDERS = {
	"google": "Google Maps Geocoding API",
	"mapbox": "Mapbox Geocoding API", 
	"opencage": "OpenCage Geocoding API",
	"nominatim": "Nominatim (OpenStreetMap)",
	"here": "HERE Geocoding API"
}

# Default accuracy thresholds in meters
ACCURACY_THRESHOLDS = {
	GLSLocationAccuracy.HIGH: 5.0,
	GLSLocationAccuracy.MEDIUM: 50.0,
	GLSLocationAccuracy.LOW: 500.0,
	GLSLocationAccuracy.APPROXIMATE: float('inf')
}

# =============================================================================
# Exception Classes
# =============================================================================

class GLSServiceError(Exception):
	"""Base exception for Geographical Location Services."""
	pass

class GLSGeocodingError(GLSServiceError):
	"""Exception for geocoding-related errors."""
	pass

class GLSGeofenceError(GLSServiceError):
	"""Exception for geofence-related errors."""
	pass

class GLSRouteOptimizationError(GLSServiceError):
	"""Exception for route optimization errors."""
	pass

class GLSComplianceError(GLSServiceError):
	"""Exception for compliance-related errors."""
	pass

# =============================================================================
# Advanced Utility Functions
# =============================================================================

def generate_h3_indices(coordinate: GLSCoordinate) -> Dict[int, str]:
	"""Generate H3 indices for all resolution levels."""
	# Mock H3 implementation (in production, use h3-py library)
	h3_indices = {}
	base_hash = abs(hash(f"{coordinate.latitude},{coordinate.longitude}"))
	
	for resolution in range(11):  # H3 resolutions 0-10
		# Simulate H3 index generation
		resolution_hash = base_hash >> (resolution * 2)  # Vary by resolution
		h3_index = f"8{resolution:01x}{resolution_hash:013x}"
		h3_indices[resolution] = h3_index
	
	return h3_indices

def calculate_geohash(coordinate: GLSCoordinate, precision: int = 12) -> str:
	"""Generate geohash for a coordinate."""
	# Mock geohash implementation
	lat_range = [-90.0, 90.0]
	lng_range = [-180.0, 180.0]
	
	lat = coordinate.latitude
	lng = coordinate.longitude
	
	geohash = ""
	bits = 0
	bit_count = 0
	even_bit = True
	
	while len(geohash) < precision:
		if even_bit:
			# Longitude
			mid = (lng_range[0] + lng_range[1]) / 2
			if lng >= mid:
				bits = (bits << 1) | 1
				lng_range[0] = mid
			else:
				bits = bits << 1
				lng_range[1] = mid
		else:
			# Latitude
			mid = (lat_range[0] + lat_range[1]) / 2
			if lat >= mid:
				bits = (bits << 1) | 1
				lat_range[0] = mid
			else:
				bits = bits << 1
				lat_range[1] = mid
		
		even_bit = not even_bit
		bit_count += 1
		
		if bit_count == 5:
			# Convert 5 bits to base32
			base32 = "0123456789bcdefghjkmnpqrstuvwxyz"
			geohash += base32[bits]
			bits = 0
			bit_count = 0
	
	return geohash

def fuzzy_string_match(query: str, target: str, algorithm: GLSFuzzyMatchType) -> float:
	"""Calculate fuzzy string similarity score."""
	if algorithm == GLSFuzzyMatchType.LEVENSHTEIN:
		return _levenshtein_similarity(query, target)
	elif algorithm == GLSFuzzyMatchType.JARO_WINKLER:
		return _jaro_winkler_similarity(query, target)
	elif algorithm == GLSFuzzyMatchType.FUZZY_PARTIAL:
		return _fuzzy_partial_similarity(query, target)
	else:
		# Default to simple similarity
		return _simple_similarity(query, target)

def _levenshtein_similarity(s1: str, s2: str) -> float:
	"""Calculate Levenshtein similarity."""
	if len(s1) == 0:
		return 0.0 if len(s2) > 0 else 1.0
	if len(s2) == 0:
		return 0.0
	
	# Create distance matrix
	d = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]
	
	for i in range(len(s1) + 1):
		d[i][0] = i
	for j in range(len(s2) + 1):
		d[0][j] = j
	
	for i in range(1, len(s1) + 1):
		for j in range(1, len(s2) + 1):
			cost = 0 if s1[i-1] == s2[j-1] else 1
			d[i][j] = min(
				d[i-1][j] + 1,      # deletion
				d[i][j-1] + 1,      # insertion
				d[i-1][j-1] + cost  # substitution
			)
	
	distance = d[len(s1)][len(s2)]
	max_len = max(len(s1), len(s2))
	return 1.0 - (distance / max_len)

def _jaro_winkler_similarity(s1: str, s2: str) -> float:
	"""Calculate Jaro-Winkler similarity (simplified version)."""
	if s1 == s2:
		return 1.0
	
	# Find common characters
	match_window = max(len(s1), len(s2)) // 2 - 1
	if match_window < 0:
		match_window = 0
	
	s1_matches = [False] * len(s1)
	s2_matches = [False] * len(s2)
	
	matches = 0
	transpositions = 0
	
	# Find matches
	for i in range(len(s1)):
		start = max(0, i - match_window)
		end = min(i + match_window + 1, len(s2))
		
		for j in range(start, end):
			if s2_matches[j] or s1[i] != s2[j]:
				continue
			s1_matches[i] = s2_matches[j] = True
			matches += 1
			break
	
	if matches == 0:
		return 0.0
	
	# Find transpositions
	k = 0
	for i in range(len(s1)):
		if not s1_matches[i]:
			continue
		while not s2_matches[k]:
			k += 1
		if s1[i] != s2[k]:
			transpositions += 1
		k += 1
	
	jaro = (matches / len(s1) + matches / len(s2) + (matches - transpositions/2) / matches) / 3.0
	
	# Winkler modification (simplified)
	prefix = 0
	for i in range(min(len(s1), len(s2), 4)):
		if s1[i] == s2[i]:
			prefix += 1
		else:
			break
	
	return jaro + (0.1 * prefix * (1 - jaro))

def _fuzzy_partial_similarity(query: str, target: str) -> float:
	"""Calculate fuzzy partial similarity."""
	query = query.lower().strip()
	target = target.lower().strip()
	
	if query in target or target in query:
		return 0.9
	
	# Token-based matching
	query_tokens = set(query.split())
	target_tokens = set(target.split())
	
	if not query_tokens or not target_tokens:
		return 0.0
	
	intersection = query_tokens.intersection(target_tokens)
	union = query_tokens.union(target_tokens)
	
	return len(intersection) / len(union)

def _simple_similarity(s1: str, s2: str) -> float:
	"""Simple character-based similarity."""
	s1, s2 = s1.lower(), s2.lower()
	if s1 == s2:
		return 1.0
	
	# Character frequency similarity
	all_chars = set(s1 + s2)
	s1_freq = {char: s1.count(char) for char in all_chars}
	s2_freq = {char: s2.count(char) for char in all_chars}
	
	dot_product = sum(s1_freq[char] * s2_freq[char] for char in all_chars)
	magnitude1 = math.sqrt(sum(freq ** 2 for freq in s1_freq.values()))
	magnitude2 = math.sqrt(sum(freq ** 2 for freq in s2_freq.values()))
	
	if magnitude1 == 0 or magnitude2 == 0:
		return 0.0
	
	return dot_product / (magnitude1 * magnitude2)

def detect_trajectory_patterns(trajectory: List[GLSCoordinate]) -> Dict[GLSTrajectoryPattern, float]:
	"""Detect movement patterns in a trajectory."""
	if len(trajectory) < 3:
		return {}
	
	patterns = {}
	
	# Calculate basic metrics
	distances = []
	bearings = []
	
	for i in range(len(trajectory) - 1):
		distance = calculate_distance(trajectory[i], trajectory[i + 1])
		bearing = calculate_bearing(trajectory[i], trajectory[i + 1])
		distances.append(distance)
		bearings.append(bearing)
	
	# Linear pattern detection
	bearing_variance = np.var(bearings) if bearings else 0
	if bearing_variance < 100:  # Low variance in direction
		patterns[GLSTrajectoryPattern.LINEAR] = 1.0 - (bearing_variance / 100)
	
	# Circular pattern detection
	start_coord = trajectory[0]
	end_coord = trajectory[-1]
	return_distance = calculate_distance(start_coord, end_coord)
	total_distance = sum(distances)
	
	if total_distance > 0:
		circularity = 1.0 - (return_distance / total_distance)
		if circularity > 0.5:
			patterns[GLSTrajectoryPattern.CIRCULAR] = circularity
	
	# Random walk detection
	if len(set(bearings[:10])) > 7:  # High directional variance
		patterns[GLSTrajectoryPattern.RANDOM_WALK] = 0.8
	
	return patterns

def calculate_spatial_autocorrelation(locations: List[GLSCoordinate], values: List[float]) -> float:
	"""Calculate Moran's I spatial autocorrelation."""
	if len(locations) != len(values) or len(locations) < 3:
		return 0.0
	
	n = len(locations)
	mean_value = statistics.mean(values)
	
	# Calculate spatial weights (inverse distance)
	weights = np.zeros((n, n))
	for i in range(n):
		for j in range(n):
			if i != j:
				distance = calculate_distance(locations[i], locations[j])
				weights[i][j] = 1.0 / (distance + 0.001)  # Avoid division by zero
	
	# Calculate Moran's I
	numerator = 0.0
	denominator = 0.0
	w_sum = 0.0
	
	for i in range(n):
		for j in range(n):
			if i != j:
				numerator += weights[i][j] * (values[i] - mean_value) * (values[j] - mean_value)
				w_sum += weights[i][j]
	
	for value in values:
		denominator += (value - mean_value) ** 2
	
	if denominator == 0 or w_sum == 0:
		return 0.0
	
	moran_i = (n / w_sum) * (numerator / denominator)
	return max(-1.0, min(1.0, moran_i))  # Clamp to [-1, 1]

# =============================================================================
# Utility Functions
# =============================================================================

def calculate_distance(coord1: GLSCoordinate, coord2: GLSCoordinate) -> float:
	"""Calculate the great circle distance between two coordinates in kilometers."""
	lat1_rad = math.radians(coord1.latitude)
	lon1_rad = math.radians(coord1.longitude)
	lat2_rad = math.radians(coord2.latitude)
	lon2_rad = math.radians(coord2.longitude)
	
	dlat = lat2_rad - lat1_rad
	dlon = lon2_rad - lon1_rad
	
	a = (math.sin(dlat / 2) ** 2 + 
		 math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2) ** 2)
	c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
	
	return EARTH_RADIUS_KM * c

def calculate_bearing(coord1: GLSCoordinate, coord2: GLSCoordinate) -> float:
	"""Calculate the initial bearing from coord1 to coord2 in degrees."""
	lat1_rad = math.radians(coord1.latitude)
	lat2_rad = math.radians(coord2.latitude)
	dlon_rad = math.radians(coord2.longitude - coord1.longitude)
	
	y = math.sin(dlon_rad) * math.cos(lat2_rad)
	x = (math.cos(lat1_rad) * math.sin(lat2_rad) - 
		 math.sin(lat1_rad) * math.cos(lat2_rad) * math.cos(dlon_rad))
	
	bearing_rad = math.atan2(y, x)
	bearing_deg = math.degrees(bearing_rad)
	
	return (bearing_deg + 360) % 360

def point_in_polygon(point: GLSCoordinate, polygon: List[GLSCoordinate]) -> bool:
	"""Check if a point is inside a polygon using ray casting algorithm."""
	if len(polygon) < 3:
		return False
	
	x, y = point.longitude, point.latitude
	n = len(polygon)
	inside = False
	
	p1x, p1y = polygon[0].longitude, polygon[0].latitude
	for i in range(1, n + 1):
		p2x, p2y = polygon[i % n].longitude, polygon[i % n].latitude
		if y > min(p1y, p2y):
			if y <= max(p1y, p2y):
				if x <= max(p1x, p2x):
					if p1y != p2y:
						xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x
					if p1x == p2x or x <= xinters:
						inside = not inside
		p1x, p1y = p2x, p2y
	
	return inside

def point_in_circle(point: GLSCoordinate, center: GLSCoordinate, radius_km: float) -> bool:
	"""Check if a point is within a circular geofence."""
	distance_km = calculate_distance(point, center)
	return distance_km <= radius_km

# =============================================================================
# Core Service Classes
# =============================================================================

class GLSGeocodingService:
	"""Advanced geocoding and address validation service."""
	
	def __init__(self, 
				 default_provider: str = "google",
				 api_keys: Optional[Dict[str, str]] = None,
				 enable_caching: bool = True,
				 cache_ttl_hours: int = 24):
		self.default_provider = default_provider
		self.api_keys = api_keys or {}
		self.enable_caching = enable_caching
		self.cache_ttl_hours = cache_ttl_hours
		self._geocode_cache: Dict[str, Dict[str, Any]] = {}
		
		logger.info(f"Initialized geocoding service with provider: {default_provider}")
	
	async def geocode_address(self, 
							  address: GLSAddress, 
							  provider: Optional[str] = None) -> GLSAddress:
		"""Geocode a single address and return enriched address data."""
		provider = provider or self.default_provider
		
		if not address.formatted_address:
			# Build formatted address from components
			address_parts = []
			if address.street_number:
				address_parts.append(address.street_number)
			if address.street_name:
				address_parts.append(f"{address.street_name} {address.street_type or ''}".strip())
			if address.unit_number:
				address_parts.append(f"Unit {address.unit_number}")
			if address.city:
				address_parts.append(address.city)
			if address.state_province:
				address_parts.append(address.state_province)
			if address.postal_code:
				address_parts.append(address.postal_code)
			if address.country:
				address_parts.append(address.country)
			
			address.formatted_address = ", ".join(filter(None, address_parts))
		
		# Check cache first
		cache_key = f"{provider}:{address.formatted_address}"
		if self.enable_caching and cache_key in self._geocode_cache:
			cached_result = self._geocode_cache[cache_key]
			if (datetime.utcnow() - cached_result['timestamp']).total_seconds() < self.cache_ttl_hours * 3600:
				logger.debug(f"Using cached geocoding result for: {address.formatted_address}")
				return GLSAddress(**cached_result['data'])
		
		try:
			# Simulate geocoding API call
			logger.info(f"Geocoding address with {provider}: {address.formatted_address}")
			
			# Mock geocoding result (in production, call actual API)
			geocoded_coordinate = GLSCoordinate(
				latitude=40.7128 + (hash(address.formatted_address) % 1000) / 10000,
				longitude=-74.0060 + (hash(address.formatted_address) % 1000) / 10000,
				accuracy_meters=10.0,
				timestamp=datetime.utcnow(),
				accuracy_level=GLSLocationAccuracy.HIGH,
				source=provider
			)
			
			# Update address with geocoding results
			address.coordinate = geocoded_coordinate
			address.geocoding_accuracy = GLSLocationAccuracy.HIGH
			address.geocoding_source = provider
			address.geocoding_timestamp = datetime.utcnow()
			address.is_validated = True
			address.validation_score = 0.95
			
			# Generate H3 indices and geohash
			geocoded_coordinate.h3_indices = generate_h3_indices(geocoded_coordinate)
			geocoded_coordinate.geohash = calculate_geohash(geocoded_coordinate)
			
			# Mock administrative resolution
			address.admin_hierarchy = {
				GLSAdminLevel.COUNTRY: "United States",
				GLSAdminLevel.ADMIN1: "New York",
				GLSAdminLevel.ADMIN2: "New York County",
				GLSAdminLevel.LOCALITY: "New York"
			}
			address.geonames_id = 5128581  # NYC GeoNames ID
			
			# Cache the result
			if self.enable_caching:
				self._geocode_cache[cache_key] = {
					'data': address.model_dump(),
					'timestamp': datetime.utcnow()
				}
			
			return address
			
		except Exception as e:
			logger.error(f"Geocoding failed for address {address.formatted_address}: {str(e)}")
			raise GLSGeocodingError(f"Failed to geocode address: {str(e)}")
	
	async def batch_geocode(self, request: GLSBatchGeocodeRequest) -> List[GLSAddress]:
		"""Perform batch geocoding of multiple addresses."""
		logger.info(f"Starting batch geocoding of {len(request.addresses)} addresses")
		
		results = []
		tasks = []
		
		# Create geocoding tasks
		for address in request.addresses:
			task = self.geocode_address(address)
			tasks.append(task)
		
		# Execute geocoding tasks concurrently
		try:
			geocoded_addresses = await asyncio.gather(*tasks, return_exceptions=True)
			
			for i, result in enumerate(geocoded_addresses):
				if isinstance(result, Exception):
					logger.warning(f"Failed to geocode address {i}: {str(result)}")
					results.append(request.addresses[i])  # Return original address
				else:
					results.append(result)
			
			logger.info(f"Completed batch geocoding: {len(results)} addresses processed")
			return results
			
		except Exception as e:
			logger.error(f"Batch geocoding failed: {str(e)}")
			raise GLSGeocodingError(f"Batch geocoding failed: {str(e)}")
	
	async def reverse_geocode(self, coordinate: GLSCoordinate) -> GLSAddress:
		"""Reverse geocode a coordinate to get address information."""
		try:
			logger.info(f"Reverse geocoding coordinate: {coordinate.latitude}, {coordinate.longitude}")
			
			# Mock reverse geocoding result (in production, call actual API)
			address = GLSAddress(
				street_number="123",
				street_name="Main",
				street_type="St",
				city="New York",
				state_province="NY",
				postal_code="10001",
				country="US",
				formatted_address="123 Main St, New York, NY 10001, US",
				coordinate=coordinate,
				geocoding_accuracy=GLSLocationAccuracy.HIGH,
				geocoding_source=self.default_provider,
				geocoding_timestamp=datetime.utcnow(),
				is_validated=True,
				validation_score=0.90
			)
			
			return address
			
		except Exception as e:
			logger.error(f"Reverse geocoding failed: {str(e)}")
			raise GLSGeocodingError(f"Reverse geocoding failed: {str(e)}")

class GLSGeofencingService:
	"""Advanced geofencing and location monitoring service."""
	
	def __init__(self):
		self._geofences: Dict[str, GLSGeofence] = {}
		self._entity_locations: Dict[str, GLSEntityLocation] = {}
		self._location_events: List[GLSLocationEvent] = []
		
		logger.info("Initialized geofencing service")
	
	async def create_geofence(self, 
							  geofence_data: Dict[str, Any], 
							  tenant_id: str, 
							  user_id: str) -> GLSGeofence:
		"""Create a new geofence with validation."""
		try:
			# Create geofence model
			geofence = GLSGeofence(
				tenant_id=tenant_id,
				created_by=user_id,
				updated_by=user_id,
				**geofence_data
			)
			
			# Validate boundary
			await self._validate_geofence_boundary(geofence.boundary)
			
			# Store geofence
			self._geofences[geofence.id] = geofence
			
			logger.info(f"Created geofence: {geofence.name} ({geofence.id})")
			return geofence
			
		except Exception as e:
			logger.error(f"Failed to create geofence: {str(e)}")
			raise GLSGeofenceError(f"Failed to create geofence: {str(e)}")
	
	async def _validate_geofence_boundary(self, boundary: GLSBoundary) -> None:
		"""Validate geofence boundary definition."""
		if boundary.boundary_type == GLSGeofenceType.CIRCLE:
			if not boundary.center_point or not boundary.radius_meters:
				raise GLSGeofenceError("Circle geofence requires center point and radius")
			if boundary.radius_meters <= 0 or boundary.radius_meters > 1000000:  # Max 1000km radius
				raise GLSGeofenceError("Circle radius must be between 0 and 1,000,000 meters")
		
		elif boundary.boundary_type == GLSGeofenceType.POLYGON:
			if len(boundary.coordinates) < 3:
				raise GLSGeofenceError("Polygon geofence requires at least 3 coordinates")
			if len(boundary.coordinates) > 1000:  # Reasonable limit
				raise GLSGeofenceError("Polygon geofence cannot have more than 1000 coordinates")
		
		# Validate coordinate ranges
		for coord in boundary.coordinates:
			if not (-90 <= coord.latitude <= 90):
				raise GLSGeofenceError(f"Invalid latitude: {coord.latitude}")
			if not (-180 <= coord.longitude <= 180):
				raise GLSGeofenceError(f"Invalid longitude: {coord.longitude}")
	
	async def process_location_update(self, 
									  entity_id: str, 
									  entity_type: GLSEntityType,
									  coordinate: GLSCoordinate,
									  tenant_id: str) -> List[GLSLocationEvent]:
		"""Process a location update and generate events."""
		events = []
		
		try:
			# Get or create entity location record
			if entity_id not in self._entity_locations:
				self._entity_locations[entity_id] = GLSEntityLocation(
					entity_id=entity_id,
					entity_type=entity_type,
					current_coordinate=coordinate,
					tenant_id=tenant_id,
					created_by="system",
					updated_by="system"
				)
			
			entity_location = self._entity_locations[entity_id]
			previous_coordinate = entity_location.current_coordinate
			previous_geofences = set(entity_location.current_geofences)
			
			# Update entity location
			entity_location.current_coordinate = coordinate
			entity_location.last_movement = datetime.utcnow()
			entity_location.updated_at = datetime.utcnow()
			
			# Check all geofences for this tenant
			current_geofences = set()
			tenant_geofences = [gf for gf in self._geofences.values() if gf.tenant_id == tenant_id and gf.is_active]
			
			for geofence in tenant_geofences:
				is_inside = await self._check_point_in_geofence(coordinate, geofence)
				
				if is_inside:
					current_geofences.add(geofence.id)
					
					# Check if this is an entry event
					if geofence.id not in previous_geofences:
						event = GLSLocationEvent(
							entity_id=entity_id,
							entity_type=entity_type,
							event_type=GLSEventType.ENTER,
							event_timestamp=datetime.utcnow(),
							coordinate=coordinate,
							geofence_id=geofence.id,
							tenant_id=tenant_id,
							created_by="system",
							updated_by="system"
						)
						events.append(event)
						
						# Update geofence counters
						geofence.entry_count += 1
						geofence.total_entries += 1
						geofence.last_triggered = datetime.utcnow()
			
			# Check for exit events
			for geofence_id in previous_geofences:
				if geofence_id not in current_geofences:
					geofence = self._geofences.get(geofence_id)
					if geofence:
						event = GLSLocationEvent(
							entity_id=entity_id,
							entity_type=entity_type,
							event_type=GLSEventType.EXIT,
							event_timestamp=datetime.utcnow(),
							coordinate=coordinate,
							geofence_id=geofence_id,
							previous_geofence_id=geofence_id,
							tenant_id=tenant_id,
							created_by="system",
							updated_by="system"
						)
						events.append(event)
						
						# Update geofence counters
						geofence.entry_count = max(0, geofence.entry_count - 1)
			
			# Update entity's current geofences
			entity_location.current_geofences = list(current_geofences)
			
			# Calculate movement metrics
			if previous_coordinate:
				distance = calculate_distance(previous_coordinate, coordinate)
				bearing = calculate_bearing(previous_coordinate, coordinate)
				
				for event in events:
					event.distance_traveled = distance * 1000  # Convert to meters
					event.heading_degrees = bearing
			
			# Store events
			self._location_events.extend(events)
			
			logger.debug(f"Processed location update for {entity_id}: {len(events)} events generated")
			return events
			
		except Exception as e:
			logger.error(f"Failed to process location update: {str(e)}")
			raise GLSGeofenceError(f"Failed to process location update: {str(e)}")
	
	async def _check_point_in_geofence(self, point: GLSCoordinate, geofence: GLSGeofence) -> bool:
		"""Check if a point is inside a geofence."""
		boundary = geofence.boundary
		
		if boundary.boundary_type == GLSGeofenceType.CIRCLE:
			return point_in_circle(point, boundary.center_point, boundary.radius_meters / 1000.0)
		
		elif boundary.boundary_type == GLSGeofenceType.POLYGON:
			return point_in_polygon(point, boundary.coordinates)
		
		elif boundary.boundary_type == GLSGeofenceType.RECTANGLE:
			if len(boundary.coordinates) != 2:
				return False
			
			sw_corner = boundary.coordinates[0]  # Southwest
			ne_corner = boundary.coordinates[1]  # Northeast
			
			return (sw_corner.latitude <= point.latitude <= ne_corner.latitude and
					sw_corner.longitude <= point.longitude <= ne_corner.longitude)
		
		return False
	
	async def get_entities_in_geofence(self, geofence_id: str) -> List[GLSEntityLocation]:
		"""Get all entities currently inside a geofence."""
		entities = []
		
		for entity in self._entity_locations.values():
			if geofence_id in entity.current_geofences:
				entities.append(entity)
		
		return entities

class GLSTerritoryService:
	"""Territory management and optimization service."""
	
	def __init__(self):
		self._territories: Dict[str, GLSTerritory] = {}
		logger.info("Initialized territory service")
	
	async def create_territory(self, 
							   territory_data: Dict[str, Any], 
							   tenant_id: str, 
							   user_id: str) -> GLSTerritory:
		"""Create a new territory."""
		try:
			territory = GLSTerritory(
				tenant_id=tenant_id,
				created_by=user_id,
				updated_by=user_id,
				**territory_data
			)
			
			# Validate territory boundary
			await self._validate_territory_boundary(territory.boundary)
			
			# Store territory
			self._territories[territory.id] = territory
			
			logger.info(f"Created territory: {territory.name} ({territory.id})")
			return territory
			
		except Exception as e:
			logger.error(f"Failed to create territory: {str(e)}")
			raise GLSServiceError(f"Failed to create territory: {str(e)}")
	
	async def _validate_territory_boundary(self, boundary: GLSBoundary) -> None:
		"""Validate territory boundary definition."""
		if len(boundary.coordinates) < 3:
			raise GLSServiceError("Territory boundary requires at least 3 coordinates")
		
		# Check for reasonable size (not too large or small)
		total_area = await self._calculate_boundary_area(boundary)
		if total_area > 1000000:  # 1M square km
			raise GLSServiceError("Territory is too large (max 1M square km)")
		if total_area < 0.01:  # 10,000 square meters
			raise GLSServiceError("Territory is too small (min 10,000 square meters)")
	
	async def _calculate_boundary_area(self, boundary: GLSBoundary) -> float:
		"""Calculate the approximate area of a boundary in square kilometers."""
		# Simple approximation using shoelace formula
		coords = boundary.coordinates
		if len(coords) < 3:
			return 0.0
		
		area = 0.0
		n = len(coords)
		
		for i in range(n):
			j = (i + 1) % n
			area += coords[i].longitude * coords[j].latitude
			area -= coords[j].longitude * coords[i].latitude
		
		return abs(area) / 2.0 * 111.32 * 111.32  # Rough conversion to sq km
	
	async def assign_entities_to_territories(self, 
											 entities: List[GLSEntityLocation]) -> Dict[str, List[str]]:
		"""Assign entities to territories based on their current locations."""
		assignments = {}
		
		for entity in entities:
			assigned_territories = []
			
			for territory in self._territories.values():
				if territory.tenant_id != entity.tenant_id:
					continue
				
				# Check if entity is in territory
				is_inside = await self._check_point_in_territory(entity.current_coordinate, territory)
				if is_inside:
					assigned_territories.append(territory.id)
			
			if assigned_territories:
				assignments[entity.entity_id] = assigned_territories
		
		return assignments
	
	async def _check_point_in_territory(self, point: GLSCoordinate, territory: GLSTerritory) -> bool:
		"""Check if a point is inside a territory."""
		return point_in_polygon(point, territory.boundary.coordinates)

class GLSRouteOptimizationService:
	"""Advanced route optimization and logistics planning service."""
	
	def __init__(self):
		self._routes: Dict[str, GLSRoute] = {}
		logger.info("Initialized route optimization service")
	
	async def optimize_route(self, 
							 waypoints: List[GLSWaypoint],
							 optimization_objective: GLSRouteOptimization,
							 constraints: Optional[Dict[str, Any]] = None) -> GLSRoute:
		"""Optimize a route through multiple waypoints."""
		try:
			if len(waypoints) < 2:
				raise GLSRouteOptimizationError("Route requires at least 2 waypoints")
			
			constraints = constraints or {}
			
			# Simple optimization algorithm (in production, use advanced algorithms)
			optimized_waypoints = await self._optimize_waypoint_order(waypoints, optimization_objective)
			
			# Calculate route metrics
			total_distance = await self._calculate_total_distance(optimized_waypoints)
			estimated_duration = await self._calculate_estimated_duration(optimized_waypoints, constraints)
			
			# Create route
			route = GLSRoute(
				route_name=f"Optimized Route {datetime.utcnow().strftime('%Y%m%d_%H%M%S')}",
				waypoints=optimized_waypoints,
				optimization_objective=optimization_objective,
				total_distance_km=total_distance,
				estimated_duration_minutes=estimated_duration,
				tenant_id="default",  # Should be passed from calling context
				created_by="system",
				updated_by="system"
			)
			
			# Generate turn-by-turn directions
			route.turn_by_turn_directions = await self._generate_directions(optimized_waypoints)
			
			# Store route
			self._routes[route.id] = route
			
			logger.info(f"Optimized route with {len(waypoints)} waypoints: {total_distance:.2f}km, {estimated_duration}min")
			return route
			
		except Exception as e:
			logger.error(f"Route optimization failed: {str(e)}")
			raise GLSRouteOptimizationError(f"Route optimization failed: {str(e)}")
	
	async def _optimize_waypoint_order(self, 
									   waypoints: List[GLSWaypoint],
									   objective: GLSRouteOptimization) -> List[GLSWaypoint]:
		"""Optimize the order of waypoints based on objective."""
		# Simple nearest neighbor algorithm (in production, use TSP solvers)
		if len(waypoints) <= 2:
			return waypoints
		
		optimized = [waypoints[0]]  # Start with first waypoint
		remaining = waypoints[1:-1]  # Exclude start and end
		current = waypoints[0]
		
		while remaining:
			if objective == GLSRouteOptimization.SHORTEST_DISTANCE:
				# Find nearest remaining waypoint
				nearest = min(remaining, key=lambda wp: calculate_distance(current.coordinate, wp.coordinate))
			else:
				# For other objectives, use distance as approximation
				nearest = min(remaining, key=lambda wp: calculate_distance(current.coordinate, wp.coordinate))
			
			optimized.append(nearest)
			remaining.remove(nearest)
			current = nearest
		
		# Add final waypoint if different from start
		if len(waypoints) > 1 and waypoints[-1] != waypoints[0]:
			optimized.append(waypoints[-1])
		
		return optimized
	
	async def _calculate_total_distance(self, waypoints: List[GLSWaypoint]) -> float:
		"""Calculate total route distance in kilometers."""
		total_distance = 0.0
		
		for i in range(len(waypoints) - 1):
			distance = calculate_distance(waypoints[i].coordinate, waypoints[i + 1].coordinate)
			total_distance += distance
		
		return total_distance
	
	async def _calculate_estimated_duration(self, 
											waypoints: List[GLSWaypoint],
											constraints: Dict[str, Any]) -> int:
		"""Calculate estimated route duration in minutes."""
		total_distance_km = await self._calculate_total_distance(waypoints)
		
		# Default speed assumptions (in production, use real traffic data)
		average_speed_kmh = constraints.get('average_speed_kmh', 50)  # 50 km/h default
		
		# Travel time
		travel_time_hours = total_distance_km / average_speed_kmh
		travel_time_minutes = travel_time_hours * 60
		
		# Service time at waypoints
		service_time_minutes = sum(wp.service_duration_minutes for wp in waypoints)
		
		return int(travel_time_minutes + service_time_minutes)
	
	async def _generate_directions(self, waypoints: List[GLSWaypoint]) -> List[Dict[str, Any]]:
		"""Generate turn-by-turn directions."""
		directions = []
		
		for i in range(len(waypoints) - 1):
			current = waypoints[i]
			next_wp = waypoints[i + 1]
			
			distance = calculate_distance(current.coordinate, next_wp.coordinate)
			bearing = calculate_bearing(current.coordinate, next_wp.coordinate)
			
			# Simple direction generation (in production, use routing APIs)
			direction = {
				"step": i + 1,
				"instruction": f"Head towards {next_wp.name or f'waypoint {i + 2}'}",
				"distance_km": round(distance, 2),
				"bearing_degrees": round(bearing, 1),
				"estimated_duration_minutes": max(1, int(distance / 50 * 60))  # Assume 50 km/h
			}
			
			directions.append(direction)
		
		return directions

class GLSComplianceService:
	"""Geographic compliance and regulatory management service."""
	
	def __init__(self):
		self._compliance_reports: Dict[str, GLSComplianceReport] = {}
		logger.info("Initialized compliance service")
	
	async def check_location_compliance(self, 
										coordinate: GLSCoordinate,
										compliance_types: List[GLSComplianceType],
										tenant_id: str) -> Dict[str, Any]:
		"""Check compliance requirements for a specific location."""
		compliance_results = {}
		
		for compliance_type in compliance_types:
			result = await self._check_specific_compliance(coordinate, compliance_type, tenant_id)
			compliance_results[compliance_type.value] = result
		
		return compliance_results
	
	async def _check_specific_compliance(self, 
										 coordinate: GLSCoordinate,
										 compliance_type: GLSComplianceType,
										 tenant_id: str) -> Dict[str, Any]:
		"""Check a specific type of compliance."""
		# Mock compliance checking (in production, integrate with regulatory APIs)
		result = {
			"compliant": True,
			"jurisdiction": "United States",
			"applicable_regulations": [],
			"restrictions": [],
			"warnings": []
		}
		
		if compliance_type == GLSComplianceType.GDPR:
			# Check if location is in EU
			if self._is_in_european_union(coordinate):
				result["applicable_regulations"].append("GDPR Article 32 - Data Protection")
				result["restrictions"].append("Data processing consent required")
		
		elif compliance_type == GLSComplianceType.DATA_RESIDENCY:
			# Check data residency requirements
			country = await self._get_country_from_coordinate(coordinate)
			result["jurisdiction"] = country
			result["applicable_regulations"].append(f"{country} Data Residency Laws")
		
		elif compliance_type == GLSComplianceType.TAX_JURISDICTION:
			# Determine tax jurisdiction
			tax_info = await self._get_tax_jurisdiction(coordinate)
			result.update(tax_info)
		
		return result
	
	def _is_in_european_union(self, coordinate: GLSCoordinate) -> bool:
		"""Check if coordinate is within European Union boundaries."""
		# Simplified check (in production, use precise boundary data)
		return (35.0 <= coordinate.latitude <= 71.0 and 
				-25.0 <= coordinate.longitude <= 45.0)
	
	async def _get_country_from_coordinate(self, coordinate: GLSCoordinate) -> str:
		"""Get country name from coordinate (reverse geocoding)."""
		# Mock implementation (in production, use reverse geocoding API)
		return "United States"
	
	async def _get_tax_jurisdiction(self, coordinate: GLSCoordinate) -> Dict[str, Any]:
		"""Get tax jurisdiction information for a coordinate."""
		# Mock implementation (in production, integrate with tax APIs)
		return {
			"tax_country": "US",
			"tax_state": "NY",
			"tax_rates": {
				"sales_tax": 8.25,
				"property_tax": 1.25
			},
			"tax_zones": ["NYC_ZONE_1"]
		}

# =============================================================================
# Advanced Spatiotemporal Services
# =============================================================================

class GLSFuzzyMatchingService:
	"""Advanced fuzzy location matching service."""
	
	def __init__(self, geonames_data: Optional[List[GLSAdministrativeRegion]] = None):
		self.geonames_data = geonames_data or []
		self._search_index = self._build_search_index()
		logger.info("Initialized fuzzy matching service")
	
	def _build_search_index(self) -> Dict[str, List[GLSAdministrativeRegion]]:
		"""Build search index for fast fuzzy matching."""
		index = defaultdict(list)
		
		for region in self.geonames_data:
			# Index by name and variants
			names = [region.name] + region.name_variants
			for name in names:
				# Simple indexing by first character
				if name:
					index[name[0].lower()].append(region)
		
		return dict(index)
	
	async def fuzzy_search(self, request: GLSFuzzySearchRequest) -> List[Tuple[GLSAdministrativeRegion, float]]:
		"""Perform fuzzy search with confidence scoring."""
		results = []
		query = request.query.lower().strip()
		
		# Search through indexed regions
		for first_char, regions in self._search_index.items():
			if query.startswith(first_char):
				for region in regions:
					# Check all name variants
					names_to_check = [region.name] + region.name_variants
					
					for name in names_to_check:
						for match_type in request.match_types:
							confidence = fuzzy_string_match(query, name.lower(), match_type)
							
							if confidence >= request.confidence_threshold:
								# Apply geographic constraints
								if self._passes_constraints(region, request):
									results.append((region, confidence))
								break
		
		# Sort by confidence and limit results
		results.sort(key=lambda x: x[1], reverse=True)
		return results[:request.max_results]
	
	def _passes_constraints(self, region: GLSAdministrativeRegion, request: GLSFuzzySearchRequest) -> bool:
		"""Check if region passes geographic constraints."""
		# Bounding box constraint
		if request.bounding_box:
			sw_corner, ne_corner = request.bounding_box
			centroid = region.centroid
			
			if not (sw_corner.latitude <= centroid.latitude <= ne_corner.latitude and
					sw_corner.longitude <= centroid.longitude <= ne_corner.longitude):
				return False
		
		# Admin level constraint
		if request.admin_levels and region.admin_level not in request.admin_levels:
			return False
		
		# Population constraint
		if request.population_min and (not region.population or region.population < request.population_min):
			return False
		
		# Area constraint
		if request.area_min_km2 and (not region.area_km2 or region.area_km2 < request.area_min_km2):
			return False
		
		return True

class GLSTrajectoryAnalysisService:
	"""Advanced trajectory analysis and pattern detection service."""
	
	def __init__(self):
		self._trajectories: Dict[str, GLSTrajectory] = {}
		logger.info("Initialized trajectory analysis service")
	
	async def analyze_trajectory(self, request: GLSTrajectoryAnalysisRequest) -> GLSTrajectory:
		"""Perform comprehensive trajectory analysis."""
		try:
			# Mock trajectory data retrieval (in production, fetch from database)
			coordinates = await self._get_entity_coordinates(
				request.entity_id, 
				request.time_window_start, 
				request.time_window_end
			)
			
			if len(coordinates) < 2:
				raise GLSServiceError("Insufficient trajectory data")
			
			# Calculate trajectory metrics
			total_distance = 0
			speeds = []
			
			for i in range(len(coordinates) - 1):
				distance = calculate_distance(coordinates[i], coordinates[i + 1])
				total_distance += distance
				
				# Calculate speed (assuming 1 minute intervals)
				time_diff = 60  # seconds
				speed_kmh = (distance / time_diff) * 3600
				speeds.append(speed_kmh)
			
			duration_seconds = len(coordinates) * 60  # Mock duration
			avg_speed = statistics.mean(speeds) if speeds else 0
			max_speed = max(speeds) if speeds else 0
			
			# Pattern detection
			detected_patterns = []
			pattern_confidence = {}
			
			if request.detect_patterns:
				patterns = detect_trajectory_patterns(coordinates)
				for pattern, confidence in patterns.items():
					if confidence >= request.pattern_confidence_threshold:
						detected_patterns.append(pattern)
						pattern_confidence[pattern] = confidence
			
			# Generate H3 cells visited
			visited_h3_cells = set()
			for coord in coordinates:
				coord.h3_indices = generate_h3_indices(coord)
				visited_h3_cells.add(coord.primary_h3_index)
			
			# Dwell point detection
			dwell_points = []
			if request.cluster_dwell_points:
				dwell_points = await self._detect_dwell_points(
					coordinates, 
					request.min_dwell_time_seconds,
					request.speed_threshold_kmh
				)
			
			# Create trajectory object
			trajectory = GLSTrajectory(
				entity_id=request.entity_id,
				entity_type=GLSEntityType.PERSON,  # Mock
				coordinates=coordinates,
				start_time=request.time_window_start,
				end_time=request.time_window_end,
				total_distance_km=total_distance,
				duration_seconds=duration_seconds,
				average_speed_kmh=avg_speed,
				max_speed_kmh=max_speed,
				detected_patterns=detected_patterns,
				pattern_confidence=pattern_confidence,
				visited_h3_cells=visited_h3_cells,
				dwell_points=dwell_points
			)
			
			# Anomaly detection
			if request.detect_anomalies:
				trajectory.anomaly_score = await self._detect_trajectory_anomalies(trajectory)
			
			# Store trajectory
			self._trajectories[trajectory.trajectory_id] = trajectory
			
			return trajectory
			
		except Exception as e:
			logger.error(f"Trajectory analysis failed: {str(e)}")
			raise GLSServiceError(f"Trajectory analysis failed: {str(e)}")
	
	async def _get_entity_coordinates(self, entity_id: str, start_time: datetime, end_time: datetime) -> List[GLSCoordinate]:
		"""Get entity coordinates for time window."""
		# Mock trajectory generation
		coordinates = []
		current_time = start_time
		lat, lng = 40.7128, -74.0060  # Start at NYC
		
		while current_time < end_time:
			# Random walk simulation
			lat += (np.random.random() - 0.5) * 0.01
			lng += (np.random.random() - 0.5) * 0.01
			
			coord = GLSCoordinate(
				latitude=lat,
				longitude=lng,
				timestamp=current_time
			)
			coordinates.append(coord)
			current_time += timedelta(minutes=1)
		
		return coordinates
	
	async def _detect_dwell_points(self, coordinates: List[GLSCoordinate], 
								   min_dwell_time: int, speed_threshold: float) -> List[Dict[str, Any]]:
		"""Detect dwell points in trajectory."""
		dwell_points = []
		
		# Simple dwell point detection
		current_cluster = []
		
		for i, coord in enumerate(coordinates):
			if not current_cluster:
				current_cluster.append((coord, i))
				continue
			
			# Check if within dwell distance
			distance = calculate_distance(current_cluster[0][0], coord)
			if distance < 0.1:  # 100 meters
				current_cluster.append((coord, i))
			else:
				# End of cluster, check if it's a dwell point
				if len(current_cluster) * 60 >= min_dwell_time:  # Convert to seconds
					center_coord = current_cluster[len(current_cluster) // 2][0]
					dwell_points.append({
						"location": center_coord.model_dump(),
						"duration_seconds": len(current_cluster) * 60,
						"start_index": current_cluster[0][1],
						"end_index": current_cluster[-1][1]
					})
				
				current_cluster = [(coord, i)]
		
		return dwell_points
	
	async def _detect_trajectory_anomalies(self, trajectory: GLSTrajectory) -> float:
		"""Detect anomalies in trajectory."""
		# Simple anomaly detection based on speed variance
		if len(trajectory.coordinates) < 3:
			return 0.0
		
		speeds = []
		for i in range(len(trajectory.coordinates) - 1):
			distance = calculate_distance(trajectory.coordinates[i], trajectory.coordinates[i + 1])
			speed = distance / (60 / 3600)  # km/h assuming 1-minute intervals
			speeds.append(speed)
		
		if not speeds:
			return 0.0
		
		mean_speed = statistics.mean(speeds)
		speed_variance = statistics.variance(speeds) if len(speeds) > 1 else 0
		
		# High variance indicates potential anomalies
		anomaly_score = min(1.0, speed_variance / (mean_speed + 0.1))
		return anomaly_score

class GLSHotspotDetectionService:
	"""Spatiotemporal hotspot detection and analysis service."""
	
	def __init__(self):
		self._hotspots: Dict[str, GLSHotspot] = {}
		logger.info("Initialized hotspot detection service")
	
	async def detect_hotspots(self, request: GLSHotspotAnalysisRequest) -> List[GLSHotspot]:
		"""Detect spatiotemporal hotspots using statistical clustering."""
		try:
			# Get events/entities in analysis area and time window
			events = await self._get_events_in_area(
				request.analysis_area,
				request.time_window_start,
				request.time_window_end,
				request.entity_types,
				request.event_types
			)
			
			if len(events) < request.min_events_per_hotspot:
				return []
			
			# Convert to H3 grid for analysis
			h3_events = self._aggregate_to_h3_grid(events, request.h3_resolution)
			
			# Perform clustering
			hotspots = []
			
			if request.clustering_algorithm == GLSClusteringAlgorithm.DBSCAN:
				hotspots = await self._dbscan_hotspot_detection(
					h3_events, request.significance_level, request.min_events_per_hotspot
				)
			elif request.clustering_algorithm == GLSClusteringAlgorithm.GRID_BASED:
				hotspots = await self._grid_based_hotspot_detection(
					h3_events, request.significance_level, request.min_events_per_hotspot
				)
			
			# Filter and validate hotspots
			validated_hotspots = []
			for hotspot in hotspots:
				if hotspot.event_count >= request.min_events_per_hotspot:
					validated_hotspots.append(hotspot)
					self._hotspots[hotspot.hotspot_id] = hotspot
			
			logger.info(f"Detected {len(validated_hotspots)} hotspots")
			return validated_hotspots
			
		except Exception as e:
			logger.error(f"Hotspot detection failed: {str(e)}")
			raise GLSServiceError(f"Hotspot detection failed: {str(e)}")
	
	async def _get_events_in_area(self, area: GLSBoundary, start_time: datetime, 
								  end_time: datetime, entity_types: List[GLSEntityType],
								  event_types: List[GLSEventType]) -> List[Dict[str, Any]]:
		"""Get events in specified area and time window."""
		# Mock event generation
		events = []
		
		# Generate random events within boundary
		for _ in range(100):  # Mock 100 events
			# Simple random point generation within boundary
			if area.boundary_type == GLSGeofenceType.CIRCLE and area.center_point:
				angle = np.random.random() * 2 * math.pi
				distance = np.random.random() * (area.radius_meters / 1000)  # Convert to km
				
				lat_offset = distance * math.cos(angle) / 111.32
				lng_offset = distance * math.sin(angle) / (111.32 * math.cos(math.radians(area.center_point.latitude)))
				
				coord = GLSCoordinate(
					latitude=area.center_point.latitude + lat_offset,
					longitude=area.center_point.longitude + lng_offset,
					timestamp=start_time + timedelta(seconds=np.random.randint(0, int((end_time - start_time).total_seconds())))
				)
				
				events.append({
					"coordinate": coord,
					"entity_type": np.random.choice(list(GLSEntityType)) if not entity_types else np.random.choice(entity_types),
					"event_type": np.random.choice(list(GLSEventType)) if not event_types else np.random.choice(event_types),
					"timestamp": coord.timestamp
				})
		
		return events
	
	def _aggregate_to_h3_grid(self, events: List[Dict[str, Any]], resolution: GLSH3Resolution) -> Dict[str, List[Dict[str, Any]]]:
		"""Aggregate events to H3 grid cells."""
		h3_grid = defaultdict(list)
		
		for event in events:
			coord = event["coordinate"]
			coord.h3_indices = generate_h3_indices(coord)
			h3_index = coord.h3_indices.get(resolution.value)
			
			if h3_index:
				h3_grid[h3_index].append(event)
		
		return dict(h3_grid)
	
	async def _dbscan_hotspot_detection(self, h3_events: Dict[str, List[Dict[str, Any]]], 
										significance_level: float, min_events: int) -> List[GLSHotspot]:
		"""Detect hotspots using DBSCAN clustering."""
		if not h3_events:
			return []
		
		# Prepare data for clustering
		coordinates = []
		event_counts = []
		h3_indices = []
		
		for h3_index, events in h3_events.items():
			if len(events) >= min_events:
				# Use first event coordinate as representative
				coord = events[0]["coordinate"]
				coordinates.append([coord.latitude, coord.longitude])
				event_counts.append(len(events))
				h3_indices.append(h3_index)
		
		if len(coordinates) < 2:
			return []
		
		# Perform DBSCAN clustering
		coordinates_array = np.array(coordinates)
		scaler = StandardScaler()
		scaled_coords = scaler.fit_transform(coordinates_array)
		
		dbscan = DBSCAN(eps=0.5, min_samples=2)
		cluster_labels = dbscan.fit_predict(scaled_coords)
		
		# Create hotspots from clusters
		hotspots = []
		unique_labels = set(cluster_labels)
		
		for label in unique_labels:
			if label == -1:  # Noise points
				continue
			
			cluster_indices = np.where(cluster_labels == label)[0]
			cluster_coords = [coordinates[i] for i in cluster_indices]
			cluster_events = sum(event_counts[i] for i in cluster_indices)
			cluster_h3_cells = {h3_indices[i] for i in cluster_indices}
			
			# Calculate cluster center
			center_lat = statistics.mean(coord[0] for coord in cluster_coords)
			center_lng = statistics.mean(coord[1] for coord in cluster_coords)
			center_coord = GLSCoordinate(latitude=center_lat, longitude=center_lng)
			
			# Calculate intensity and significance
			intensity = cluster_events / len(cluster_coords)
			z_score = (intensity - statistics.mean(event_counts)) / (statistics.stdev(event_counts) + 0.001)
			p_value = max(0.001, 1 - stats.norm.cdf(abs(z_score)))
			
			if p_value <= significance_level:
				hotspot = GLSHotspot(
					hotspot_type="activity_cluster",
					location=center_coord,
					boundary=GLSBoundary(
						boundary_type=GLSGeofenceType.CIRCLE,
						coordinates=[center_coord],
						center_point=center_coord,
						radius_meters=1000.0  # Default 1km radius
					),
					h3_cells=cluster_h3_cells,
					time_window_start=datetime.utcnow() - timedelta(hours=24),  # Mock
					time_window_end=datetime.utcnow(),
					intensity=intensity,
					significance_level=significance_level,
					z_score=z_score,
					p_value=p_value,
					event_count=cluster_events,
					event_density=cluster_events / 1.0  # Events per kmÂ²
				)
				hotspots.append(hotspot)
		
		return hotspots
	
	async def _grid_based_hotspot_detection(self, h3_events: Dict[str, List[Dict[str, Any]]], 
											significance_level: float, min_events: int) -> List[GLSHotspot]:
		"""Detect hotspots using grid-based analysis."""
		hotspots = []
		
		# Calculate global statistics
		all_counts = [len(events) for events in h3_events.values()]
		if not all_counts:
			return []
		
		mean_count = statistics.mean(all_counts)
		std_count = statistics.stdev(all_counts) if len(all_counts) > 1 else 1.0
		
		# Identify significant cells
		for h3_index, events in h3_events.items():
			event_count = len(events)
			
			if event_count >= min_events:
				z_score = (event_count - mean_count) / std_count
				p_value = 1 - stats.norm.cdf(abs(z_score))
				
				if p_value <= significance_level:
					# Create hotspot for this cell
					coord = events[0]["coordinate"]  # Representative coordinate
					
					hotspot = GLSHotspot(
						hotspot_type="grid_hotspot",
						location=coord,
						boundary=GLSBoundary(
							boundary_type=GLSGeofenceType.CIRCLE,
							coordinates=[coord],
							center_point=coord,
							radius_meters=500.0  # Smaller radius for grid-based
						),
						h3_cells={h3_index},
						time_window_start=min(event["timestamp"] for event in events),
						time_window_end=max(event["timestamp"] for event in events),
						intensity=event_count,
						significance_level=significance_level,
						z_score=z_score,
						p_value=p_value,
						event_count=event_count,
						event_density=event_count / 0.25  # Events per 0.25 kmÂ²
					)
					hotspots.append(hotspot)
		
		return hotspots

class GLSPredictiveModelingService:
	"""Predictive modeling service for entity positions and events."""
	
	def __init__(self):
		self._models: Dict[str, GLSPredictionModel] = {}
		logger.info("Initialized predictive modeling service")
	
	async def create_prediction_model(self, request: GLSPredictiveAnalysisRequest) -> GLSPredictionModel:
		"""Create and train a predictive model."""
		try:
			# Get training data
			training_end = datetime.utcnow()
			training_start = training_end - timedelta(days=request.training_days)
			
			historical_data = await self._get_historical_data(
				request.entity_id,
				training_start,
				training_end,
				request.include_weather,
				request.include_traffic,
				request.include_events,
				request.include_patterns
			)
			
			# Train model (mock implementation)
			model_accuracy = 0.85  # Mock accuracy
			feature_importance = {
				"historical_location": 0.4,
				"time_of_day": 0.25,
				"day_of_week": 0.15,
				"weather": 0.1 if request.include_weather else 0,
				"traffic": 0.05 if request.include_traffic else 0,
				"events": 0.05 if request.include_events else 0
			}
			
			# Generate predictions
			predictions = await self._generate_predictions(
				request.entity_id,
				request.prediction_horizon_hours,
				historical_data,
				request.confidence_level
			)
			
			# Create model
			model = GLSPredictionModel(
				model_type=request.model_type,
				entity_id=request.entity_id,
				training_period_start=training_start,
				training_period_end=training_end,
				model_accuracy=model_accuracy,
				confidence_interval=request.confidence_level,
				predicted_locations=predictions,
				feature_importance=feature_importance,
				model_parameters={
					"horizon_hours": request.prediction_horizon_hours,
					"training_days": request.training_days,
					"model_type": request.model_type
				},
				validation_metrics={
					"mae": 0.5,  # Mean Absolute Error in km
					"rmse": 0.8,  # Root Mean Square Error in km
					"r2_score": 0.72  # R-squared score
				}
			)
			
			# Store model
			self._models[model.model_id] = model
			
			logger.info(f"Created prediction model {model.model_id} with accuracy {model_accuracy:.2f}")
			return model
			
		except Exception as e:
			logger.error(f"Prediction model creation failed: {str(e)}")
			raise GLSServiceError(f"Prediction model creation failed: {str(e)}")
	
	async def _get_historical_data(self, entity_id: str, start_time: datetime, end_time: datetime,
								   include_weather: bool, include_traffic: bool, 
								   include_events: bool, include_patterns: bool) -> Dict[str, Any]:
		"""Get historical data for model training."""
		# Mock historical data
		return {
			"locations": [],  # Historical coordinates
			"weather": [] if include_weather else None,
			"traffic": [] if include_traffic else None,
			"events": [] if include_events else None,
			"patterns": {} if include_patterns else None
		}
	
	async def _generate_predictions(self, entity_id: str, horizon_hours: int,
									historical_data: Dict[str, Any], confidence: float) -> List[Tuple[GLSCoordinate, datetime, float]]:
		"""Generate location predictions."""
		predictions = []
		current_time = datetime.utcnow()
		
		# Mock prediction generation
		base_lat, base_lng = 40.7128, -74.0060  # NYC
		
		for hour in range(horizon_hours):
			prediction_time = current_time + timedelta(hours=hour)
			
			# Add some randomness and trend
			lat_offset = math.sin(hour * 0.1) * 0.01 + np.random.normal(0, 0.005)
			lng_offset = math.cos(hour * 0.1) * 0.01 + np.random.normal(0, 0.005)
			
			predicted_coord = GLSCoordinate(
				latitude=base_lat + lat_offset,
				longitude=base_lng + lng_offset,
				timestamp=prediction_time
			)
			
			# Confidence decreases with time horizon
			prediction_confidence = confidence * (1 - hour / (horizon_hours * 2))
			prediction_confidence = max(0.1, prediction_confidence)
			
			predictions.append((predicted_coord, prediction_time, prediction_confidence))
		
		return predictions

class GLSAnomalyDetectionService:
	"""Anomaly detection service for spatiotemporal patterns."""
	
	def __init__(self):
		self._anomalies: Dict[str, GLSAnomalyDetection] = {}
		self._baseline_patterns: Dict[str, Dict[str, Any]] = {}
		logger.info("Initialized anomaly detection service")
	
	async def detect_anomalies(self, entity_id: str, current_location: GLSCoordinate,
							   timestamp: Optional[datetime] = None) -> Optional[GLSAnomalyDetection]:
		"""Detect anomalies in entity behavior."""
		try:
			timestamp = timestamp or datetime.utcnow()
			
			# Get baseline patterns for entity
			baseline = await self._get_baseline_patterns(entity_id)
			
			if not baseline:
				# No baseline yet, start building it
				await self._update_baseline_patterns(entity_id, current_location, timestamp)
				return None
			
			# Check for spatial anomalies
			spatial_anomaly = await self._detect_spatial_anomaly(
				entity_id, current_location, timestamp, baseline
			)
			
			# Check for temporal anomalies
			temporal_anomaly = await self._detect_temporal_anomaly(
				entity_id, current_location, timestamp, baseline
			)
			
			# Combine anomaly scores
			if spatial_anomaly or temporal_anomaly:
				anomaly_score = max(
					spatial_anomaly.get("score", 0) if spatial_anomaly else 0,
					temporal_anomaly.get("score", 0) if temporal_anomaly else 0
				)
				
				anomaly_type = "spatial" if spatial_anomaly else "temporal"
				if spatial_anomaly and temporal_anomaly:
					anomaly_type = "spatiotemporal"
				
				anomaly = GLSAnomalyDetection(
					entity_id=entity_id,
					detection_time=timestamp,
					anomaly_type=anomaly_type,
					anomaly_score=anomaly_score,
					confidence=0.8,  # Mock confidence
					anomalous_location=current_location,
					expected_location=spatial_anomaly.get("expected_location") if spatial_anomaly else None,
					spatial_deviation_km=spatial_anomaly.get("deviation_km") if spatial_anomaly else None,
					anomalous_time=timestamp if temporal_anomaly else None,
					expected_time_window=temporal_anomaly.get("expected_window") if temporal_anomaly else None,
					context_factors=["unusual_location", "unexpected_time"],
					explanation=f"Entity {entity_id} detected at unusual location/time",
					recommended_actions=["Verify entity status", "Check for equipment issues", "Investigate cause"]
				)
				
				self._anomalies[anomaly.anomaly_id] = anomaly
				return anomaly
			
			# Update baseline patterns with normal behavior
			await self._update_baseline_patterns(entity_id, current_location, timestamp)
			return None
			
		except Exception as e:
			logger.error(f"Anomaly detection failed: {str(e)}")
			return None
	
	async def _get_baseline_patterns(self, entity_id: str) -> Optional[Dict[str, Any]]:
		"""Get baseline patterns for entity."""
		return self._baseline_patterns.get(entity_id)
	
	async def _update_baseline_patterns(self, entity_id: str, location: GLSCoordinate, timestamp: datetime):
		"""Update baseline patterns with new data."""
		if entity_id not in self._baseline_patterns:
			self._baseline_patterns[entity_id] = {
				"locations": [],
				"time_patterns": defaultdict(list),
				"weekly_patterns": defaultdict(list)
			}
		
		patterns = self._baseline_patterns[entity_id]
		patterns["locations"].append(location)
		
		# Keep only recent locations for baseline
		if len(patterns["locations"]) > 1000:
			patterns["locations"] = patterns["locations"][-1000:]
		
		# Update time patterns
		hour = timestamp.hour
		weekday = timestamp.weekday()
		
		patterns["time_patterns"][hour].append(location)
		patterns["weekly_patterns"][weekday].append(location)
	
	async def _detect_spatial_anomaly(self, entity_id: str, location: GLSCoordinate,
									  timestamp: datetime, baseline: Dict[str, Any]) -> Optional[Dict[str, Any]]:
		"""Detect spatial anomalies."""
		baseline_locations = baseline.get("locations", [])
		if len(baseline_locations) < 10:
			return None
		
		# Calculate distances to all baseline locations
		distances = [calculate_distance(location, bl) for bl in baseline_locations]
		min_distance = min(distances)
		mean_distance = statistics.mean(distances)
		
		# If significantly far from all baseline locations
		if min_distance > 5.0 and mean_distance > 10.0:  # 5km and 10km thresholds
			# Find nearest baseline location as expected
			nearest_idx = distances.index(min_distance)
			expected_location = baseline_locations[nearest_idx]
			
			return {
				"score": min(1.0, min_distance / 20.0),  # Normalize to 0-1
				"deviation_km": min_distance,
				"expected_location": expected_location
			}
		
		return None
	
	async def _detect_temporal_anomaly(self, entity_id: str, location: GLSCoordinate,
									   timestamp: datetime, baseline: Dict[str, Any]) -> Optional[Dict[str, Any]]:
		"""Detect temporal anomalies."""
		hour = timestamp.hour
		weekday = timestamp.weekday()
		
		# Check hourly patterns
		hourly_locations = baseline.get("time_patterns", {}).get(hour, [])
		if len(hourly_locations) < 5:
			return None
		
		# Calculate distance to typical locations for this hour
		distances = [calculate_distance(location, hl) for hl in hourly_locations]
		min_distance = min(distances)
		
		# If unusually far from typical locations for this time
		if min_distance > 3.0:  # 3km threshold
			expected_start = timestamp.replace(minute=0, second=0, microsecond=0)
			expected_end = expected_start + timedelta(hours=1)
			
			return {
				"score": min(1.0, min_distance / 10.0),
				"expected_window": (expected_start, expected_end)
			}
		
		return None

class GLSVisualizationService:
	"""Multi-renderer mapping and visualization service."""
	
	def __init__(self):
		self._map_cache: Dict[str, Dict[str, Any]] = {}
		logger.info("Initialized visualization service")
	
	async def create_map(self, config: GLSMapConfiguration, data: Dict[str, Any]) -> Dict[str, Any]:
		"""Create a map with specified configuration and data."""
		try:
			cache_key = f"{config.renderer.value}_{hash(str(config.model_dump()))}"
			
			if cache_key in self._map_cache:
				logger.debug(f"Using cached map: {cache_key}")
				return self._map_cache[cache_key]
			
			# Generate map based on renderer
			if config.renderer == GLSMapRenderer.FOLIUM:
				map_result = await self._create_folium_map(config, data)
			elif config.renderer == GLSMapRenderer.MATPLOTLIB:
				map_result = await self._create_matplotlib_map(config, data)
			elif config.renderer == GLSMapRenderer.PLOTLY:
				map_result = await self._create_plotly_map(config, data)
			else:
				raise GLSServiceError(f"Unsupported renderer: {config.renderer}")
			
			# Cache result
			self._map_cache[cache_key] = map_result
			
			return map_result
			
		except Exception as e:
			logger.error(f"Map creation failed: {str(e)}")
			raise GLSServiceError(f"Map creation failed: {str(e)}")
	
	async def _create_folium_map(self, config: GLSMapConfiguration, data: Dict[str, Any]) -> Dict[str, Any]:
		"""Create map using Folium renderer."""
		# Mock Folium map creation
		return {
			"renderer": "folium",
			"map_html": "<div>Folium map placeholder</div>",
			"interactive": True,
			"center": config.center.model_dump(),
			"zoom": config.zoom_level,
			"layers": len(config.base_layers) + len(config.overlay_layers),
			"features_count": len(data.get("features", []))
		}
	
	async def _create_matplotlib_map(self, config: GLSMapConfiguration, data: Dict[str, Any]) -> Dict[str, Any]:
		"""Create map using Matplotlib renderer."""
		# Mock Matplotlib map creation
		return {
			"renderer": "matplotlib",
			"image_data": "base64_encoded_image_placeholder",
			"interactive": False,
			"dimensions": {"width": config.width, "height": config.height},
			"dpi": config.dpi,
			"features_count": len(data.get("features", []))
		}
	
	async def _create_plotly_map(self, config: GLSMapConfiguration, data: Dict[str, Any]) -> Dict[str, Any]:
		"""Create map using Plotly renderer."""
		# Mock Plotly map creation
		return {
			"renderer": "plotly",
			"plotly_json": {"data": [], "layout": {}},
			"interactive": True,
			"center": config.center.model_dump(),
			"zoom": config.zoom_level,
			"features_count": len(data.get("features", []))
		}

class GLSRealTimeStreamingService:
	"""Real-time data streaming service with WebSocket support."""
	
	def __init__(self):
		self._streams: Dict[str, GLSRealTimeStream] = {}
		self._stream_buffers: Dict[str, List[Dict[str, Any]]] = {}
		logger.info("Initialized real-time streaming service")
	
	async def create_stream(self, entity_id: str, config: Dict[str, Any]) -> GLSRealTimeStream:
		"""Create a new real-time data stream."""
		try:
			stream = GLSRealTimeStream(
				entity_id=entity_id,
				update_interval_seconds=config.get("update_interval_seconds", 30),
				data_retention_hours=config.get("data_retention_hours", 24),
				max_buffer_size=config.get("max_buffer_size", 1000)
			)
			
			self._streams[stream.stream_id] = stream
			self._stream_buffers[stream.stream_id] = []
			
			logger.info(f"Created real-time stream {stream.stream_id} for entity {entity_id}")
			return stream
			
		except Exception as e:
			logger.error(f"Stream creation failed: {str(e)}")
			raise GLSServiceError(f"Stream creation failed: {str(e)}")
	
	async def add_stream_data(self, stream_id: str, data: Dict[str, Any]) -> bool:
		"""Add data to a stream buffer."""
		try:
			if stream_id not in self._streams:
				return False
			
			stream = self._streams[stream_id]
			buffer = self._stream_buffers[stream_id]
			
			# Add data with timestamp
			timestamped_data = {
				**data,
				"timestamp": datetime.utcnow(),
				"stream_id": stream_id
			}
			
			# Add to buffer
			buffer.append(timestamped_data)
			
			# Maintain buffer size
			if len(buffer) > stream.max_buffer_size:
				buffer.pop(0)
			
			# Update stream statistics
			stream.last_update = datetime.utcnow()
			stream.update_count += 1
			
			# Calculate data quality (mock)
			stream.data_quality_score = min(1.0, stream.update_count / 100)
			
			return True
			
		except Exception as e:
			logger.error(f"Failed to add stream data: {str(e)}")
			return False
	
	async def get_stream_data(self, stream_id: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
		"""Get recent data from stream buffer."""
		if stream_id not in self._stream_buffers:
			return []
		
		buffer = self._stream_buffers[stream_id]
		
		if limit:
			return buffer[-limit:]
		
		return buffer.copy()
	
	async def cleanup_expired_data(self):
		"""Clean up expired data from all streams."""
		current_time = datetime.utcnow()
		
		for stream_id, stream in self._streams.items():
			if stream_id in self._stream_buffers:
				buffer = self._stream_buffers[stream_id]
				retention_cutoff = current_time - timedelta(hours=stream.data_retention_hours)
				
				# Remove expired data
				self._stream_buffers[stream_id] = [
					data for data in buffer 
					if data.get("timestamp", current_time) > retention_cutoff
				]

# =============================================================================
# Main Service Orchestrator
# =============================================================================

class GeographicalLocationService:
	"""Main orchestrator for all geographical location services."""
	
	def __init__(self,
				 geocoding_provider: str = "google",
				 enable_caching: bool = True,
				 enable_analytics: bool = True):
		
		# Initialize sub-services
		self.geocoding = GLSGeocodingService(default_provider=geocoding_provider, enable_caching=enable_caching)
		self.geofencing = GLSGeofencingService()
		self.territory = GLSTerritoryService()
		self.routing = GLSRouteOptimizationService()
		self.compliance = GLSComplianceService()
		
		self.enable_analytics = enable_analytics
		self._analytics_events: List[Dict[str, Any]] = []
		
		logger.info("Initialized Geographical Location Service with all sub-services")
	
	# =============================================================================
	# Geocoding Operations
	# =============================================================================
	
	async def geocode_address(self, address: GLSAddress, provider: Optional[str] = None) -> GLSAddress:
		"""Geocode a single address."""
		result = await self.geocoding.geocode_address(address, provider)
		await self._log_analytics_event("geocode_address", {"success": True, "provider": provider})
		return result
	
	async def batch_geocode_addresses(self, request: GLSBatchGeocodeRequest) -> List[GLSAddress]:
		"""Batch geocode multiple addresses."""
		results = await self.geocoding.batch_geocode(request)
		await self._log_analytics_event("batch_geocode", {"count": len(request.addresses), "success": True})
		return results
	
	async def reverse_geocode(self, coordinate: GLSCoordinate) -> GLSAddress:
		"""Reverse geocode a coordinate to get address."""
		result = await self.geocoding.reverse_geocode(coordinate)
		await self._log_analytics_event("reverse_geocode", {"success": True})
		return result
	
	# =============================================================================
	# Geofencing Operations
	# =============================================================================
	
	async def create_geofence(self, geofence_data: Dict[str, Any], tenant_id: str, user_id: str) -> GLSGeofence:
		"""Create a new geofence."""
		result = await self.geofencing.create_geofence(geofence_data, tenant_id, user_id)
		await self._log_analytics_event("create_geofence", {"geofence_type": result.fence_type.value})
		return result
	
	async def process_location_update(self, entity_id: str, entity_type: GLSEntityType, 
									  coordinate: GLSCoordinate, tenant_id: str) -> List[GLSLocationEvent]:
		"""Process a location update and generate events."""
		events = await self.geofencing.process_location_update(entity_id, entity_type, coordinate, tenant_id)
		await self._log_analytics_event("location_update", {"entity_type": entity_type.value, "events_generated": len(events)})
		return events
	
	async def get_entities_in_geofence(self, geofence_id: str) -> List[GLSEntityLocation]:
		"""Get entities currently in a geofence."""
		return await self.geofencing.get_entities_in_geofence(geofence_id)
	
	# =============================================================================
	# Territory Operations
	# =============================================================================
	
	async def create_territory(self, territory_data: Dict[str, Any], tenant_id: str, user_id: str) -> GLSTerritory:
		"""Create a new territory."""
		result = await self.territory.create_territory(territory_data, tenant_id, user_id)
		await self._log_analytics_event("create_territory", {"territory_type": result.territory_type.value})
		return result
	
	async def assign_entities_to_territories(self, entities: List[GLSEntityLocation]) -> Dict[str, List[str]]:
		"""Assign entities to territories."""
		return await self.territory.assign_entities_to_territories(entities)
	
	# =============================================================================
	# Route Optimization Operations
	# =============================================================================
	
	async def optimize_route(self, waypoints: List[GLSWaypoint], 
							 optimization_objective: GLSRouteOptimization,
							 constraints: Optional[Dict[str, Any]] = None) -> GLSRoute:
		"""Optimize a route through waypoints."""
		result = await self.routing.optimize_route(waypoints, optimization_objective, constraints)
		await self._log_analytics_event("optimize_route", {
			"waypoint_count": len(waypoints),
			"objective": optimization_objective.value,
			"distance_km": result.total_distance_km
		})
		return result
	
	# =============================================================================
	# Compliance Operations
	# =============================================================================
	
	async def check_location_compliance(self, coordinate: GLSCoordinate, 
										compliance_types: List[GLSComplianceType],
										tenant_id: str) -> Dict[str, Any]:
		"""Check compliance for a location."""
		result = await self.compliance.check_location_compliance(coordinate, compliance_types, tenant_id)
		await self._log_analytics_event("compliance_check", {"types": [ct.value for ct in compliance_types]})
		return result
	
	# =============================================================================
	# Analytics and Utilities
	# =============================================================================
	
	async def _log_analytics_event(self, event_type: str, data: Dict[str, Any]) -> None:
		"""Log analytics event."""
		if not self.enable_analytics:
			return
		
		event = {
			"event_type": event_type,
			"timestamp": datetime.utcnow().isoformat(),
			"data": data
		}
		self._analytics_events.append(event)
		
		# Keep only recent events (last 1000)
		if len(self._analytics_events) > 1000:
			self._analytics_events = self._analytics_events[-1000:]
	
	async def get_analytics_summary(self, start_time: datetime, end_time: datetime) -> GLSLocationAnalytics:
		"""Get analytics summary for a time period."""
		# Filter events by time period
		relevant_events = [
			event for event in self._analytics_events
			if start_time <= datetime.fromisoformat(event["timestamp"]) <= end_time
		]
		
		# Aggregate analytics
		analytics = GLSLocationAnalytics(
			analysis_period_start=start_time,
			analysis_period_end=end_time,
			entity_metrics={
				"total_location_updates": len([e for e in relevant_events if e["event_type"] == "location_update"]),
				"total_geocodes": len([e for e in relevant_events if e["event_type"] == "geocode_address"]),
				"unique_entities": len(set(e["data"].get("entity_id") for e in relevant_events if "entity_id" in e.get("data", {})))
			},
			geofence_performance={
				"geofences_created": len([e for e in relevant_events if e["event_type"] == "create_geofence"]),
				"total_events_generated": sum(e["data"].get("events_generated", 0) for e in relevant_events if "events_generated" in e.get("data", {}))
			},
			territory_performance={
				"territories_created": len([e for e in relevant_events if e["event_type"] == "create_territory"])
			},
			route_efficiency={
				"routes_optimized": len([e for e in relevant_events if e["event_type"] == "optimize_route"]),
				"total_distance_optimized": sum(e["data"].get("distance_km", 0) for e in relevant_events if e["event_type"] == "optimize_route")
			}
		)
		
		return analytics
	
	async def health_check(self) -> Dict[str, Any]:
		"""Perform health check on all services."""
		health = {
			"status": "healthy",
			"timestamp": datetime.utcnow().isoformat(),
			"services": {
				"geocoding": {"status": "healthy", "provider": self.geocoding.default_provider},
				"geofencing": {"status": "healthy", "active_geofences": len(self.geofencing._geofences)},
				"territory": {"status": "healthy", "active_territories": len(self.territory._territories)},
				"routing": {"status": "healthy", "active_routes": len(self.routing._routes)},
				"compliance": {"status": "healthy"}
			},
			"analytics": {
				"enabled": self.enable_analytics,
				"events_count": len(self._analytics_events)
			}
		}
		
		return health